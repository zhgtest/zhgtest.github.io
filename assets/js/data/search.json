[ { "title": "How to Learn New Skills Effectively: Work Smarter, Not Harder", "url": "/posts/6fc300705d47/", "categories": "", "tags": "medium, jekyll, github-pages, backup, convert", "date": "2025-01-18 16:17:21 +0800", "snippet": "How to Learn New Skills Effectively: Work Smarter, Not HarderDemo Posts for testing medium-to-jekyll-starter project.Photo by ZhgChgLi , Taipei 101Notice: All content below is generated by ChatGPT ...", "content": "How to Learn New Skills Effectively: Work Smarter, Not HarderDemo Posts for testing medium-to-jekyll-starter project.Photo by ZhgChgLi , Taipei 101Notice: All content below is generated by ChatGPT and is intended solely for testing the medium-to-jekyll-starter .— — —In today’s fast-paced world, learning new skills is essential for staying competitive and growing personally and professionally. But with limited time, how can you learn effectively? This article will share practical strategies to help you master new skills efficiently.1. Set Clear GoalsBefore diving into any new skill, ask yourself: Why do I want to learn this skill?Having a clear goal not only gives you direction but also helps you measure progress. For example, are you learning programming to get a job, build a project, or just for fun? The more specific your goal, the more focused your actions will be.Example Goals:• Build a Python automation tool within six months. Participate in an online hackathon and complete a project.2. Break It DownEvery complex skill can be broken into smaller, more manageable parts. Start with the basics and the most important aspects. This not only makes learning easier but also helps you see quick wins.For example, when learning guitar:• Step 1: Learn basic chords (C, G, D, Am) .• Step 2: Practice simple chord transitions. Step 3: Play easy songs.3. Leverage ResourcesModern learners are fortunate to have countless resources at their fingertips. Here are a few common types:• Online Courses: Platforms like Udemy, Coursera, or YouTube.• Books: Classics often provide deep insights into specific fields.• Communities: Join professional forums or groups to learn from others.Choose high-quality resources that suit your learning style, and focus on a few to avoid feeling overwhelmed.4. Practice and Seek FeedbackThe core of learning is applying what you’ve learned. Actively use your skills in real-life scenarios and seek feedback from others to improve quickly.Examples:• Learning to write: Publish a blog post every week and ask friends for constructive feedback. Learning photography: Shoot photos with different themes and join community challenges.5. Build a Learning HabitHabits are more reliable than motivation. Dedicate a consistent time each day for learning, even if it’s just 30 minutes. Small, consistent efforts lead to significant results over time.Tips:• Set daily micro-goals, like practicing five new words today. Use the Pomodoro technique: focus for 25 minutes, then take a 5-minute break.6. Embrace Failure and Stay PatientLearning is a gradual process. When you encounter setbacks, accept them as part of the journey and learn from them. Remember: every expert was once a beginner, and persistence is key.ConclusionLearning a new skill isn’t about doing it all at once — it’s about finding the right approach and staying consistent. Start today by setting your next learning goal and see how far you can go!Post converted from Medium by ZMediumToMarkdown." }, { "title": "ZMediumToJekyll Starter", "url": "/posts/Welcome/", "categories": "tools", "tags": "meidum, github, jekyll, ruby", "date": "2025-01-17 08:00:00 +0800", "snippet": "Medium To Jekyll Starter (based on Chirpy Starter and ZMediumToMarkdown)This project enables you to effortlessly and quickly create a mirrored content site for your Medium blog.Automatically syncs ...", "content": "Medium To Jekyll Starter (based on Chirpy Starter and ZMediumToMarkdown)This project enables you to effortlessly and quickly create a mirrored content site for your Medium blog.Automatically syncs your Medium posts, retrieves all content (including images and embedded code snippets), converts them to Markdown, and serves them as a static website using Jekyll with the Chirpy theme on GitHub Pages.It’s user-friendly, stable, completely free, and requires a one-time setup for lifetime service.Live Demo https://zhgchg.li/ https://github.com/ZhgChgLi/zhgchgli.github.ioPowered by Jekyll Chirpy Jekyll Theme v7.x ZMediumToMarkdown LatestUsage Check out the docs. [Traditional Chinese] Behind the scenes storyBuy me a coffee ❤️❤️❤️If this project has helped you, feel free to sponsor me a cup of coffee, thank you.Other worksIntegration Tools ZReviewTender is a tool for fetching app reviews from the App Store and Google Play Console and integrating them into your workflow. ZMediumToMarkdown is a powerful tool that allows you to effortlessly download and convert your Medium posts to Markdown format. linkyee is a fully customized, open-source LinkTree alternative deployed directly on GitHub Pages." }, { "title": "AWS 雲端 信義房屋 賣房爬蟲架構", "url": "/posts/da095b9be519/", "categories": "", "tags": "aws-ec2, aws-lambda, elasticache, documentdb, crawler", "date": "2024-10-25 18:18:28 +0800", "snippet": "AWS 雲端 信義房屋 賣房爬蟲架構這篇文章主要是來記錄一下信義房屋買房爬蟲的一些細節，那有這麼多房仲為什麼會選擇信義房屋，主要原因是之前找他們問租屋的時候他們的人服務態度很好，然後做事情感覺蠻實在的，所以想說做個 side proejct 先找他們的，大家以後爬蟲完看到好的房子，也可以找信義的房仲，他們人都很好。最後上面這張圖是用 ChatGPT 做的，挺有趣的，反映出來現在台灣第七波打房...", "content": "AWS 雲端 信義房屋 賣房爬蟲架構這篇文章主要是來記錄一下信義房屋買房爬蟲的一些細節，那有這麼多房仲為什麼會選擇信義房屋，主要原因是之前找他們問租屋的時候他們的人服務態度很好，然後做事情感覺蠻實在的，所以想說做個 side proejct 先找他們的，大家以後爬蟲完看到好的房子，也可以找信義的房仲，他們人都很好。最後上面這張圖是用 ChatGPT 做的，挺有趣的，反映出來現在台灣第七波打房的現實殘酷。AWS 架構圖解說 我們這次使用 EC2 將信義房屋的資料撈下來，然後存在 Elasticache redis 中。 接著程式會每一個小時將 Redis 的資料寫進去到 DocumentDB 裡面來做長期的備份 (如果問我說為什麼，我不要直接寫到 DocumentDB 裡面，其實原因很簡單，就是 Elasticache For Redis 是我負責的服務，我主要是在玩 side project 測 bug ，還有看看有沒有什麼坑需要知道的，沒有什麼其他原因 ＸＤ) 我透過 EventBridge 搭配 Lambda 來實現固定小時後，就會把 EC2 的 IP 做切換來避免被網站 Bang 掉不讓爬蟲。 每次我透過爬蟲撈完的物件，會先與 DocumentDB 得資料做比對，如果已經有出現過並且價錢相同就不要再通知我，只有當價錢不同與沒有出現過再透過 SNS 寄信到我的 mail 裡面。 另外我也透過 Data Lifecycle Manager 將定期將我的檔案做備份，以確保我的 log 或者開發程式突然掛了就尷尬了。專案功能說明：這次想說用 Project 的形式玩玩看，看之後有沒有人想要一起寫這個 Project ，所以我就把它放在 Github 上面了，大家有興趣的可以到我的 Project 中看完整 Source Code.那信義房屋的資料可以怎麼撈呢，下面我會簡單的介紹我的程式步驟，讓大家如果要 Fork project 或者想一起寫 Code 比較簡單。 進入到 信義房屋後，並且點選捷運地址時，可以看出他的規律，向他是按照捷運的顏色線與座標做整合。例如：象山的 URLhttps://www.sinyi.com.tw/buy/mrt/Taipei-city/Taipei-R-mrtline/02-mrt Taipei 101 的 URLhttps://www.sinyi.com.tw/buy/mrt/Taipei-city/Taipei-R-mrtline/03-mrt 可以看出他是這樣的規律去命名撈取資料的這樣我們在做比對的時候只要有捷運的數字做 match 就可以將想要的站名與 url 直接結合，另外價錢與房屋類型也是，大家可以照這個規律就可以撈到想要得資訊。 然而如果大家有發現，就會得知他一次搜尋只能搜尋一條線路，那這個部分我在 Project 中也幫大家整合再一起了，大家可以在搜尋的地址中，打自己想要的捷運站，全部都可以幫大家搜尋到。# main.pyimport osimport loggingfrom config.logger import setup_loggerfrom controllers.crawler_controller import CrawlerControllerdef main(): # 確保日誌目錄存在 os.makedirs('logs', exist_ok=True) # 設置主日誌記錄器 logger = setup_logger('main_logger', 'logs/main.log') logger.info(\"應用程序啟動\") try: price_min = 800 price_max = 3000 station_names = [] # 您可以根據需要修改站點名稱 crawler = CrawlerController(price_min, price_max, station_names, logger=logger) crawler.run() except Exception as e: logger.critical(f\"應用程序發生致命錯誤: {e}\") finally: logger.info(\"應用程序結束\")if __name__ == \"__main__\": main()3. 由於我租房子的時候都喜歡離捷運進一點，因此我在過濾每一個房子的戲向的時候我有過濾下面幾個條件，這樣我在收到 mail 的時候，可以快速的確認是否是我想要的房子，來確認要不要點開連結下繼續看下去。 &lt;div&gt; &lt;h2&gt;&lt;a href=” https://www.sinyi.com.tw/buy/house/7183HD ”&gt;查看詳情&lt;/a&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;標題: 戀戀台大次高樓景觀&lt;/li&gt; &lt;li&gt;價格: 2300&lt;/li&gt; &lt;li&gt;建築面積: 15.17&lt;/li&gt; &lt;li&gt;房型: 2廳1衛&lt;/li&gt; &lt;li&gt;樓層: 14樓/15樓&lt;/li&gt; &lt;li&gt;房齡: 18.8&lt;/li&gt; &lt;li&gt;最近車站: [ [‘nearly_station: 公館站 — 出口1, 距離: 7 公尺’] ]&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;上述這幾個是主要的大功能，其他細節歡迎大家 trace code ，或者看完 code 哪裡需要補充的可以我再上來寫一下，或者大家可以看我的前一篇文章，裡面的 code 蠻多都蠻像的，可以透過文章來了解。最後沒意外的話這個 Project 會一直在修改更新，因為我還一些想玩的功能，因此大家有什麼想要做的功能也可以跟我說，有空的話我會繼續寫，並且 medium, github 同步更新，然後有大的版本更新，我在 PO linkedin 通知大家。最後偷偷說，很多地方的邏輯懶得寫我都請 ChatGPT 幫我寫完複製貼上，如果有 bug 在多多擔待了。我的 linkedin: https://www.linkedin.com/in/jackycsie/最後，祝大家每個人都可以買到喜歡且溫馨的家。Post converted from Medium by ZMediumToMarkdown." }, { "title": "AWS EC2 中部署 LLM", "url": "/posts/82bebda01507/", "categories": "Jackycsie", "tags": "ec2, llama-3", "date": "2024-08-25 11:33:37 +0800", "snippet": "EC2 中部署 LLM前言由於最近開刀在家休息，沒辦法出去走走，想說閒閒沒事做，來把 LLM 部署在 AWS EC2 上玩看看。那下面幾個是我在找 LLM Open Source Project 時的幾個考量。 安全性：當 LLM 下載到 EC2 以後，可以隔離對外網路也可以正常使用，不然我就使用 ChatGPT 就好拉 ＸＤＤ，就是不想敏感資料上傳網路。 方便性：目前很多都是透過 AP...", "content": "EC2 中部署 LLM前言由於最近開刀在家休息，沒辦法出去走走，想說閒閒沒事做，來把 LLM 部署在 AWS EC2 上玩看看。那下面幾個是我在找 LLM Open Source Project 時的幾個考量。 安全性：當 LLM 下載到 EC2 以後，可以隔離對外網路也可以正常使用，不然我就使用 ChatGPT 就好拉 ＸＤＤ，就是不想敏感資料上傳網路。 方便性：目前很多都是透過 API 的方式自己寫程式進行溝通，但我又蠻懶的，想要直接找可以像 ChatGPT 有互動式的介面可以直接使用。 可以有登入的介面，並且搭配 Security Group 讓想要使用的朋友同事，也可以使用隔絕對外網路的 LLM。經過簡單的查找，我發現只要兩個 Github Project 就可以達到我的目標。 Ollama (6.7K Fork)： https://github.com/ollama/ollamaOllama 是一個開源的專案，目標是讓使用者能夠在自己的電腦上運行大型語言模型，而不需要依賴雲端服務或強大的硬體設備。以下是 Ollama 專案的一些主要特色： 本地化運行 ：Ollama 可以在 macOS 和 Linux 系統上本地運行，讓使用者能夠更方便地控制和使用大型語言模型，同時保護資料隱私。 高效能 ：Ollama 採用了多種優化技術，包括量化、模型壓縮等，使得大型語言模型能夠在消費級硬體上高效運行。 易於使用 ：Ollama 提供了簡單易用的命令列介面和圖形化使用者介面，讓使用者能夠輕鬆地安裝、管理和使用各種大型語言模型。 開放原始碼 ：Ollama 是一個完全開源的專案，使用者可以自由地檢視、修改和貢獻程式碼。2. Open WebUI (Formerly Ollama WebUI) (4.2K Fork): https://github.com/open-webui/open-webui這個 GitHub 專案 Open WebUI 提供了一個使用者友善的網頁介面，用於操作大型語言模型。它是一個自架設的網頁應用程式，支援多種 LLM 執行器。Open WebUI 的主要特色包括： 輕鬆設定 ：可輕鬆安裝和設定，無需複雜的配置。 Ollama/OpenAI API 整合 ：支援 Ollama 和 OpenAI API，可使用各種大型語言模型。 可自訂的管線 ：使用者可以自訂模型處理管線，以滿足特定需求。 響應式設計 ：介面可自動調整大小，以適應不同螢幕尺寸。那說明一下，其實我也只是搬運工，本身沒有做什麼事情，純粹只是跟大家分享一下，在 EC2 部署隔絕外網的 LLM 以及有身份驗證的功能而已 ＸＤ步驟分享： 我先在 EC2 ubuntu 24.04 上安裝 Ollama，使用的機型是 p3.8xlarge，當然可以不需要用那麼好的設備，我是想要玩超大的 LLM 才用比較好的設備，玩完以後就用最便宜的了。$ curl -fsSL https://ollama.com/install.sh | sh2. 因為 Open WebUI 這個 Project 是使用 Docker 運行，那因為若是 Docker 運行的話要先安裝一個 nvidia-container-toolkit ，到時候 Docker 調用 GPU 的時候才不會報錯，所以我先安裝了 nvidia-container-toolkit ，步驟如下。# Configure the repository:1. curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey |sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\&amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list \\&amp;&amp; sudo apt-get update# Install the NVIDIA Container Toolkit packages:$ sudo apt-get install -y nvidia-container-toolkit# Configure the container runtime by using the nvidia-ctk command:$ sudo nvidia-ctk runtime configure --runtime=docker# Restart the Docker daemon:$ sudo systemctl restart docker3. 安裝 Open WebUIdocker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama4. 確認有安裝成功$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES671f6fc22b24 ghcr.io/open-webui/open-webui:ollama \"bash start.sh\" About an hour ago Up About an hour (healthy) 0.0.0.0:3000-&gt;8080/tcp, :::3000-&gt;8080/tcp open-webui5. 透過 Public IP 搭配 Port 登入到 EC2，介面如下。一開始需要註冊一組帳號密碼，之後再透過這組帳號密碼登入就可以了。6. 登入進來後的介面如下，但這個時候我們還沒有安裝 LLM ，所以還不能使用。7. 下面教學怎麼安裝 LLM model。8. 安裝成功9. 我圖片安裝的是 Llama 的 他有 70B 的參數，約 40GB的大小，若是想要玩玩看其他的 LLM 可以查看這個 Github 網站，他有眾多其他 LLM 可以選擇。10. 我請他用 Java 寫一個簡單的 9 X 9 乘法表。11.請他解讀照片目前都還解讀失敗中，看樣子私有雲的 LLM 大模型，可能還不能解讀照片跟檔案，之後再認真研究一下，因為這塊我的需求比較小，所以沒有認真玩，我主要都在寫程式 ＸＤ11. 好玩的地方在於，當 LLM 在回覆時，全部的 GPU 都會跑，但是我在寫入資料時，只有一個 GPU 在跑。結束超級簡單的分享 ＸＤＤ，想說來做個筆記如何佈建的，大家記得 Security group 在 outbound 也要設定自己的 IP 喔，不然這就不屬於個人使用的 LLM 模型了，敏感資料就可以能 Share 出去了，畢竟是 Open Source Project，大概是這樣～ 其他的大家可以自己部署玩玩看～如果有其他好玩的 LLM 相關的 Project 也可以分享一下喔，感謝各位大大。Post converted from Medium by ZMediumToMarkdown." }, { "title": "AWS 雲端 591 租屋爬蟲架構", "url": "/posts/4a17936aea1a/", "categories": "Jackycsie", "tags": "aws, memorydb, sns", "date": "2024-08-06 15:30:21 +0800", "snippet": "AWS 雲端 591 租屋爬蟲架構緣由最近因為家裡有租屋的需求，剛好比較忙比較沒有空自己一直刷 591 來看，想說剛好自己 own 的服務自己測，所以我就拿了自己的服務玩一下，看看有哪裡沒有很熟的，剛好來了解一下學習，為了避免怕我以後忘記我在寫什麼程式想說久違的使用文章記錄一下。架構圖架構解說我這邊主要使用到的服務是 EC2 On-Demand, EC2 Spot, SNS, MemoryD...", "content": "AWS 雲端 591 租屋爬蟲架構緣由最近因為家裡有租屋的需求，剛好比較忙比較沒有空自己一直刷 591 來看，想說剛好自己 own 的服務自己測，所以我就拿了自己的服務玩一下，看看有哪裡沒有很熟的，剛好來了解一下學習，為了避免怕我以後忘記我在寫什麼程式想說久違的使用文章記錄一下。架構圖架構解說我這邊主要使用到的服務是 EC2 On-Demand, EC2 Spot, SNS, MemoryDB；程式這邊我是使用 Python3.10.12 來做開發。 每天早上 9 點到晚上 9 點，透過 EC2 591 Crawler 程式將 591 上面的房屋資訊撈下來。 將撈下來的房屋資訊與 MemoryDB 做比對，若是重複的 ID 就不儲存。 確認 ID 沒有重複過後透過 SNS 傳送給 Mail 讓我們收到目前最新的租屋資訊。 Spot Change EIP 每天的晚上 12 點會出發 crontab 將 EC2 591 Crawler 的 EIP 切換，避免追蹤被黑名單。 每天透過 Data Lifecycle Manager 幫 EC2 591 Crawler 做備份，目的在於每一次都會儲存 logs 以及每天都可能會修改資料所以確保我的資料都還會存在，我的 snapshot 是保留 5 天，超過 5 天我就刪掉。591 撈取房屋資料介紹關於 591 撈取房屋資料的程式我是參考這篇文章來進行修改的 [Python爬蟲實例] 591 租屋網 — 搜尋房屋與房屋詳情 ，這篇文章說明的很詳細，不管是買房，租屋，都可以透過這個程式做修改達到自己的需求，推推。2024/10/15 更新: 由於 591 那邊的租屋 API 有更新我這邊有寫了新版的爬蟲程式，請大家跟著下面的這個教學網址搭配使用，謝謝:import timeimport randomimport requestsimport refrom bs4 import BeautifulSoupfrom urllib.parse import urlencodeclass House591Spider: def __init__(self): \"\"\"初始化 session 和 headers\"\"\" self.session = requests.Session() self.headers = { 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36', } def get_house_info(self, house_url): \"\"\"根據給定的 URL 取得網頁內容並解析出房屋 ID\"\"\" try: response = self.session.get(house_url, headers=self.headers) response.raise_for_status() # 如果狀態碼不是 200，則引發異常 except requests.exceptions.RequestException as e: print(f\"Error fetching data: {e}\") return set() # 返回空的 set soup = BeautifulSoup(response.text, 'html.parser') house_ids = self.get_house_ids_from_page(soup) return house_ids def get_house_ids_from_page(self, soup): \"\"\"從網頁內容中提取所有房屋 ID\"\"\" house_ids = set() # 使用 set 來存儲唯一的房屋 ID for link in soup.find_all('a', href=True): href = link['href'] match = re.search(r'https://rent\\.591\\.com\\.tw/(\\d+)', href) if match: house_id = match.group(1) # 提取數字部分 house_ids.add(house_id) # 使用 set 的 add 方法來添加元素 return house_ids def search(self, filter_params=None, sort_params=None): \"\"\"搜尋房屋，根據篩選條件和排序條件\"\"\" base_url = 'https://rent.591.com.tw/list?' params = filter_params or {'region': '1', 'kind': '0'} # 預設篩選條件 if sort_params: params.update(sort_params) # 添加排序條件 # 使用 urllib.parse 組合查詢參數 search_url = base_url + urlencode(params) print(f\"Requesting: {search_url}\") # 獲取房屋資訊並添加隨機延遲 house_ids = self.get_house_info(search_url) time.sleep(random.uniform(1, 3)) # 添加隨機延遲，模擬人為請求 return len(house_ids), house_idsif __name__ == \"__main__\": house591_spider = House591Spider() # 定義篩選條件 filter_params = { 'metro': '125', # 捷運線：淡水信義線 'station': '66300,4188' # 捷運站：信義安和 &amp; 大安 } # 定義排序條件 sort_params = { 'sort': 'money_desc' # 租金由高到低排序 } # 執行搜尋 total_count, houses = house591_spider.search(filter_params, sort_params) # 打印結果 print(f\"總共找到 {total_count} 間房屋\") print(\"房屋 ID 列表：\", houses)去重複，寫入 DB 有介紹到我有透過 TLS 加密所以在 python 中需要特別設定 SSL 為 True 另外我這邊有使用 connection pool 接著再使用 redis connect 這樣做的好處可以減少每次程式需要連線時先經過 TLS 的協定，來提高效能。 寫入資料前先比對，資料是否重複，若是資料已經重複就不儲存。 若是資料沒有存在 DB 中 開始做資料前處理，來做傳送給 SNS 的資料做前處理。 資料前處理完成，傳給 SNS feature 來做傳送。import redisimport jsonimport timeimport randomimport requestsimport boto3import datetimeimport pytzfrom bs4 import BeautifulSoupdef connect(REDIS_HOST, REDIS_PORT, REDIS_SSL_CONNECTION, redis_db_instance): try: # Check if SSL connection is required is_ssl_connection = REDIS_SSL_CONNECTION # Create a Redis connection pool with SSL/TLS support redis_connection_pool = redis.ConnectionPool( host=REDIS_HOST, port=REDIS_PORT, db=redis_db_instance, connection_class=redis.SSLConnection if is_ssl_connection else redis.Connection ) # Create a Redis client using the connection pool redis_client = redis.Redis(connection_pool=redis_connection_pool) return redis_client except Exception as err: print(\"Error while connecting Redis client &gt;&gt; \", str(err)) return Falsedef write_db(redis_client, house_info): new_house_list_str = \"\" # 初始化為空字串 i = 1 if redis_client: print(\"Redis 連線成功！\") # 將資料寫入 MemoryDB for key, value in house_info.items(): # 檢查 key 是否已存在 (使用 EXISTS 命令) if not redis_client.exists(key): # 將資料寫入 MemoryDB redis_client.set(key, json.dumps(value)) house_url = \"https://rent.591.com.tw/home/\" + str(key) new_house_list_str += f\"{i} {house_url}: {value}\\n\" # 將資料添加到字串 i = i+1 print(f\"已儲存 key: {key}, value: {value}\") else: # print(redis_client.get(key)) print(f\"key: {key} 已存在，不儲存\") else: print(\"Redis 連線失敗\") return new_house_list_strif __name__ == \"__main__\": REDIS_HOST=\"clustercfg.crawler591.m0szxl.memorydb.ap-northeast-1.amazonaws.com\" REDIS_PORT=\"6379\" REDIS_SSL_CONNECTION=True redis_db_instance=0 # 測試連線 redis_client = connect(REDIS_HOST, REDIS_PORT, REDIS_SSL_CONNECTION, redis_db_instance) new_house_list = write_db(redis_client, house_detail)透過 SNS 將資料 Push 到 Mail當 519 Crawler 的資料前處理完成以後我們會將資料的內容透過 String 的方式傳送到 Mail box，而那個區間沒有新資料的，我這邊還是會傳送 mail 給我知道目前沒有新資料，不然會擔心服務是不是掛掉了，這樣可比較安心。另外我在寄送郵件時也有放個今天的時間以確保收到 mail 的時候不會混亂。import redisimport jsonimport timeimport randomimport requestsimport boto3import datetimeimport pytzfrom bs4 import BeautifulSoupdef get_currect_time(): tz = pytz.timezone('Asia/Taipei') # 或 pytz.timezone('Etc/GMT+8') utc_now = datetime.datetime.now(datetime.timezone.utc) local_now = utc_now.astimezone(tz) formatted_time = local_now.strftime(\"%Y-%m-%d\") return formatted_timedef send_SNS_mail(new_house_list): formatted_time = get_currect_time() # 建立 SNS Client (無需提供憑證) sns = boto3.client('sns', region_name='ap-northeast-1') # 設定郵件內容 subject = formatted_time + ' --- 大安租屋爬蟲' topic_arn = 'arn:aws:sns:ap-northeast-1:237089372480:Rent_591' if new_house_list != \"\": message = f'這是來自 591 爬蟲的通知郵件。\\n\\n{new_house_list}' # 將 new_house_list_str 添加到訊息中 # print(message_with_default) # 發送郵件 response = sns.publish( TopicArn=topic_arn, Message=message, Subject=subject, MessageStructure='string' # 確保純文字格式 ) # print(f'Message ID: {response[\"MessageId\"]}') else: message = f'目前沒有新的資料' response = sns.publish( TopicArn=topic_arn, Message=message, Subject=subject, MessageStructure='string' # 確保純文字格式 )if __name__ == \"__main__\": send_SNS_mail(new_house_list)隨機睡眠時間寫這個的目的主要在於降低被抓的機率，如果每天在都在相同時間撈資料，久而久之就容易被 block 掉，這樣的好處就是可以再降低被發現是爬蟲的服務。def random_sleep(): \"\"\" 隨機休眠 5 分鐘到 10 分鐘之間。 \"\"\" # 計算休眠秒數範圍 min_seconds = 5 * 60 # 5 分鐘 max_seconds = 10 * 60 # 10 分鐘 # 隨機生成休眠秒數 sleep_seconds = random.randint(min_seconds, max_seconds) print(f\"開始休眠 {sleep_seconds} 秒...\") time.sleep(sleep_seconds) # 進行休眠 print(\"休眠結束！\")更改 EIP由於擔心一直爬網站會被 591 Bang ，所以我這邊用了一個定時換 Public IP 的程式，讓實例每天在晚上 12 點的時候，會自動把舊的 EIP 刪除，並且換上新的 EIP 。import boto3import osec2_client = boto3.client('ec2')def get_eip_allocation_id_by_instance_id(instance_id): \"\"\"根據 Instance ID 取得 EIP Allocation ID\"\"\" try: response = ec2_client.describe_addresses( Filters=[ { 'Name': 'instance-id', 'Values': [instance_id] } ] ) if response['Addresses']: return response['Addresses'][0]['AllocationId'], response['Addresses'][0]['AssociationId'] else: return None except Exception as e: print(f\"取得 EIP Allocation ID 時發生錯誤: {e}\") return Nonedef dissociate_eip(allocation_id): \"\"\"將 EIP 與 EC2 instance 解除關聯\"\"\" try: response = ec2_client.disassociate_address( AssociationId=allocation_id ) print(f\"EIP {allocation_id} 已解除關聯\") except Exception as e: print(f\"解除關聯 EIP {allocation_id} 時發生錯誤: {e}\")def release_eip(allocation_id): \"\"\"釋放 EIP\"\"\" try: response = ec2_client.release_address( AllocationId=allocation_id ) print(f\"EIP {allocation_id} 已釋放\") except Exception as e: print(f\"釋放 EIP {allocation_id} 時發生錯誤: {e}\")def change_ec2_public_ip(instance_id, region_name): \"\"\" 更換指定 EC2 執行個體的公有 IP 地址，安全處理 EIP 和 Public IP 的各種情況。 Args: instance_id (str): EC2 執行個體的 ID。 region_name (str): EC2 執行個體所在的 AWS 區域名稱。 \"\"\" ec2 = boto3.resource('ec2', region_name=region_name) instance = ec2.Instance(instance_id) # 分配新的 EIP new_allocation = ec2.meta.client.allocate_address(Domain='vpc') response = ec2.meta.client.associate_address( AllocationId=new_allocation['AllocationId'], InstanceId=instance_id ) print(f\"已成功為 EC2 執行個體 {instance_id} 更換公有 IP 地址為 {response['AssociationId']}\")if __name__ == \"__main__\": instance_id = [\"\"] region_name = \"ap-northeast-1\" for i in range(0, len(instance_id), 1): eip_data = get_eip_allocation_id_by_instance_id(instance_id[i]) if eip_data: eip_AllocationId, eip_AssociationId = eip_data print(f\"Instance {instance_id[i]} 的 EIP Allocation ID 為: {eip_AssociationId}\") dissociate_eip(eip_AssociationId) release_eip(eip_AllocationId) else: print(f\"Instance {instance_id[i]} 沒有關聯的 EIP\") change_ec2_public_ip(instance_id[i], region_name)總結簡單的架構圖與程式碼差不多就寫好了，整體來說蠻簡單的，主要是來練習一下透過 Python3 程式碼與這些串接，以及來測試一下平常客戶端可能會碰到什麼痛點來模擬一下，以及用客戶開發的想法來思考事情，祝大家能找到自己喜歡的房子。下一個計畫目前看到信義房屋這邊還沒有人寫，下次用其他 DB 來玩一下，不然 MemoryDB 這個太貴了，燒我太多錢了。Post converted from Medium by ZMediumToMarkdown." }, { "title": "AWS Certified Cloud Practitioner", "url": "/posts/8f93c9b4fc31/", "categories": "Jackycsie", "tags": "aws, cloud", "date": "2022-05-19 01:06:29 +0800", "snippet": "AWS Certified Cloud PractitionerHi 大家, 最近考過了 Cloud Practitioner 的證照, 想說趁著記憶猶新紀錄一下當初準備的內容.背景過去是在 IT 行業工作, 主要的領域在於 on-premise 的 infrastructure, 在 cloud 中只有玩一些基礎的 AWS service 經驗約 6 個月上下。Outline 上課資源 ...", "content": "AWS Certified Cloud PractitionerHi 大家, 最近考過了 Cloud Practitioner 的證照, 想說趁著記憶猶新紀錄一下當初準備的內容.背景過去是在 IT 行業工作, 主要的領域在於 on-premise 的 infrastructure, 在 cloud 中只有玩一些基礎的 AWS service 經驗約 6 個月上下。Outline 上課資源 模擬考測驗 考試 Tips 大大們的參考網站 心得1. 上課資源1.1 首先個人主要學習的內容是透過 Udemy 線上課程，講師是這一位 Stephane Maarek ，當初是透過特價的時候買的花了 NT$ 390 元，這位講師教得非常的清楚而且淺顯易懂，並且有完整的 PDF 可以重點整理，另外他也有錄製這些 service 的 Lab ，可以跟著他做加深學習印象，他也有 Youtube Channel 都有片段課程可以學習。https://www.udemy.com/user/stephane-maarek/https://www.youtube.com/c/StephaneMaarek/featured1.2 官網根據 Certified Cloud Practitioner 出的課程可以學習，另外上完這個課程以及 AWS 提供的考試前 TIPS，就可以拿到 US$50 折價券， 相當不錯，想省一筆的老鐵們可以把握機會，相關資訊有可以問 AWS member, “aws-tw@amazon.com”https://aws.amazon.com/tw/training/learn-about/cloud-practitioner/https://pages.awscloud.com/TRAINCERT-GCR-tw_get_certified_cp_20220419_RegPage.html?trk=fc1de7d0-0289-4a8b-802d-18daeab7dd7b&amp;sc_channel=em1.3 有時候我們並非為了考試而考試，而是真的想去學習一個 Service 的內容這時候就非常推薦 Neal Davis 的 Medium 文章，裡面對非常多的 Service 有細節的描述，另外他也有在 Udemy 也有販售課程，可以去試聽看看。https://www.udemy.com/course/aws-certified-cloud-practitioner-training-course/https://medium.com/@neal-davis1.4 這是我去 Google 搜尋中大家都大推的 Youtube 課程，我自己看了一些以後覺得適合已經對 AWS 有一定的認識以及主要在於想要考證照的人影片，不太適合邊學邊做的人。1.5 實體書籍，這方法也是我最早學習 AWS 的方法，當初我是先買這本書將它看完才來玩 lab 的，我個人覺得非常不錯他是以平常的實體案例套入進去 AWS 的 Service 中，加速我們理解每個 Service 之間對應的關係，若是平常喜歡唸實體書的朋友們歡迎也可以買這本書看看。1.6 說完了前面幾種有系統地學習方法後，推薦最後一種方法當有些課程介紹 Service 不夠詳細時，我自己會做的方法去加深印象，首先就是去 AWS 官網找他們對這個 Service 的介紹，中英文的解釋都看一遍，接著就是去 AWS Service 實作一下，去了解他的整個流程，相信這個方法會對整個 Service 更加印象深刻，推薦給大家。2. 模擬考測驗模擬測驗個人認為是重中之重，之前在做模擬測驗前我認為關於基礎的 Service 都已經有足夠的瞭解了沒想到，模擬考出來後只對了一半，這時候才發現原來準備的方現跟內容不夠，所以下面會推薦幾種我自己有測驗的方法。2.1 上網購買 Udemy 的線上課程個人非常推薦購買此課程，當初特價花了 NT$ 399 好處是可以模擬實際的考試環境，可以降低考試時的緊張感，並且每一題題目都有詳細解答可以學習，他可以考無限次數，總共有 6 份試卷，並且可以記錄每一次考試錯誤的內容細項。https://www.udemy.com/course/practice-exams-aws-certified-cloud-practitioner/2.2 免費的線上考試題目，題目非常的多樣性，好幾百題，我到考試前都還是看不完，但有個小缺點就是他的答案不一定是完全正確的，好處是每一題都會有許多的討論思考。https://www.examtopics.com/exams/amazon/aws-certified-cloud-practitioner/2.3 官網有提供 10 題題目，讓你熟悉一下考試的題型，也可以參考看看，這幾題算是非常基礎，若是朋友們不太會的話，可能要再加油！https://d1.awsstatic.com/training-and-certification/docs-cloud-practitioner/AWS-Certified-Cloud-Practitioner_Sample-Questions.pdf3. 考試 Tips在這個章節主要會跟大家分享兩個考試 Tips, 第一個是分享如何增加 30 分鐘的考試時間，第二個是我自己考試前的重點筆記。3.1 在這個 Part 我們必須選擇英文考試，才能有考試增加 30 分鐘，那下圖就來介紹吧！3.1.1 首先先登入 AWS 的考試認證網站。https://www.aws.training/Certification3.1.2 點選 Request Exam Accommodationshttps://aws.amazon.com/tw/certification/policies/before-testing/3.1.3 點選 Request Accommodations，在這裡我已經有出現 ESL +30 Minutes，可以先忽略，先 Follow 下面步驟。3.1.4 在紅匡中選擇 ESL + 30 MINUTES3.1.5 回到上一個頁面就會出現 Approved 的狀態顯示了，這樣就代表成功了，之後的所以考試只要選擇英文考試，並且你是非英語系母語國家的，考試時間就會自動增加 30 分鐘，算是給自己多一點思考的時間。3.2 這個章節我會附上當初考試的一些重點整理思考，但因為本身也是菜鳥，為了怕說錯內容，文中有哪裡沒寫對的，再麻煩通知一下了，謝謝。AWS 中有哪些 Service 是屬於 Global Services. Identity and Access Management (IAM) Route 53 (DNS service) CloudFront (Content Delivery Network) WAF (Web Application Firewall)Shared Responsibility Model 當考時題目出 “in” 的時候是問 User 的 Responsibility， “of ” 的時候是出 AWS 的 Responsibility. 以及哪些東西是共同需要負責的？IAM User/Group/Policy/Role 分別代表的意思是什麼？下面這個影片講解得非常清楚，有興趣的老鐵可以學習一下。Security Group 與 NACL 的差異是什麼？ Security Group 主要是 Instance 導向，NACL 是 IP 做為導向。 Security Group 是 Stateful，NACL 是 Stateless Security Group inbound default 是 disable, OutBond default 是 AuthorisedMFA 的功用為何？他所應用的場所是什麼？他是哪個第三方的硬體 USB 設備做驗證？ MFA 做第二次的身份驗證 U2F security keyAWS CLI/SDK/Mangement Console 的差別是什麼？ Mangement Console: GUI SDK: Program CLI: Command LineEC2 Image Builder 與 AMI 對應的關係是什麼？ EC2 Image Builder 可以定期定時 Build 出 AMIELB 與 ASG 都是可以使用 Multi-AZ ELB 有三種 Type, Application LB (Layer — 7), Network LB(Layer-L4) ASG Scale EC2 instances on your system, replace unhealthyS3 個系列的重點整理，主要可以去官網找細節。 https://aws.amazon.com/tw/s3/pricing/ One Zone IA：當有資料需要備份到某個單點 AZ 時，節省花費 不確定自己的資料會多久才儲存的，並確保可以有 High Availability，可以選擇 Intelligent- Tiering 有些資料確定短時間不會讀取，想要有更省 Cost 的儲存方式可以選擇 Glacier 類型。 Glacier 也分三種類型，Retrieval Time 與 花費的差別，分別是 Instant Retrieval, Flexible Retrieval, Deep Archive, 若需要立馬恢復就選擇第一個， 5m ~ 12hr 選擇 Flexible, 恢復速度最慢的是 Deep 需要約 12~48 小時。S3 加密總共分為 2 種選擇給 User Server Side Encryption Client Side Encryption當需要 Data migration 時Snow Family 有分為三種，依照能存放資料的大小最排序。 Snow one, Snowball Edge, Snow MobileData Type 類型 Block Type: EBS, EC2 instance store File Type: EFS Object Type: S3當需要有一個 Service 可以將雲端與地端整合在一起 Storage GatewayStorage Gateway 在地端 support 3 種 Type Files, Volume, TapesRDS 是 關連式資料庫，Support 的類型有 Mysql MariaDB Microsoft SQL ServerAurora 是 AWS for PostgreSQL 與 MySQL performance tuning 的 DB;DocumentDB 是 AWS for NoSQL performance tuning 的 DB;ElastiCache for memory 加速運算的 DB 就像是 Redis.其他常考的 DB 內容: Key/Value Database: DynamoDB (serverless) Warehouse — OLAP: Redshift AmazonQLDB: For 金融使用 Database Migration: DMS Neptune:Graph DatabaseContainer 系列： Elastic Container Service(ECS) for Container 使用，當有 user 想要使用 Container 技術並且有需要在自己的管理的 server 上使用時會選擇的服務。 Fargate(Serverless): User 只想 Launch Container 在 AWS 上，不想管底層時，所選擇的服務。 Elastic Container Registry(ECR): 可以想像成地端的 Harbor。AWS 上常使用的 Serverless 產品有 Lambda Fargate DynamoDB S3當使用 CronJob 時主要有兩種選擇 Lambda，有時間限制 Batch，沒有時間限制當一個 Customer 沒有 IT 背景，但想要快速 Build Service 起來時。 lightsail有一群 Startup 對 IT 有基礎知識，但只想專注在 PaaS 以上的服務時，如 Web 開發。 BeanstalkCodeArtifact 公司軟體的中心Systems Manager (SSM) 可以不需要開 Port 就讓 Mangement Console 登入的方法。OpsWorks 整合 Puppt 與 CHEF，以及他是 HybridRoute 53 有 4 種 Mode Simple Routing Policy Weighted Routing Policy Latency Routing Policy Failover Routing Policy防止 DDOS 的 3 種 Service Shield WAF CloudFront(CDN)持續更新中…3.3 當天考試注意事項因為那時候我考試太抖，所以連從哪裡可以連線到 VUE 都不知道，這裡直接幫大家截圖了，就不用擔心了，從 Mange Person VUE Exams 點進去就可以拿到 key 碼然後就可以網路連線考試囉。另外系統會手機發送簡訊，做一些人臉識別與桌子與附近還近驗證的照片，我自己蠻推先下載 Person VUE 這樣可以確保當下是 Stable 的，個人運氣蠻差的光驗證花了 40 分鐘。4. 大大們的參考網站非常感謝各位大神們的文章，才可以這麼順利的考過考試，也從當中學到了不少知識。推推這篇文章，好險有看到文章有提醒要使用英文認證，我才有準備護照，提醒大家一定要考線上考試的話一定要用外國人有國際認證的喔，那時候我有 mail 給 AWS support ，他說可以用國民身分證，但實際考試其實不行 ＱＱ，印度考官會請你重新拿一份有含英文名字的身份做驗證下面這邊文章也是位大大寫的，裡面也記錄了許多考證照時需要用到的知識，相當值得一看。" }, { "title": "Try Try Podman", "url": "/posts/8419853b7a4f/", "categories": "Jackycsie", "tags": "podman, kubernetes, containers", "date": "2021-12-17 19:22:54 +0800", "snippet": "Try Try Podman本篇簡單的紀錄一下，Podman 的一些基礎用法，適合給超級新手村的人快速上手，因為基礎的東西跟 docker 幾乎都一樣，難怪看網路上的人說可以直接這樣，在過程中我也常常太順打成 dockeralias docker=podmanEnvironmentOS: Red Hat Enterprise Linux release 8.4Podman: 3.3.1Ste...", "content": "Try Try Podman本篇簡單的紀錄一下，Podman 的一些基礎用法，適合給超級新手村的人快速上手，因為基礎的東西跟 docker 幾乎都一樣，難怪看網路上的人說可以直接這樣，在過程中我也常常太順打成 dockeralias docker=podmanEnvironmentOS: Red Hat Enterprise Linux release 8.4Podman: 3.3.1Step Create Dockerfile Build image Run container Stop container# create folder，folder tree like this.Below is my dockerfile. This time I only use python image. Otherwise, I install python package through “requirements.txt”.Build imageFROM python:3.6.8-slimWORKDIR /appCOPY requirements.txt ./RUN pip3 install -r requirements.txt“requirements.txt” is mean your python package list. when you use this package list. It’s more easily to management your package version and reduce your command.paramiko==2.8.0In this crawler_project folder type below command.$ podman build -t jacky_project .#show image$ podman imagesRun container$ podman run --name jacky_python -v /tmp:/tmp -id localhost/jacky_project$ podman ps# Then, you can see container status.Stop container$ podman stop jacky_pythonRemove Container$ podman rm jacky_pythonRemove image$ podman rmi localhost/jacky_project原本以為從 docker 轉來 podman 會非常的麻煩，看來真的無縫轉移呢。之後再去玩 podman 的其他特色功能，再跟大家分享。初步使用 podman 後的心得Ref: Podman official webiste Podman pod introudce How to write dockerfile Python Docker file 實作撰寫第一個 Dockerfile 小飛機的 podman 介紹 Day 5 關於 Container 的那些大小事 An Introduction to PodmanPost converted from Medium by ZMediumToMarkdown." }, { "title": "Through Python SSH Tool get infra data.", "url": "/posts/b711d343967c/", "categories": "", "tags": "paramiko, cisco", "date": "2021-12-14 19:53:20 +0800", "snippet": "Through Python SSH Tool get infra data.本篇文章主要是紀錄一下，如過透過 python SSH tool(paramiko) 達到如同 CLI 的交互是命令的功能。因為最近有需要撈我們 Cisco UCS FI 的一些 system 資訊，但又不想要一台台進去下 command 撈資料，而這些 system 的資訊， Cisco 官方並也沒有提供 SDK...", "content": "Through Python SSH Tool get infra data.本篇文章主要是紀錄一下，如過透過 python SSH tool(paramiko) 達到如同 CLI 的交互是命令的功能。因為最近有需要撈我們 Cisco UCS FI 的一些 system 資訊，但又不想要一台台進去下 command 撈資料，而這些 system 的資訊， Cisco 官方並也沒有提供 SDK 可以讓我們直接 filter 出來，因次只能自己稍微開發一下讓她快速簡單的自動化處理。那我們這次使用的工具是 python 的 paramiko tool。官方文件: https://www.paramiko.org/pip install paramiko透過 pip install 以後就可以開始使用了。下面方法為普通開一個 session 的使用方法，但這個方法有一個壞處就是沒辦法同時下多個指令，當然若你連接的是 Linux Server 的話其實沒關係，因為可以使用 “;” 號做多行指令的切割或者 “|“，當然若是嫌麻煩，ansible or shell script 也是你的好夥伴。#!/usr/bin/pythonimport paramikossh = paramiko.SSHClient()ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())ssh.connect(\"Your_IP_address\", username=\"username\", password=\"password\")stdin, stdout, stderr = ssh.exec_command(\"pwd\")print stdout.readlines()ssh.close()但是 Cisco UCS 的 CLI 是一層包一層的，他沒辦法透過上述的方法完成，所以就稍微研究了一下有沒有可以透過 paramiko 實現 如同 terminal 一樣，是否交互式問答的方法，那結果就列在下面當作筆記。def get_system_time(position): #use invoke_shell open interactive interface channel = ssh.invoke_shell() #send command to Cisco UCS channel.send('connect nxos ' + position + '\\n') #waiting Cisco UCS reply time.sleep(5) #send other Cisco UCS command channel.send('show system uptime\\n') time.sleep(5) #Save terminal out = channel.recv(9999) system_output = out.decode(\"ascii\") #data clean split_string = system_output.split(\"\\r\\n\") return split_string#create Cisco sessionssh.connect(\"Cisco_UCS\", username=\"username\", password=\"password\")FI_A_uptime = get_system_time('a')ssh.close() 完成，整體難度不高，只是需要找一下 key function 而已。接著就是 data clean 就可以實現 infra data 可以自動化收集的部分，那這樣對於做 SRE 相關的 member 在思考 SLA 或者 create dashboard 會更加的快速且方便。Post converted from Medium by ZMediumToMarkdown." }, { "title": "AWS — 學習筆記(3) Deploy ELB", "url": "/posts/2b0d86c776f7/", "categories": "Jackycsie", "tags": "elb, aws", "date": "2021-10-18 18:37:34 +0800", "snippet": "AWS — 學習筆記(3) Deploy ELBHi all,這篇文章主要是記錄 AWS ELB (Elastic Load Balancing)的內容，而 ELB 當中又有分四種負載平衡器，分別是 Application Load Balancer，Network Load Balancer，Gateway Load Balancer，Classic Load Balancer，而這時紀錄...", "content": "AWS — 學習筆記(3) Deploy ELBHi all,這篇文章主要是記錄 AWS ELB (Elastic Load Balancing)的內容，而 ELB 當中又有分四種負載平衡器，分別是 Application Load Balancer，Network Load Balancer，Gateway Load Balancer，Classic Load Balancer，而這時紀錄的是Application Load Balancer 這種方法，之後再慢慢的將其他幾種方法也記錄下來，分享給大家。簡單介紹一下負載平衡不只是為了分擔服務的業務量，也是確保服務在不同地區有任何樣狀況時，可以確保服務穩定性的重要概念，那就不多描述了。ELB 這幾種他的主要差別，有去找了幾篇簡單易懂的大師們寫的文章，歡迎大家去看，他們真的寫得不錯，推推。 有圖介紹更容易懂: Blackie Tsai 外國大大的文章: AWS — Elastic Load Balancer (ELB) Overview 淺顯易懂: AWS Load Balance 基本概念介紹下面是今天的架構圖，承接上一次的實驗，我們這次會建立一個 Application Load Balancer，一個 Target group(裡頭包含兩個 instance)，Create 一個新 AZ-c 裡的 subnet，專門開一個通道給 balance 的 security group port 80.系列文章AWS — 學習筆記(1) Deploy ENV/EC2AWS — 學習筆記(2) NAT/Container Service接著是建置步驟: 建立新的子網路 1c. 佈建在新的 AZ — 1c 中使用的 instance. 建立專門給 ALB 用的 Security Group 建立目標組 Target group 建立 ALB DemoNaming rule： AZ: jackysideproject-web-1c, 172.16.11.0/24 EC2: 172.16.11.10, jackysideproject-web2 給 ALB 使用的 SG: jackysideproject-alb-sg Target group: jackysideproject-web-tg ALB: jackysideproject-web-alb建立新的子網路 1C第一步建立新的 subnet，目的在於之後不確定因素將其中一個 AZ 掛了，還是可以透過 ELB 進行負載平衡，在這裡我們選擇 singapore的 1c AZ。這裡需要特別注意因為我們的網路需要對外連線，所以這裡需要選擇我們之前設定可以對外的 Routing Table.佈建在新的 AZ — 1c 中使用的 instance建立一個 instance 選擇在 1c 的 subnet.設定 instance 為 172.16.11.10.設定 Tag，方便未來識別。設定完以後，透過 ssh 連進去 web2 container 設定黨，進行 web page 的修改。$ docker exec -it deeplearnaws-web sh$ vi app.js$ docker container restart deeplearnaws-web建立專門給 ALB 用的 Security Group建立目標組 Target group首先我們需要先將我們想要 Load balance 的 instance 打開，並且確認已經開啟，接著 Create Target group。這裡有很多的選項，這裡我們這次紀錄的是 Instance 的方法做 load balance.這個地方蠻有趣的記得要選到對的 VPC 喔～我們需要選擇自己建立的 VPC，不是 default 的，不然會找不到需要 load balance 的 instance。出現了我們剛剛 lunch 的 instance ，選擇這兩個 web1/2。建立 ALB選擇 load balancer。看到很多有趣的圖案以後，我們這次選擇第一個 Application Load Balancer。細節設定，確認他是對著公開網路進行 load balance。選擇要使用哪個 VPC 的那個 instances。選擇開通的 port number。Demo注意: ALB 建立完以後需要等待一陣子確認他完成，在開始測試 ELB 是否成功。首先看到我們的紅框複製。在網頁中，輸入剛剛的 DNS，多 refresh 幾次就可以看到不同 instance 所秀出來的網頁內容。 web1 顯示的是 Hi, Sir. 另一個 web2 顯示的是 Hi, Madam.酷吧，之後再跟大家聊聊 DNS 53。感想:這次做完覺得最有趣的是畫架構圖，要完整理解並且畫的大家看得懂，始終是未來的目標，目前還畫得很爛，需要再進步，另外走了維運後發現，固然這些元件很重要，但真正的 key idea ，還是覺得若是能知道他是如何實作的，才會是真正的核心，這也是 james 以前不斷教導我的，雖然重視的人不多，但來了這裡一年發現，這才是真正的核心，也是最重要的，中間迷失了一陣子，希望我能確保自己往正確的道路前進，加油。Post converted from Medium by ZMediumToMarkdown." }, { "title": "AWS — 學習筆記(2) NAT/Container Service", "url": "/posts/e23d78f1ab55/", "categories": "Jackycsie", "tags": "aws", "date": "2021-10-06 21:56:05 +0800", "snippet": "AWS — 學習筆記(2) NAT/Container ServiceHi all,本篇的筆記主要紀錄的是 Private IP 的 OS 如何透過 NAT 進行對外的網路溝通，另外也會將 Public IP 與 Private IP 的服務進行連接。Public subnet 使用的是 web container ，Private subnet 使用的是 DB container，中間溝通...", "content": "AWS — 學習筆記(2) NAT/Container ServiceHi all,本篇的筆記主要紀錄的是 Private IP 的 OS 如何透過 NAT 進行對外的網路溝通，另外也會將 Public IP 與 Private IP 的服務進行連接。Public subnet 使用的是 web container ，Private subnet 使用的是 DB container，中間溝通的方法是使用 Port 3306。最後此篇 container 是從 小馬 blog pull 下來的，因為我目前只想專注 AWS 元件的佈建，等之後需要用到 EKS 或者 ECS 在自己做一下 container，但若是目前還不太會 container 技術的可以玩一下 Podman or Docker 。下圖是本次的架構圖，承接了 AWS — 學習筆記(1) 的內容繼續實作。接著是建置步驟: 建立 AWS NAT 建立屬於 Private subnet 的 Routing table 建立 AWS Private subnet 的 Security group 建立 Private subnet 的 EC2 instance. 在 Private subnet 中 pull 小馬的 DB container 在 Public subnet 中 pull 小馬的 Web container，將 Public subnet 的 OS 與 Private subnet 進行溝通。Naming rule： NAT : jackysideproject-web-nat DB Routing Table: jackysideproject-db-rtb DB Securty Group: jackysideproject-db-sg EC2 Private IP: 172.16.20.10/32 Private subnet OS: jackysideproject-db11. 建立 AWS NAT參考文章： AWS NAT 有什麼不同？注意：建立 NAT 是需要收費的喔，它沒有免費的額度可以使用。我們 create NAT gateway接著我們選擇此 NAT 元件在 Public subnet 當中，並且 launch 一個 Elastic IP 來使用。2. 建立屬於 Private subnet 的 Routing table首先我們會先建立一個 routing table，並設定成此 VPC 都可以使用。接著建立好路由表以後，我們將所有非 172.16.0.0/16 的網路倒給 NAT 去做轉送，讓網路透過 NAT 對外溝通，這樣 private EC2 才可以做通訊。將這個 routing table 設定給 private subnet 做使用。3. 建立 AWS Private subnet 的 Security group此步驟我們建立 Prviate subnet 的 Securty group，將 Port 22/3306 allow 進 private subnet 中。Allow SSH, 否則沒有人可以連得進去 Private subnet 的 OS. 哈哈4. 建立 Private subnet 的 EC2 instance.選擇 private subnet，並且將 auto-assiang public IP disable 掉。設定這個 EC2 的 IP 為 172.16.20.10選擇此 EC2 使用哪一個 Security group，在這裡我們使用 DB 的那個，允許 22 port 以及 3306 port 進來。Cool~5. Private subnet 中 pull 小馬的 DB container在 pull container 前我們必須 pre-check Public IP 連的進 private IP 的 OS 嗎？ 可以確認可以透過 NAT gateway 對外溝通嗎？# Use public OS ssh to private OSssh -i aws_ssh_key.pem ec2-user@172.16.20.10# Check NAT gatewaysudo yum update -yping 8.8.8.8確認有網路後， 從網路上 pull 一個 container 使用，在這裡我是參考小馬 github 的 container 直接使用，有興趣的也可以自己建立一個。# pull mysql container $ docker pull komavideo/deeplearnaws-mysql:latest# running container and allow 3306 port$ docker run --name deeplearnaws-mysql -e MYSQL_ROOT_PASSWORD=12345678 -p 3306:3306 -d --restart=always komavideo/deeplearnaws-mysql:latest# check mysql container is running.$ docker ps# access to mysql container$ docker exec -it deeplearnaws-mysql bash -p# 按照下面指令，就可以看到 db 的資料&gt;mysql -u root -p -h 127.0.0.1mysql&gt; show databases;mysql&gt; use blogdb;mysql&gt; select * from user;mysql&gt; exit;成功6. Pull web container 以及與 Private subnet 的 DB 溝通。# Switch 到 Public IP 的 OS 中# Pull web container$ docker pull komavideo/deeplearnaws-web:latest# Running web container，allow port 30 到 container 裡面的 3000$ docker run --name deeplearnaws-web -p 80:3000 -d --restart=always komavideo/deeplearnaws-web:latest# Check container status$ docker container ls -a$ docker logs -f deeplearnaws-web$ docker ps接著在自己的 local PC 輸入 public IP 即可，成功登入。感想雖然過程非常的簡單快速，但其實這當中有非常多的細節可以去學習，而我也還在這個道路上努力中，若真的有看完這兩份文章的你，希望對你有幫助。但這只是個開始，希望這個流程不會變成你們公司或者團隊的 SOP，自己對 SOP不喜歡，因為這樣會讓你缺乏了思考的能力，所以我只寫個能 working 的大方向，細節一定要學習，希望我們都不會被 “體制化” ，加油！一起跳出舒適圈吧！！！下集預告下一篇，會使用打包Web-AMI，去實作如何使用 ELB 打造 Load balance 的 service. Name: jackysideproject-web-amiPost converted from Medium by ZMediumToMarkdown." }, { "title": "AWS — 學習筆記(1) Deploy ENV/EC2", "url": "/posts/ea5e5f56d936/", "categories": "Jackycsie", "tags": "aws, ec2", "date": "2021-09-29 23:48:02 +0800", "snippet": "AWS — 學習筆記(1) Deploy ENV/EC2Hi, All本篇文章將會介紹從無到紀錄如何在 AWS 環境中，模擬出一個與地端相同 server 環境下圖是本次實驗的架構圖與 follow action，本篇文章主要是記錄每一個步驟，若是有元件概念不懂或者寫得不夠詳細的歡迎來信詢問。 建立自己的 VPC VPC 切開分群為 public subnet &amp; private...", "content": "AWS — 學習筆記(1) Deploy ENV/EC2Hi, All本篇文章將會介紹從無到紀錄如何在 AWS 環境中，模擬出一個與地端相同 server 環境下圖是本次實驗的架構圖與 follow action，本篇文章主要是記錄每一個步驟，若是有元件概念不懂或者寫得不夠詳細的歡迎來信詢問。 建立自己的 VPC VPC 切開分群為 public subnet &amp; private subnet 設定 IGW, 建立對外的 route table 建立 ACL 與 SG 建立 EC2 透過自己的 terminal 連接 AWS EC2前提概要：VPC: 172.16.0.0/16Public Subnet, jackysideproject-web-1a: 172.16.10.0/24Private Subnet, jackysideproject-db-1a: 172.16.20.0/24EC2 Private IP: 172.16.10.10Create Security Group: jackysideproject-web-sgSecurity Group Allow Port: 22Create IGW routing table: jackysideproject-web-rtb標籤:VPC: jackysideproject-vpcPublic Subnet: jackysideproject-web-1aPrivate Subnet: jackysideproject-db-1aIGW: jackysideproject-igwRouting Table: jackysideproject-web-rtbSecurity Group: jackysideproject-web-sg建立自己的 VPC參考文章: 什麼是 AWS VPC第一步，點開我們的 VPC，Create VPC.建立 VPC，設定 VPC subnet，建立 vpc tag。建立完成。Create Public/Private subnet建立完 VPC 後開始切割網段，在這個部分我們將會分為 172.16.10.0/24, 172.16.20.0/24 兩個網段。Public Subnet, jackysideproject-web-1a: 172.16.10.0/24Regin: 1aTag: jackysideproject-web-1aPrivate Subnet, jackysideproject-db-1a: 172.16.20.0/24Regin: 1aTag: jackysideproject-db-1a設定完成。Setting IGW and routing table參考文章: 什麼是 IGW設定完兩個 subnet 以後此時 subnet 還沒有對外連網的功能，我們將 jackysideproject-web-1a 設定外網的功能。設定 IGW。從這裡可以看的到，目前設定好的 igw 是 detached 的狀態所以需要將 VPC attach 到 igw中。選擇我們 create 好的 VPC。接著我們需要設定 routing table，下圖這個是 default 的 routing table，主要的目的在於同一個 VPC 內的 IP 都能夠互通，但為了能夠連外網我們需要再建立一個 routing table。建立 jackysideproject-web-rtb，這個 routing table 依賴於我們所建立的 jackysideproject-vpc。從下圖可以看到我們建立完 routing table 了，接著將外網的流量導入進來。設定 0.0.0.0/0， Target 我們自己設定的 IGW，此時所有的流量都會導入進來了。但是我們還沒設定導入到哪一個 subnet 當中，所以我們選擇 routing table，並且將 routing table 導入到 web-1a 的 subnet 當中。將 routing table 導入到 web-1a 的 subnet 當中。接著我們去 subnets 的頁面看，就可以看到 web-1a 的 routing table 中多了一個對外的 0.0.0.0/0 的 content，此時我們就可以知道所有非 172.16.0.0/16 的流量都會被導入到 igw 做轉送。建立 ACL 與 SG參考文章: 什麼是 ACL ， 什麼是 SG做完上述的步驟以後，我們可以思考，在一個沒有安全機制的狀況下，所有 IP, Port 都可以連進到一個公開的 server 中，是否不太合理，因此本節的目標就是將我們的網路多設一層防護網，我們將只允許 SSH port 22, 可以流進我們的 EC2 當中，當然之後若是你想要再多開 80, 443, 8888 都可以。點選 Security Group, Create security group.建立 jackysidepojrect-web-sg，Inbound rules 設定 22 port， source 這裡設定 0.0.0.0/0，但其實這是不好的，比較佳的方式是輸入你自己的 IP，增加安全控管程度。另外小 tips，aws 的 inbound 策略是，進得來的流量都出得去，大家可以去看參考文章有詳細講解喔。建立 EC2參考文章: 什麼是 EC2安全跟基礎的網路建立起來後，我們就可以 create 我們的 EC2 instance 了。在這裡我們選擇的是 Amazon Linux 2 AMI, 當然你也可以選擇 CentOS 或者 Ubuntu 都可以。接著我們選擇免費的 Instance。選擇我們前面建立好的 VPC, 以及我們對外的 web-1a subnet，另外 enable Public IP 這樣我們才可以從外網設備連的到 AWS 的網路選擇 private IP設定 tag name選擇我們剛剛所建立的 Security Group，增加我們的安全性。若是你是第一次使用 AWS 就選擇 create new key, 然後下載下來，下圖是我已經建立好的 ssh key，記得 ssh key 記得保存好喔。確認 download 後儲存的位置。# Setting your file permission chmod 400 aws_ssh_key.pem到剛剛 EC2 的地方，找一下你的 public IP，這個部分我碼起來了，哈哈，就是黑色框框的那個，把它記錄下來。# SSH your EC2 ssh -i aws_ssh_key.pem ec2-user@Your_EC2_Public_IP登入成功。下集預告下次會把 EC2 的 web 服務建立起來，以及連線到 private 的 DB，開始做一些有趣的應用。Post converted from Medium by ZMediumToMarkdown." }, { "title": "透過 K8S 建立 NFS 服務", "url": "/posts/6bbe4aeaf2cf/", "categories": "Jackycsie", "tags": "kubernetes, ceph, nfs-server, k8s", "date": "2020-09-25 09:48:22 +0800", "snippet": "透過 K8S 建立 NFS 服務本文將介紹，透過 kubernetes 建立 NFS 服務，而 Storage class 我們將會使用 CEPH RBD 做儲存空間。下圖是建立在 kubernetes 建立一個 NFS 服務時，所使用的流程圖。參考資料行前注意 建立好 kubernetes 集群( 建立集群文章 )。 在 kubernetes 已擁有 CEPH 服務( 使用 Rook ...", "content": "透過 K8S 建立 NFS 服務本文將介紹，透過 kubernetes 建立 NFS 服務，而 Storage class 我們將會使用 CEPH RBD 做儲存空間。下圖是建立在 kubernetes 建立一個 NFS 服務時，所使用的流程圖。參考資料行前注意 建立好 kubernetes 集群( 建立集群文章 )。 在 kubernetes 已擁有 CEPH 服務( 使用 Rook 建立 CEPH 儲存架構 )。Outline Step 1: 建立 StorageClass Step 2: 建立 PVC 與 StorageClass 連接 Step 3: Deploy NFS Operator Step 4: 建立 Pod 與 PVC 連接 Step 5: 讓外部機器可以連接到 nfs serverStep 1: 建立 StorageClass透過 yaml 檔建立 CEPH RBD StorageClass。$ kubectl create -f rook/cluster/examples/kubernetes/ceph/csi/rbd/storageclass.yamlapiVersion: ceph.rook.io/v1kind: CephBlockPoolmetadata: name: replicapool namespace: rook-cephspec: failureDomain: host replicated: size: 3 # Disallow setting pool with replica 1, this could lead to data loss without recovery. # Make sure you're *ABSOLUTELY CERTAIN* that is what you want requireSafeReplicaSize: true # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size #targetSizeRatio: .5---apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: rook-ceph-block# Change \"rook-ceph\" provisioner prefix to match the operator namespace if neededprovisioner: rook-ceph.rbd.csi.ceph.comparameters: # clusterID is the namespace where the rook cluster is running # If you change this namespace, also change the namespace below where the secret namespaces are defined clusterID: rook-ceph # If you want to use erasure coded pool with RBD, you need to create # two pools. one erasure coded and one replicated. # You need to specify the replicated pool here in the `pool` parameter, it is # used for the metadata of the images. # The erasure coded pool must be set as the `dataPool` parameter below. #dataPool: ec-data-pool pool: replicapool # RBD image format. Defaults to \"2\". imageFormat: \"2\" # RBD image features. Available for imageFormat: \"2\". CSI RBD currently supports only `layering` feature. imageFeatures: layering # The secrets contain Ceph admin credentials. These are generated automatically by the operator # in the same namespace as the cluster. csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph # Specify the filesystem type of the volume. If not specified, csi-provisioner # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock # in hyperconverged settings where the volume is mounted on the same node as the osds. csi.storage.k8s.io/fstype: ext4# uncomment the following to use rbd-nbd as mounter on supported nodes# **IMPORTANT**: If you are using rbd-nbd as the mounter, during upgrade you will be hit a ceph-csi# issue that causes the mount to be disconnected. You will need to follow special upgrade steps# to restart your application pods. Therefore, this option is not recommended.#mounter: rbd-nbdallowVolumeExpansion: truereclaimPolicy: DeleteStep 2: 建立 PVC 與 StorageClass 連接$ kubectl create -f ceph-nfs-pvc.yamlapiVersion: v1kind: Namespacemetadata: name: rook-nfs---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: nfs-ceph-claim namespace: rook-nfsspec: storageClassName: rook-ceph-block accessModes: - ReadWriteOnce resources: requests: storage: 700GiStep 3: Deploy NFS Operator$ git clone https://github.com/rook/rook.git$ cd rook/cluster/examples/kubernetes/nfs$ kubectl create -f common.yaml$ kubectl create -f provisioner.yaml$ kubectl create -f operator.yamlStep 4: 建立 Pod 與 PVC 連接先加入 ServiceAccount，這部分蠻重要的，因為官網是沒有提到需要加入 Service Account 的，但是若是沒有設定的話，就會沒有權限建立 POD。kubectl apply -f - &lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: rook-nfs-server namespace: rook-nfsEOF$ kubectl create -f nfs-ceph-pod.yamlapiVersion: nfs.rook.io/v1alpha1kind: NFSServermetadata: name: rook-nfs namespace: rook-nfsspec: replicas: 1 type: NodePort ports: - targetPort: 111 exports: - name: nfs-share server: accessMode: ReadWrite squash: \"none\" # A Persistent Volume Claim must be created before creating NFS CRD instance. # Create a Ceph cluster for using this example # Create a ceph PVC after creating the rook ceph cluster using ceph-pvc.yaml persistentVolumeClaim: claimName: nfs-ceph-claimStep 5: 讓外部機器可以連接到 NFS server當我們 NFS 建立完成後，我們已經擁有對內部的分享資料的功能了，但我們時常需要讓外部不屬於 Cluster 的機器連到 container 做事情，這時候就必須透過 NodePort 開啟 Port ，讓外部的機器可以連進 NFS service 中。$ kubectl -n rook-nfs get svc -o yaml &gt;&gt; rook-nfs-svc-backup.yaml$ cp rook-nfs-svc-backup.yaml rook-nfs-svc.yaml$ vim rook-nfs-svc.yaml$ kubectl apply -f rook-nfs-svc.yaml$ kubectl -n rook-nfs get all -o wide外部 Port 就開好了，分別是 30377, 31431。換到其他想連接到 nfs 的機器上。$ apt-get install nfs-common$ mkdir -p /mnt/nfs/var/nfsshare$ mount -t nfs -o port=30377 master_node_IP:/ /mnt/nfs/var/nfsshare/參考資料 https://rook.io/docs/rook/v1.4/ceph-block.html https://rook.io/docs/rook/v1.4/nfs.html https://ithelp.ithome.com.tw/articles/10195944 https://www.opencli.com/linux/debian-ubuntu-install-nfs-server https://godleon.github.io/blog/Kubernetes/k8s-Config-StorageClass-with-NFS/Post converted from Medium by ZMediumToMarkdown." }, { "title": "LeetCode Counting Bits", "url": "/posts/6981cd89fa73/", "categories": "Jackycsie", "tags": "leetcode, python", "date": "2020-09-22 21:58:05 +0800", "snippet": "[LeetCode] Counting BitsGiven a non negative integer number num . For every numbers i in the range 0 ≤ i ≤ num calculate the number of 1’s in their binary representation and return them as an array...", "content": "[LeetCode] Counting BitsGiven a non negative integer number num . For every numbers i in the range 0 ≤ i ≤ num calculate the number of 1’s in their binary representation and return them as an array.Example 1:Input: 2Output: [0,1,1]Example 2:Input: 5Output: [0,1,1,2,1,2]Follow up: It is very easy to come up with a solution with run time O(n*sizeof(integer) ) . But can you do it in linear time O(n) /possibly in a single pass? Space complexity should be O(n) .Can you do it like a boss? Do it without using any builtin function like _ _builtin_popcount in c+ + or in any other language.解題想法：這題的題目是輸入一個 10 進制的數，然後印出一段 list ，這段 list 是從 0 到 這個數字的每個位數會有多少個 1，下圖有範例。--------------------------10 進制 | 2 進制 | 幾個 1--------------------------0 000 01 001 12 010 13 011 24 100 15 101 26 110 27 111 38 1000 1在這當中我們有發現兩種規律，第一個就是到 8 時，前面的數又全部重來了，所以我們可以按照這個邏輯去寫演算法，第二個是更簡單的方法，當 2 以後，只要他的本身除以 2 就等於了那個數列的 1 的數再加上 餘數 就可以推算出他有幾個 1，例如 初始值 num = [0, 1] 2 = num[int (2 / 2) ] + num[ (2 %2) ] = 2num = [0, 1, 1] 3 = num[int (3 / 2) ] + num[ (3 %2) ] = 2num = [0, 1, 1, 2] 4 = num[int (4 / 2) ] + num[ (4 %2) ] = 1num = [0, 1, 1, 2, 1] 5 = num[int (5 / 2) ] + num[ (5 %2) ] = 2num = [0, 1, 1, 2, 1, 2] 6 = num[int (6 / 2) ] + num[ (6 %2) ] = 2num = [0, 1, 1, 2, 1, 2, 2] 7 = num[int (7 / 2) ] + num[ (7 %2) ] = 3num = [0, 1, 1, 2, 1, 2, 2, 3]非常的好懂對吧，另外要注意的是如果 小於 1 時，初始值記得要設 [0]，即可。class Solution(object): def countBits(self, num): \"\"\" :type num: int :rtype: List[int] \"\"\" if num == 0: return [0] else: result = [0, 1] for i in range(2, num+1, 1): result.append((result[int(i/2)])+result[i%2]) return result Runtime: 76 ms, faster than 59.79% Memory Usage: 16.8 MB, less than 32.64%感想發現有時候，在寫 leetcode 時，再會去思考這題有沒有 pattern 可以依循，覺得有點小可惜，或許這就是刷 leetcode 的原因，透過更多的題目學著讓自己在面對職場業務時，能有相同的思維去思考，而不是一股腦地下去做。參考 [LeetCode] Counting Bits 计数位 Leetcode 338(medium) . 推，圖文並茂Post converted from Medium by ZMediumToMarkdown." }, { "title": "LeetCode 3Sum", "url": "/posts/bb1deec8ba31/", "categories": "Jackycsie", "tags": "leetcode, python", "date": "2020-09-21 21:48:01 +0800", "snippet": "[LeetCode] 3SumGiven an array nums of n integers, are there elements a , b , c in nums such that a + b + c = 0? Find all unique triplets in the array which gives the sum of zero.Notice that the sol...", "content": "[LeetCode] 3SumGiven an array nums of n integers, are there elements a , b , c in nums such that a + b + c = 0? Find all unique triplets in the array which gives the sum of zero.Notice that the solution set must not contain duplicate triplets.Example 1:Input: nums = [-1,0,1,2,-1,-4]Output: [[-1,-1,2],[-1,0,1]]Example 2:Input: nums = []Output: []Example 3:Input: nums = [0]Output: []解題想法：這題本來想寫自己的想法下去，但發現這位大大真的寫得太好了，根本無懈可擊，讓我直接引用她的寫法吧，再次感謝 Fion carry，但她是用 C#，會 C# 的也可以學一下大大的寫法，她的文章有圖文並茂喔，值得大家去看。[Day 7] 演算法刷題 LeetCode 15. 3Sum (Medium) 將 array 從小到大升冪排序 將須要找出的3個數的 index 分別表示為 first , second , third 用 for 迴圈計算，並將 first 做為 nums 的 起始點 second 則為 first + 1 為起始點 third 則為 nums.Length - 1 為起始點 並判斷 nums[first] + nums[second] + nums[third] 是否為 0 若等於 0，則為解答之一，Add 到 List 若小於 0，則代表負數太大，需要將 second 移至下一個較小的負數 (second++) 若大於 0，則代表正數太大，需要將 third 移至上一個較小的正數 (third--)7. 另外判斷 first 是否已經重複，若重複則跳過此次迴圈，因為答案也會是一樣的 如 { -1, -1, 0, 1, 2}8. 另外判斷 second 是否已經重複，若重複則 second+ +，並跳過此次迴圈，因為答案也會是一樣的 如 { -4, 2, 2, 2, 3}7. 與 8. 是讓效能再更好的其中之一條件，不用重複查找已經查找過的數字。 若沒有寫也是會過的哦！簡單好懂版：class Solution(object): def threeSum(self, nums): \"\"\" :type nums: List[int] :rtype: List[List[int]] \"\"\" if len(nums) &lt; 3: return [] nums.sort() print(nums) answer_list = [] for first_position in range(0, len(nums)-2, 1): second_position = first_position + 1 third_position = len(nums) - 1 while (second_position != third_position): total = (nums[first_position] + nums[second_position]) total = total + nums[third_position] if total == 0: answer_list.append([nums[first_position], nums[second_position], nums[third_position]]) while(second_position &lt; third_position): second_position += 1 if(nums[second_position] != nums[second_position-1]): break while(third_position &gt; second_position): third_position -= 1 if(nums[third_position] != nums[third_position+1]): break elif total &lt; 0: second_position += 1 elif total &gt; 0: third_position -=1 answer_list = set(tuple(l) for l in answer_list) answer_list = [list(t) for t in answer_list] return answer_list Runtime: 1648 ms, faster than 13.79% Memory Usage: 17.8 MB, less than 7.12%概念相同，程式縮減版：class Solution(object): def threeSum(self, nums): \"\"\" :type nums: List[int] :rtype: List[List[int]] \"\"\" nums.sort() first=[] i=0 while(i&lt;len(nums)-2): if(nums[i]!=nums[i-1] or i==0): target=0-nums[i] left=i+1 right=len(nums)-1 while(left!=right): if(nums[left]+nums[right]==target): first.append([nums[i],nums[left],nums[right]]) while(left&lt;right): left+=1 if(nums[left]!=nums[left-1]): break while(right&gt;left): right-=1 if(nums[right]!=nums[right+1]): break elif(nums[left]+nums[right]&gt;target): right-=1 elif(nums[left]+nums[right]&lt;target): left+=1 i+=1 return first Runtime: 780 ms, faster than 51.52% Memory Usage: 16 MB, less than 63.27%參考文獻： [Day 7] 演算法刷題 LeetCode 15. 3Sum (Medium) [Day 4] 從LeetCode學演算法 — 0015. 3Sum (Medium) 15. 3Sum 解题报告（Python）Post converted from Medium by ZMediumToMarkdown." }, { "title": "LeetCode Longest Common Prefix", "url": "/posts/bef9cc68d498/", "categories": "Jackycsie", "tags": "leetcode", "date": "2020-09-16 23:23:05 +0800", "snippet": "[LeetCode] Longest Common PrefixWrite a function to find the longest common prefix string amongst an array of strings.If there is no common prefix, return an empty string \"\" .Example 1:Input: [\"flo...", "content": "[LeetCode] Longest Common PrefixWrite a function to find the longest common prefix string amongst an array of strings.If there is no common prefix, return an empty string \"\" .Example 1:Input: [\"flower\",\"flow\",\"flight\"]Output: \"fl\"Example 2:Input: [\"dog\",\"racecar\",\"car\"]Output: \"\"Explanation: There is no common prefix among the input strings.Note:All given inputs are in lowercase letters a-z .解題思考：這題有一些有趣的小問題，感覺在面白板題時一定會問到，他並沒有要求兩個想法，第一個就是如果是在字串中的有相同的連續字串能否抓出，第二個是如果字串很長，有相同數目的字串該如何解決，上述是面試時，感覺百板題面試官一定會問的題。那我們這裡主要解決最基礎的問題，首先我們拿第一個字串的第一個字比對，第 2,3 個字串的第一個字，如果相同就以此類推，如果不同就直接跳出，時間複雜度最好就是 O (1)，最差就是第一個字串的 O(n)。class Solution(object): def longestCommonPrefix(self, strs): \"\"\" :type strs: List[str] :rtype: str \"\"\" if not strs: return '' all_combine = '' for element in range(len(strs[0])): for list_step in range(1, len(strs)): if element &gt;= len(strs[list_step]) or strs[list_step][element] != strs[0][element]: return all_combine all_combine += strs[0][element] return all_combineRuntime: 24 ms, faster than 63.98%.Memory Usage: 12.8 MB, less than 58.38%.有趣的回答，使用 min, max 做 list sort ，再抓 string，非常好理解，但都要先做 sort 會比較消耗時間，程式不好掌握。class Solution(object): def longestCommonPrefix(self, strs): if not strs: return \"\" s1 = min(strs) s2 = max(strs) for i, c in enumerate(s1): if c != s2[i]: return s1[:i] return s1 https://ithelp.ithome.com.tw/articles/10213258結論：有機會的話，會補充上述 2 個類型的百板題，並補充在下來。 字串中是否有更長的相同字母，而非只能從字首。 承上題，如果字串中有多個相同多的字母，需全部印下來。Post converted from Medium by ZMediumToMarkdown." }, { "title": "在 K8S 使用 Rook 安裝 CEPH", "url": "/posts/1999f52a6fb9/", "categories": "Jackycsie", "tags": "ceph, kubernetes, k8s", "date": "2020-09-16 16:42:16 +0800", "snippet": "在 K8S 使用 Rook 安裝 CEPH下圖是 K8S, Rook, CEPH 之間的關係。使用 Rook 安裝完 CEPH 後的示意圖，我們可以使用 PVC 對 CEPH 的 RBD, CephFS, RGW，進行操作，這樣的好處在於本身並未研究 CEPH 的工程師，也可以輕鬆的享受使用 CEPH 所帶來的便利。環境配置: Kubernetes: 1.19 Rook: 1.4 Ce...", "content": "在 K8S 使用 Rook 安裝 CEPH下圖是 K8S, Rook, CEPH 之間的關係。使用 Rook 安裝完 CEPH 後的示意圖，我們可以使用 PVC 對 CEPH 的 RBD, CephFS, RGW，進行操作，這樣的好處在於本身並未研究 CEPH 的工程師，也可以輕鬆的享受使用 CEPH 所帶來的便利。環境配置: Kubernetes: 1.19 Rook: 1.4 Ceph: 15.2.4不能不注意: 使用 lvs, vgs, pvs 查看全部機器有無未刪除 LVM。 先 pvscan 查看非 OS disk 有無 LVM 紀錄。 使用 wipefs -a /dev/sd[b-c] 刪除過去垃圾。Step 1:下載 Rook$ git clone https://github.com/rook/rook.git$ cd rook$ git checkout release-1.4$ cd cluster/examples/kubernetes/cephStep 2: 使用 yaml 安裝 CEPH$ kubectl create -f common.yaml# 檢查 namesapce 是否有 rook-ceph 了$ kubectl get namespace$ kubectl create -f operator.yaml# 上述的步驟必須確定 pods 已經處於 running or complete 才能做下一個階段，否則很有可能會 fail，上述的步驟可以需要等一下子。$ kubectl create -f cluster.yaml# 在 cluster.yaml 中，需要注意的是# 你的 pysical machine IP 必須不能使用 192.168.x.x# 因為這是 kubernetes LAN 使用的 IP，這樣 k8s 會 confuse，導致失敗。# 需要非常久的時間，請耐心等待.....Step 3: 建立 Toolbox Pod 查看 CEPH 細節$ kubectl create -f toolbox.yaml$ kubectl -n rook-ceph get pods | grep ceph-toolsrook-ceph-tools-649c4dd574-gw8tx 1/1 Running 0 3m20s$ kubectl -n rook-ceph exec -it rook-ceph-tools-649c4dd574-gw8tx bash$ ceph -scluster: id: 9ca03dd5-05bc-467f-89a8-d3dfce3b9430 health: HEALTH_OK services: mon: 3 daemons, quorum a,d,e (age 12m) mgr: a(active, since 8m) osd: 44 osds: 44 up (since 13m), 44 in (since 13m) data: pools: 1 pools, 1 pgs objects: 0 objects, 0 B usage: 45 GiB used, 19 TiB / 19 TiB avail pgs: 1 active+clean# ceph 集群可以使用的容量$ ceph df# ceph osd 與 node 的關係分布$ ceph osd treeStep 4: 使用 Dashboard 查看 Ceph 內容在這邊我們會直接使用外部的 PC 連進 k8s service 當中，來查看 k8s 的內容。$ vim dashboard-external-https.yamlapiVersion: v1kind: Servicemetadata: name: rook-ceph-mgr-dashboard-external-https namespace: rook-ceph labels: app: rook-ceph-mgr rook_cluster: rook-cephspec: ports: - name: dashboard port: 8443 protocol: TCP targetPort: 8443 selector: app: rook-ceph-mgr rook_cluster: rook-ceph sessionAffinity: None type: NodePort$ kubectl create -f dashboard-external-https.yaml$ kubectl -n rook-ceph get service打開 PC 瀏覽器輸入 https://master_IP:31955/，這個時候會需要輸入帳號密碼，帳號是 admin，密碼需要輸入下面命令。$ kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=\"{['data']['password']}\" | base64 --decode &amp;&amp; echoOne more thing刪除 CEPH 集群$ cd /rook/cluster/examples/kubernetes/ceph$ kubectl -n rook-ceph delete cephcluster rook-ceph$ kubectl -n rook-ceph get cephcluster# 確認 rook-ceph 被刪除$ kubectl delete -f operator.yaml參考資料: 官方 Rook 對 CEPH 的介紹 官方安裝教學 官方移除 CEPH 教學 Kubernetes上使用Rook部署Ceph系统并提供PV服务Post converted from Medium by ZMediumToMarkdown." }, { "title": "使用 Kubeadm 安裝 K8S", "url": "/posts/abe1631aa600/", "categories": "Jackycsie", "tags": "kubernetes, kubectl, kubeadm, k8s", "date": "2020-09-15 16:16:43 +0800", "snippet": "使用 Kubeadm 佈署 K8S 集群本文將簡單介紹如何透過 kubeadm 手動安裝 k8s 集群，因為 k8s 本身的教學文件時在寫得太好了，所以其實也可以去 官網 看，這裡是順便紀錄我本身碰到的坑。實驗環境 OS: ubuntu 18.04 k8s: 1.19 docker-engine version: 19.03.12Outline: Step 1: Delete SWA...", "content": "使用 Kubeadm 佈署 K8S 集群本文將簡單介紹如何透過 kubeadm 手動安裝 k8s 集群，因為 k8s 本身的教學文件時在寫得太好了，所以其實也可以去 官網 看，這裡是順便紀錄我本身碰到的坑。實驗環境 OS: ubuntu 18.04 k8s: 1.19 docker-engine version: 19.03.12Outline: Step 1: Delete SWAP (all machine) Step 2: Install docker (all machine) Step 3: Install Kubectl Kubeadm Kubelet (all machine) Step 3–1: Init k8s master Step 4: Install CNI (master) Step 5: Apply cluster node (all node)Step 1: Delete SWAP在安裝 k8s 前，必須把所有 node 的 swap disable 。$ sudo swapoff -a$ sudo vim /etc/fstab# /swapfile ... ...Step 2: Install docker在所有的 node 的中 install dockersudo apt-get updatesudo apt-get install apt-transport-https ca-certificates curl software-properties-common -ycurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\"sudo apt-get updatesudo apt-get install docker-ce -ycat &gt; /etc/docker/daemon.json &lt;&lt;EOF{ \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\"}EOFmkdir -p /etc/systemd/system/docker.service.dsystemctl daemon-reloadsystemctl restart dockerdocker versionStep 3: Install Kubectl Kubeadm Kubelet首先在所有節點安裝下面指令sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https curlcurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -cat &lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.listdeb https://apt.kubernetes.io/ kubernetes-xenial mainEOFsudo apt-get updatesudo apt-get install -y kubelet kubeadm kubectlsudo apt-mark hold kubelet kubeadm kubectl安裝完後，確認版本$ kubectl version$ kubeadm version$ kubelet version控制版本# 將 kubelet kubeadm kubectl mark 起來，不要讓 Node 自己升級。$ sudo apt-mark hold kubelet kubeadm kubectl$ kubeadm config images pullStep 3–1 : 在 master 中初始化 kubeadmkubeadm init接著會出現 Your Kubernetes control-plane has initialized successfully! 以及可以加入其他節點的 command 。$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config現在下 command，Status 會顯示 NotReady ，因為還沒安裝 CNI。$ kubectl get nodes 看一下目前現有的 pods 有哪些。$ kubectl -n kube-system get podsStep 4: Install CNI接著安裝 CNI ，我所使用的是 Calico 因為這是官方建議的$ curl https://docs.projectcalico.org/manifests/calico.yaml -O$ kubectl apply -f calico.yaml$ kubectl -n kube-system get pod -w$ kubectl get nodesStep 5: Apply cluster node在每一個 node 打上剛剛 kubeadm init 後出現的 command 。$ kubeadm join 172.16.96.13:6443 \\ — token 6bxsn0.1baqaetwjzwqbvii \\ — discovery-token-ca-cert-hash \\ sha256:b66de07029fb42d7a17d0cf01bbea5953445892ab5d9870afadfdab6b76f5bc5# 如果 token 不見得話，可以使用$ kubeadm token list# 另外若是 token 過期的時候，就可以使用$ kubeadm token create --print-join-command最後可以使用下面 command 看一下目前有的元件服務細節。$ kubectl -n kube-system get all刪除 Kubenetes cluster$ kubeadm reset$ sudo apt-get purge kubeadm kubectl kubelet kubernetes-cni kube* $ sudo apt-get autoremove $ sudo rm -rf ~/.kube參考資料: 官方網站 解說超詳細的 k8s kubeadm 介紹 Calico 官方網站Post converted from Medium by ZMediumToMarkdown." }, { "title": "LeetCode Roman to Integer", "url": "/posts/2f1d9374730b/", "categories": "Jackycsie", "tags": "leetcode, python", "date": "2020-09-10 21:00:56 +0800", "snippet": "[LeetCode] Roman to IntegerRoman numerals are represented by seven different symbols: I , V , X , L , C , D and M .Symbol ValueI 1V 5X 10L 50C ...", "content": "[LeetCode] Roman to IntegerRoman numerals are represented by seven different symbols: I , V , X , L , C , D and M .Symbol ValueI 1V 5X 10L 50C 100D 500M 1000Example 1:Input: \"III\"Output: 3Example 2:Input: \"IV\"Output: 4Example 3:Input: \"IX\"Output: 9Example 4:Input: \"LVIII\"Output: 58Explanation: L = 50, V= 5, III = 3.Example 5:Input: \"MCMXCIV\"Output: 1994Explanation: M = 1000, CM = 900, XC = 90 and IV = 4.解題思路：個人覺得剛開始覺得這題在程式邏輯上，蠻無趣的，因為難的不在寫 code ，在於尋找 pattern ，但仔細沈澱了一下，發現難道這世界不就是這樣嗎？使用者的消費習慣，做事的邏輯都是透過個人尋找的 pattern，再去做 rule-based 研究出來的，看來我還是太嫩了，輕瞧了這題了，陷入個人的思維陷阱了。 那這題的思路就是有兩種，從後向前掃描，遇到前面數大於等於後面的最大數的時候，相加；遇到前面數小於後面的最大數的時候，相減。 從前向後掃描，遇到後面數大於等於前面的最大數的時候，相減；遇到後面數小於前面的最大數的時候，相加。另外可以知道一個特性，當羅馬數字會減時，只會看當下的那個數字跟他右邊的數字，但相加可以一直往右加好幾個。class Solution(object): def romanToInt(self, s): \"\"\" :type s: str :rtype: int \"\"\" roman_dict = {'I':1,'V':5,'X':10,'L':50,'C':100,'D':500,'M':1000} count_sum = 0 # pass_step 的目的是為了可以快速紀錄相減的數字，讓未來不在計算這個字母。 pass_step = 999 for step in range(len(s)): if pass_step == step: continue now_step_value = roman_dict.get(s[step]) if len(s)-1 != step: if roman_dict.get(s[step+1]) &gt; now_step_value: count_sum = count_sum + roman_dict.get(s[step+1]) - now_step_value pass_step = step + 1 else: count_sum = count_sum + now_step_value else: count_sum = count_sum + now_step_value return count_sum Runtime: 36 ms, faster than 89.55% Memory Usage: 12.7 MB, less than 45.32%右邊算到左邊class Solution(object): def romanToInt(self, s): res, prev = 0, 0 dict = {'I':1, 'V':5, 'X':10, 'L':50, 'C':100, 'D':500, 'M':1000} for i in s[::-1]: # rev the s if dict[i] &gt;= prev: res += dict[i] # sum the value iff previous value same or more else: res -= dict[i] # substract when value is like \"IV\" --&gt; 5-1, \"IX\" --&gt; 10 -1 etc prev = dict[i] return res Runtime: 40 ms, faster than 80.13% Memory Usage: 12.7 MB, less than 47.16%參考： Roman to Integer [easy] (Python)Post converted from Medium by ZMediumToMarkdown." }, { "title": "LeetCode Palindrome Number", "url": "/posts/f61d3f6be611/", "categories": "Jackycsie", "tags": "leetcode, python", "date": "2020-09-09 23:30:20 +0800", "snippet": "[LeetCode] Palindrome NumberDetermine whether an integer is a palindrome. An integer is a palindrome when it reads the same backward as forward.Example 1:Input: 121Output: trueExample 2:Input: -121...", "content": "[LeetCode] Palindrome NumberDetermine whether an integer is a palindrome. An integer is a palindrome when it reads the same backward as forward.Example 1:Input: 121Output: trueExample 2:Input: -121Output: falseExplanation: From left to right, it reads -121. From right to left, it becomes 121-. Therefore it is not a palindrome.Example 3:Input: 10Output: falseExplanation: Reads 01 from right to left. Therefore it is not a palindrome.解題思路：首先我們可以知道只要是負的就可以傳 false ，接著判斷字串是基數偶數，如果是偶數直接對半切，拿第一個比對最後一個，依此類推，如果是基數，直接忽略最中間的那個。若是中間任何一個只要不同就直接傳 false 。這樣最佳時間複雜度是 O(1)，最差則是 O(n/2)。class Solution(object): def isPalindrome(self, x): if x &lt; 0: return False input_string = str(x) input_length = len(input_string) input_length_mod = input_length %2 tail_tmp = 0 for step in range((input_length - input_length_mod)/2): tail_tmp -= 1 if (input_string[step] == input_string[tail_tmp]): pass else: return False return True Runtime: 48 ms, faster than 86.46% Memory Usage: 12.8 MB, less than 43.23%解題思路二：將字串反轉後，跟原本的數字做比對，但這樣的時間複雜度一定是 O(n)，不是非常聰明的作法。class Solution(object): def isPalindrome(self, x): if x &lt; 0: return False reserve_number = 0 input_string = x while(input_string != 0): temp = input_string % 10 reserve_number = reserve_number * 10 + temp input_string = int(input_string / 10) if x &gt;= 0 and x == reserve_number: return True else: return False Runtime: 52 ms, faster than 79.30% Memory Usage: 12.7 MB, less than 80.05%Post converted from Medium by ZMediumToMarkdown." }, { "title": "LeetCodeReverse Integer", "url": "/posts/d0e35096050a/", "categories": "Jackycsie", "tags": "leetcode, python", "date": "2020-09-08 23:31:10 +0800", "snippet": "[LeetCode] Reverse IntegerGiven a 32-bit signed integer, reverse digits of an integer.Example 1:Input: 123Output: 321Example 2:Input: -123Output: -321Example 3:Input: 120Note: Assume we are dealing...", "content": "[LeetCode] Reverse IntegerGiven a 32-bit signed integer, reverse digits of an integer.Example 1:Input: 123Output: 321Example 2:Input: -123Output: -321Example 3:Input: 120Note: Assume we are dealing with an environment which could only store integers within the 32-bit signed integer range: [−231, 231 − 1] . For the purpose of this problem, assume that your function returns 0 when the reversed integer overflows.解題這題其實蠻愜意的，需要在乎的是最後的尾數 0 與正負數就可以解決此問題，另外要注意的是 32-bit 的小陷阱，如果超過正負 2147483648，就會傳 0 這點需要注意。那本題的思維是 check 正負，轉 string ，從最後面讀回來，用 string 的方法相加，就可以囉。class Solution(object): def reverse(self, x): input_string = str(abs(x)) input_string_length = len(input_string) if x &lt; 0: answer = -1 else: answer = 1 return_answer = \"\" for step in range(input_string_length-1,-1,-1): reverse_tmp = input_string[step] return_answer += reverse_tmp return_answer = int(return_answer) * answer return return_answer if return_answer &lt; 2147483648 and return_answer &gt;= -2147483648 else 0 Runtime: 28 ms, faster than 37.79% Memory Usage: 12.7 MB, less than 49.07%另外如果比較 high-level 的 programer 其實可以使用下面方法，蠻不錯的，我個人也比較 prefer 這種寫法，不管是 code review 或者 memory 上都可以節省多一點的資源跟時間，而且又清晰易懂。class Solution: def reverse(self, x): rev = int(str(abs(x))[::-1]) return (-rev if x &lt; 0 else rev) if rev.bit_length() &lt; 32 else 0 Runtime: 24 ms, faster than 60.45% Memory Usage: 12.6 MB, less than 90.82%好文推推： Leetcode 小分享 — Reverse IntegerPost converted from Medium by ZMediumToMarkdown." }, { "title": "LeetCode Two Sum", "url": "/posts/b7b40ce61d39/", "categories": "Jackycsie", "tags": "leetcode, hashable", "date": "2020-09-07 23:54:02 +0800", "snippet": "[LeetCode] Two SumGiven an array of integers nums and an integer target , return indices of the two numbers such that they add up to target .You may assume that each input would have exactly one so...", "content": "[LeetCode] Two SumGiven an array of integers nums and an integer target , return indices of the two numbers such that they add up to target .You may assume that each input would have exactly one solution , and you may not use the same element twice.You can return the answer in any order.Example 1:Input: nums = [2,7,11,15], target = 9Output: [0,1]Output: Because nums[0] + nums[1] == 9, we return [0, 1].Example 2:Input: nums = [3,2,4], target = 6Output: [1,2]Example 3:Input: nums = [3,3], target = 6Output: [0,1]解答其實最簡單的想法，兩個 for 迴圈就可以解決了，但時間複雜度非常的高 O(n²)。class Solution(object): def twoSum(self, nums, target): for i in range(len(nums)): for j in range(len(nums)): if (nums[i]+nums[j]==target) and ( i != j): return [i, j] Runtime: 6696 ms, faster than 5.46% Memory Usage: 13.9 MB, less than 60.44%第二種方法就是使用 hash table ，因為題目有說您可以假定每個輸入都只有一個解決方案，並且您可能不會兩次使用同一元素，所以不會有 collision 的問題。Hash Table 的核心概念，主要有兩個，第一個就是快速索引通常只需要 O(1)的時間就可以索引到，第二個就是降低記憶體空間浪費，但是 hash table 使用時還是要非常注意，例如下列的範例就是 collision 就會很刺激了，所以通常 hash table 放的時候會有兩種策略 linear probing 及 quadratic probing。 h(26)=26 mod 6=2 h(50)=50 mod 6=2class Solution(object): def twoSum(self, nums, target): hashtable_tmp = {} for i in range(len(nums)): answer_tmp = target - nums[i] if (answer_tmp in hashtable_tmp) and (hashtable_tmp[answer_tmp] != i): return (i, hashtable_tmp[answer_tmp]) hashtable_tmp[nums[i]] = i Runtime: 36 ms, faster than 88.29%. Memory Usage: 14 MB, less than 39.79%.One more thing本題的目標主要是想呈現，演算法不同導致明顯的速度差別，所以 faster 超過多少 % 倒是不是非常重要了…. ，那在 memory 方面因為我們需要多一個 table 儲存自然的 memory 使用上就會較多。參考資料 Hash Table：Intro(簡介) 白話的Hash Table 簡介 — TechBridge 技術共筆部落格 設計高效能的Hash Table（一） Ceph殺手鐧CRUSH和主流分布式存儲一致性哈希算法 LeetCode (1) Two Sum (python) [Day 2] 從LeetCode學演算法 — 0001. Two Sum (Easy)Post converted from Medium by ZMediumToMarkdown." }, { "title": "寫點 LeetCode 吧 !", "url": "/posts/17f4b68ff664/", "categories": "Jackycsie", "tags": "life, leetcode", "date": "2020-09-01 21:58:12 +0800", "snippet": "寫點 LeetCode 吧 !想法本篇文章感謝 Dr. Simon 有一天在群組中傳了一句很平凡的話：我們是一直用 1 年的工作經驗過這 3 年的職場生活？還是今天我又比昨天又進步一點點呢 ？我覺得大家一定會說天天的我都有進步，但這是不是就很像是公園下象棋的老爺爺，每天下棋卻永遠沒辦法當一個職業的棋者？或許，這算是一萬小時的弊病，這個想法我也跟我的 tech lead 討論過，always ...", "content": "寫點 LeetCode 吧 !想法本篇文章感謝 Dr. Simon 有一天在群組中傳了一句很平凡的話：我們是一直用 1 年的工作經驗過這 3 年的職場生活？還是今天我又比昨天又進步一點點呢 ？我覺得大家一定會說天天的我都有進步，但這是不是就很像是公園下象棋的老爺爺，每天下棋卻永遠沒辦法當一個職業的棋者？或許，這算是一萬小時的弊病，這個想法我也跟我的 tech lead 討論過，always have pattern，做機器學習的人都懂，garbage in garbage out，如果只是不斷盲目練習，一定有天花板，因此我們必須清晰的了解在 coding 思維上該如何成長，並且擬出策略，對我未來的程式邏輯能力才可以有所進步。作法晚上想了千條路，白天起來走原路，這是多數人的通病，當然我也是。因此，我會依照下面的基礎作法，慢慢累積自己的經驗，在看哪裡不足，再加進來新的方法，以提升我的能力。 一天盡量維持一個 easy 或者 medium 的題目，寫完並放到 blog 上。 Blog 中不只記錄自己的 code 跟想法，也紀錄寫的效率高或者程式碼寫的漂亮的人，把它記錄起來，並試著講述他的思維。 變數清晰，程式碼順眼。 如果有演算法順便解釋一下演算法的時間複雜度等等。(聖經，持續唸…) 目前想不到了。JUST DO IT簡單的大綱就是這樣，寫太多沒做也是白寫，會順便把之前放在 github 上的 leetcode 備份放過來，做整理。17 加油吧 ！種樹的最佳時間是 10 年前，僅次於它的最佳時間是現在。Post converted from Medium by ZMediumToMarkdown." }, { "title": "Deploy Ceph cache tier & erasure code", "url": "/posts/9132e0ee7018/", "categories": "Jackycsie", "tags": "ceph, storage", "date": "2020-07-10 15:31:51 +0800", "snippet": "Deploy Ceph cache tier &amp; erasure code本篇文章主要紀錄的是如何應用 cache tier 與 erasure code 在 Cephfs 當中。本篇文章將會分 4 個部分撰寫：1. 建立 cache pool，撰寫 crush map rule 將 SSD 與 HDD 分開。2. 將 cephfs pool 建立 ，並使用 erasure code...", "content": "Deploy Ceph cache tier &amp; erasure code本篇文章主要紀錄的是如何應用 cache tier 與 erasure code 在 Cephfs 當中。本篇文章將會分 4 個部分撰寫：1. 建立 cache pool，撰寫 crush map rule 將 SSD 與 HDD 分開。2. 將 cephfs pool 建立 ，並使用 erasure code 與 HDD rule.3. 將 cache tier 與 cephfs pool 合併。4. 刪除 cache tier 與 cephfs 。Step 1.撰寫 crush map 將 SSD 與 HDD 分開 Check SDD 與 HDD 的 OSD.ID 編號ceph osd tree | grep hddceph osd tree | grep ssd 先將 crush map decodeceph osd getcrushmap -o crushmapdumpcrushtool -d crushmapdump -o crushmapdump-decompiled vim crushmapdump-decompiled 開始撰寫 rule。第一步區分 SSD block 跟 HDD blockroot ssd { id -40 # do not change unnecessarily # weight 0.546 alg straw2 hash 0 # rjenkins1 item osd.6 weight 0.182 item osd.7 weight 0.182 item osd.8 weight 0.182}root sata { id -41 # do not change unnecessarily # weight 20.019 alg straw2 hash 0 # rjenkins1 item osd.0 weight 0.455 item osd.1 weight 0.455 item osd.2 weight 0.455 item osd.3 weight 0.455 item osd.4 weight 0.455 item osd.5 weight 0.455 item osd.9 weight 0.455 item osd.10 weight 0.455 item osd.11 weight 0.455 item osd.12 weight 0.455 item osd.13 weight 0.455 item osd.14 weight 0.455 item osd.15 weight 0.455 item osd.16 weight 0.455 item osd.17 weight 0.455 item osd.18 weight 0.455 item osd.19 weight 0.455 item osd.20 weight 0.455 item osd.21 weight 0.455 item osd.22 weight 0.455 item osd.23 weight 0.455 item osd.24 weight 0.455 item osd.25 weight 0.455 item osd.26 weight 0.455 item osd.27 weight 0.455 item osd.28 weight 0.455 item osd.29 weight 0.455 item osd.30 weight 0.455 item osd.31 weight 0.455 item osd.32 weight 0.455 item osd.33 weight 0.455 item osd.34 weight 0.455 item osd.36 weight 0.455 item osd.37 weight 0.455 item osd.38 weight 0.455 item osd.39 weight 0.455 item osd.40 weight 0.455 item osd.41 weight 0.455 item osd.42 weight 0.455 item osd.43 weight 0.455 item osd.44 weight 0.455 item osd.45 weight 0.455 item osd.46 weight 0.455 item osd.47 weight 0.455} 第二步撰寫 tier 所需要的 rule。rule ssd-pool { id 1 type replicated min_size 1 max_size 10 step take ssd step chooseleaf firstn 0 type osd step emit}rule sata-pool { id 2 type erasure # very import. if you want use erasure. You must set this type. min_size 1 max_size 10 step take sata step chooseleaf firstn 0 type osd step emit}Crush map 都寫完以後 encode 然後 set 到 system 中.crushtool -c crushmapdump-decompiled -o crushmapdump-compiledceph osd setcrushmap -i crushmapdump-compiled第三步建立 cache-pool# create cache-poolceph osd pool create cache-pool 32 32Step 2.將 cephfs pool 建立 ，並使用 erasure code 與 HDD rule.# 建立 cold storage 的 pool，若不需要 erasure 可以刪除，但在 crush map 也需要更改。ceph osd pool create cephfs_data 1024 1024 erasureStep 3.將 cache tier 與 cephfs pool 合併。# 將兩個 pool 合併，cephfs_data 為 cold storage，cache-pool 為 hot storage。ceph osd tier add cephfs_data cache-pool# 設定 cache-pool 的模式為 writeback 模式(其實還有readonly 但不推)。ceph osd tier cache-mode cache-pool writeback因為本篇應用的方法是使用 cephfs 所以接著建立 cephfs。# cephfs 還是需要 metadata 的 pool(雖然用不到)ceph osd pool create cephfs_metadata 1024 1024完成 ! !有興趣的可以玩一下，當中也有很多 cache-tier 調優的方法。ceph osd pool set cache-pool hit_set_type bloomceph osd pool set cache-pool hit_set_count 1ceph osd pool set cache-pool hit_set_period 3000ceph osd pool set cache-pool target_max_bytes 161061273600ceph osd pool set cache-pool target_max_objects 1000000ceph osd pool set cache-pool cache_min_flush_age 600ceph osd pool set cache-pool cache_min_evict_age 1800ceph osd pool set cache-pool cache_target_dirty_ratio .6ceph osd pool set cache-pool cache_target_dirty_high_ratio .7ceph osd pool set cache-pool cache_target_full_ratio .8另外小 tips 可以透過指令觀察 cache tier 的變化。watch -n1 ceph df Step 4.刪除 cache-tier 與 cephfs。在做這個步驟時必須十分警慎，不然處理起來會複雜很多，自己實測按照下面的步驟可以減少很多冤枉路。# Check cache-pool 中有沒有 dirty objectceph df detail# 如果有 dirty object 請使用，若沒有可忽略，會花一段時間。rados -p cache-pool cache-flush-evict-all# 將 writeback 模式先設定成 readproxy，再改成 none 模式，它無法像 readonly 一樣可以直接改成 none 模式。# ceph osd tier cache-mod cache-pool forward(研究中)ceph osd tier cache-mode cache-pool readproxyceph osd tier cache-mode cache-pool none 刪除 cephfs ，因為個人實測無法直接在 tier 刪除 cephfs_data pool 必須先移除 cephfs 才能解除 cache-tier 的相依性。# 關閉 mdssystemctl stop ceph-mds@ceph-007# 關閉 cephfsceph fs fail cephfs 移除 cache tier 的 cold-storage 以及 cache tierceph osd tier remove-overlay cephfs_dataceph osd tier remove cephfs_data cache-pool結束了 ! !現在大家都回復自由之身了， cold storge pool 的內容都還保存，沒有問題。大致上就結束了，有任何問題歡迎討論。參考資料 : Ceph 官網 Cache tier tuning 管理 cache tier cache tier 介紹與基礎教學 SUSE 教學Post converted from Medium by ZMediumToMarkdown." }, { "title": "MLflow: 紀錄您 Train 步驟的平台", "url": "/posts/c239929d2176/", "categories": "Jackycsie", "tags": "mlflow, machine-learning, automation, jupyter, tools", "date": "2020-05-18 20:08:38 +0800", "snippet": "MLflow: 紀錄您 Train 步驟的平台過往，我們在做機器學習時，總會碰到以下幾個問題。 我們在 train model 時，我們總是忘記選過那些超參數。 剛剛前一個 model 訓練的效果不錯，它的參數為何 ，想保留 model 又遭到覆蓋，只能重 train 。 過了幾週，我們想要重現前一次好的訓練結果，但是我們的資料集已經不見了。 當團隊在 co-work 時，是不是因為...", "content": "MLflow: 紀錄您 Train 步驟的平台過往，我們在做機器學習時，總會碰到以下幾個問題。 我們在 train model 時，我們總是忘記選過那些超參數。 剛剛前一個 model 訓練的效果不錯，它的參數為何 ，想保留 model 又遭到覆蓋，只能重 train 。 過了幾週，我們想要重現前一次好的訓練結果，但是我們的資料集已經不見了。 當團隊在 co-work 時，是不是因為不同的環境導致有不同的結果出現 ？這些問題，也困擾了我非常長一段段時間，而這些問題可能也困擾著你，或許這篇文章，會是您不錯的解決方案。那這個工具的名字叫做 MLflow。MLflow 是由 Databricks 開發而成，那 Databricks 旗下也開發了許多 amazing 的 工具，像是 Spark 也是由同一個創辦人開發而成的。1. 環境 Demo在本篇的分享中，將會在 container 中進行，另外也會在 jupyter 與 CLI 中分別分享，它們的結果。2. 安裝 MLflowpip3 install mlflow3. 快速開始首先我們使用官方建議的 code 來練練手。import osfrom mlflow import log_metric, log_param, log_artifactif __name__ == \"__main__\": # Log a parameter (key-value pair) log_param(\"param1\", 5) # Log a metric; metrics can be updated throughout the run log_metric(\"foo\", 1) log_metric(\"foo\", 2) log_metric(\"foo\", 3) # Log an artifact (output file) with open(\"output.txt\", \"w\") as f: f.write(\"Hello world!\") log_artifact(\"output.txt\") 使用 python 執行 quick_start.py file. 在 mnt 資料夾中，開啟 MLflow 服務。mlflow server --host 0.0.0.0 在網址列中輸入 server 的 ip 以及 port 號，例如 http://127.0.0.1:5000，本文用的是 5555 是因為我們是 docker 環境。有開啟的畫面以及資料的話，代表成功了，若是沒有剛剛的執行結果，可能是您開啟的服務在錯誤的資料夾，跟 quick_start.py 同一層即可。4. 訓練手寫識別在這節，我們透過訓練手寫識別模型，並且調整它的超參數，跟大家分享如何快速上手 MLflow，看完馬上可以直接使用。 變數參數化下面的步驟我們會將幾項超參數，改為 MLflow 可以儲存 log 的方式去撰寫。 Convolutional filter Kernel_size Max pooling Dropout Dense Batch_size Epochsfrom __future__ import print_functionimport kerasfrom keras.datasets import mnistfrom keras.models import Sequentialfrom keras.layers import Dense, Dropout, Flattenfrom keras.layers import Conv2D, MaxPooling2Dfrom keras import backend as K####### mlflow 紀錄的方法，先指定好超參數的變數import mlflow.kerasmlflow.keras.autolog()first_layer_conv = 32first_layer_kernel_size = (3,3)second_layer_conv = 64second_layer_kernel_size = (3,3)pool_size = (2,2)dropout_rate = 0.2dense_neural = 128mlflow.log_param(\"first_layer_conv\", first_layer_conv)mlflow.log_param(\"first_layer_kernel_size\", first_layer_kernel_size)mlflow.log_param(\"second_layer_conv\", second_layer_conv)mlflow.log_param(\"second_layer_kernel_size\", second_layer_kernel_size)mlflow.log_param(\"pool_size\", pool_size)mlflow.log_param(\"dropout\", dropout_rate)mlflow.log_param(\"dense_neural\", dense_neural)batch_size = 128num_classes = 10epochs = 12# 將這些變數放到 model 設置，超參數的地方###### input image dimensionsimg_rows, img_cols = 28, 28# the data, split between train and test sets(x_train, y_train), (x_test, y_test) = mnist.load_data()if K.image_data_format() == 'channels_first': x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) input_shape = (1, img_rows, img_cols)else: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1) input_shape = (img_rows, img_cols, 1)x_train = x_train.astype('float32')x_test = x_test.astype('float32')x_train /= 255x_test /= 255print('x_train shape:', x_train.shape)print(x_train.shape[0], 'train samples')print(x_test.shape[0], 'test samples')# convert class vectors to binary class matricesy_train = keras.utils.to_categorical(y_train, num_classes)y_test = keras.utils.to_categorical(y_test, num_classes)model = Sequential()model.add(Conv2D(first_layer_conv, kernel_size=first_layer_kernel_size, activation='relu', input_shape=input_shape))model.add(Conv2D(second_layer_conv, second_layer_kernel_size, activation='relu'))model.add(MaxPooling2D(pool_size=pool_size))model.add(Dropout(dropout_rate))model.add(Flatten())model.add(Dense(dense_neural, activation='relu'))model.add(Dropout(dropout_rate))model.add(Dense(num_classes, activation='softmax'))model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))score = model.evaluate(x_test, y_test, verbose=0)print('Test loss:', score[0])print('Test accuracy:', score[1]) 訓練 modelpython official_keras_mnist.py 訓練完，連到剛剛的網站，就可以看到多了一個訓練結果與我們剛剛設定 model 時，用到的參數。 點進去專案看實驗的細節。 可以看到列出了非常詳細的參數內容，方便進行回推(回顧)。 在往下滑可以看到，剛剛計算的 accuracy, loss 等等資訊。 點進去看以後就可以查看剛剛訓練每一個步驟的細節。 接著在回到上一頁，往下滑，可以看到環境設定，所使用的工具版本，儲存的 model ，model summary 細節資訊。5. 比較模型上述是我們的第一次 MNIST 訓練，在平常實戰中，我們相同的資料集會訓練非常多不同的超參數，但若本身沒有紀錄，根本不會確切地記得當初下的參數內容如何，本節來跟大家分享我對 mlflow 的心動的原因。因此我們修改了參數 train 了第二個 model，可以從圖中看到我們的第二個結果明顯變差，但是我們只改了參數，這實我們可以回推到底是改了那些參數導致結果變差。 點選(兩個或多個) checkbox，並且按 compare，即可做表格類型比較，黃色的就是我們跟前一個 model 不同的地方，透過這些不同的地方就可以快速回推，我們該如何重現或修改模型。 看數據細節結果，可以更快速的知道，雖然 accuracy 落差蠻大，但是 val_accuracy 並無像 train data 差別如此之大。 透過視覺化比較圖表。Search bar 我們可以像 Kibana 一樣，在 search bar 中快速過濾掉我們不想看的資訊。metrics.val_accuracy &gt; 0.96組織化可以看到，當我們有很多 model 需要訓練或者儲存時，只有一個 table，不但沒有增加理解度，反而把事情搞砸了，這邊想跟大家分享，當我們有好幾個 model 時，我們應該如何將每個 model 的訓練參數放到正確的 table 中。 建立新的 experiment，您可以使用 CLI 或者是 GUI 建立。mlflow experiments create --experiment-name sklearn_train 接著預估會在哪個 experiment 跑實驗下 command 切換紀錄 table。export MLFLOW_EXPERIMENT_NAME=sklearn_train 跑 Training 實驗。# The data set used in this example is from http://archive.ics.uci.edu/ml/datasets/Wine+Quality# P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.# Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.import pandas as pdimport numpy as npfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_scorefrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import ElasticNetimport mlflowimport mlflow.sklearndef eval_metrics(actual, pred): rmse = np.sqrt(mean_squared_error(actual, pred)) mae = mean_absolute_error(actual, pred) r2 = r2_score(actual, pred) return rmse, mae, r2 alphas = [0.5, 0.7, 0.9]l1_ratios = [0.3, 0.5, 0.9]np.random.seed(40)# Read the wine-quality csv file from the URLcsv_url ='http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'try: data = pd.read_csv(csv_url, sep=';')except Exception as e: print(\"Unable to download training &amp; test CSV, check your internet connection. Error: %s\", e)# Split the data into training and test sets. (0.75, 0.25) split.train, test = train_test_split(data)# The predicted column is \"quality\" which is a scalar from [3, 9]train_x = train.drop([\"quality\"], axis=1)test_x = test.drop([\"quality\"], axis=1)train_y = train[[\"quality\"]]test_y = test[[\"quality\"]]for alpha in alphas: for l1_ratio in l1_ratios: with mlflow.start_run(): lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42) lr.fit(train_x, train_y) predicted_qualities = lr.predict(test_x) (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities) mlflow.log_param(\"alpha\", alpha) mlflow.log_param(\"l1_ratio\", l1_ratio) mlflow.log_metric(\"rmse\", rmse) mlflow.log_metric(\"r2\", r2) mlflow.log_metric(\"mae\", mae) mlflow.sklearn.log_model(lr, \"model\")python sklearn_elasticNet.py 可以看到切換紀錄 table 就是如此的輕鬆愜意。 Jupyter 上如何實做 MLflow 追蹤以及組織化的整理呢 ，在這當中有兩種做法。 第一選擇想要跑的 experiment table 接著開啟 jupyter，在執行上述的程式，但這種方法過於麻煩。 第二直接在 jupyter 上選擇想要跑的 experiment table，但小壞處是，所有的 log 必須自己寫儲存方式。 最後，若是只想專注在 1 個 model ，其它儲存的東西都交給 MLflow 處理的話，那使用剛剛上述的 MNIST program 就可以再 jupyter 上執行。下面的程式碼是介紹，第二種方法的 MLflow。from mlflow.tracking import MlflowClientimport mlflowimport mlflow.kerasfrom __future__ import print_functionimport kerasfrom keras.datasets import mnistfrom keras.models import Sequentialfrom keras.layers import Dense, Dropout, Flattenfrom keras.layers import Conv2D, MaxPooling2Dfrom keras import backend as K####### mlflow 紀錄的方法，先指定好超參數的變數，另外設定想要儲存在哪一個 experimentsclient = MlflowClient()experiments = client.list_experiments()run = client.create_run(experiments[1].experiment_id)first_layer_conv = 32first_layer_kernel_size = (3,3)second_layer_conv = 64second_layer_kernel_size = (3,3)pool_size = (2,2)dropout_rate = 0.2dense_neural = 128client.log_param(run.info.run_id, \"first_layer_conv\", first_layer_conv)client.log_param(run.info.run_id, \"first_layer_kernel_size\", first_layer_kernel_size)client.log_param(run.info.run_id, \"second_layer_conv\", second_layer_conv)client.log_param(run.info.run_id, \"second_layer_kernel_size\", second_layer_kernel_size)client.log_param(run.info.run_id, \"pool_size\", pool_size)client.log_param(run.info.run_id, \"dropout\", dropout_rate)client.log_param(run.info.run_id, \"dense_neural\", dense_neural)batch_size = 128num_classes = 10epochs = 1# 將這些變數放到 model 設置，超參數的地方###### input image dimensionsimg_rows, img_cols = 28, 28# the data, split between train and test sets(x_train, y_train), (x_test, y_test) = mnist.load_data()if K.image_data_format() == 'channels_first': x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) input_shape = (1, img_rows, img_cols)else: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1) input_shape = (img_rows, img_cols, 1)x_train = x_train.astype('float32')x_test = x_test.astype('float32')x_train /= 255x_test /= 255print('x_train shape:', x_train.shape)print(x_train.shape[0], 'train samples')print(x_test.shape[0], 'test samples')# convert class vectors to binary class matricesy_train = keras.utils.to_categorical(y_train, num_classes)y_test = keras.utils.to_categorical(y_test, num_classes)model = Sequential()model.add(Conv2D(first_layer_conv, kernel_size=first_layer_kernel_size, activation='relu', input_shape=input_shape))model.add(Conv2D(second_layer_conv, second_layer_kernel_size, activation='relu'))model.add(MaxPooling2D(pool_size=pool_size))model.add(Dropout(dropout_rate))model.add(Flatten())model.add(Dense(dense_neural, activation='relu'))model.add(Dropout(dropout_rate))model.add(Dense(num_classes, activation='softmax'))model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))train_accuracy = history.history['accuracy']test_accuracy = history.history['val_accuracy']train_loss= history.history['loss']test_loss = history.history['val_loss']client.log_metric(run.info.run_id, \"train_accuracy\", train_accuracy[0])client.log_metric(run.info.run_id, \"test_accuracy\", test_accuracy[0])client.log_metric(run.info.run_id, \"train_loss\", train_loss[0])client.log_metric(run.info.run_id, \"test_loss\", test_loss[0])score = model.evaluate(x_test, y_test, verbose=0) 透過上述的第二種方法，就可以使資料科學家快速的在 jupyter 調參，也可以達到我們希望自動記錄 MLflow log 的目標。 最後小提醒，若是想要可以在 jupyter 上快速調參，記得複製我 # # # 上下的 code ，在一個 cell ，在執行 shift + enter，不然它會把你的調參認為是前一份工作的事情，導致誤報。MLflow 再探 到這邊 Mlflow 初探大致上結束，其實 MLflow 還有非常多值得跟大家分享的特色，如實驗資料儲存 DB，data version control，不同版本 model 直接存取，超參數自動選擇儲存，選擇 Model 跑不同的環境等等。 這篇的目標在於讓大家可以快速上手 MLflow ，之後再把上述說特色一個個補齊吧。參考網址 MLflow 官方網站 ( website ) AIA 教學 ( website ) MLflow github ( website ) MLflow 介紹文 ( website )感謝本篇文的產生，還是需要感謝我們 team 願意讓我嘗試與部屬，另外也非常感謝 james 的 k8s ，快速的 training model ，讓我開始思考，如何更自動化，有效率的幫助自己快速釐清，我 train 了幾百個 model 之間，不同參數所代表的意義。當 GPU 的運算力，是你的競爭力時，如何做出藍寶堅尼與布加迪之間的差別，就是我們工程師的能力了。Post converted from Medium by ZMediumToMarkdown." }, { "title": "Ubuntu 透過 Ansible 部署 CEPH", "url": "/posts/6a4af7a1f208/", "categories": "Jackycsie", "tags": "ceph, ansible, ubuntu, filesystem", "date": "2020-04-15 10:29:16 +0800", "snippet": "[Ubuntu] 透過 Ansible 部署 CEPH本篇會介紹如何透過 ceph-ansible 工具安裝一個 ceph 叢集，使用的環境是 ubuntu 18.04 LTS ，一個最簡單的 Ceph 儲存叢集至少要一台 Monitor 與三台 OSD 。而 MDS 是當需要使用到 CephFS 的時候才需要部署。1.環境準備本次安裝會擁有 5 台 node，叢集拓樸圖如下所示：Tips:...", "content": "[Ubuntu] 透過 Ansible 部署 CEPH本篇會介紹如何透過 ceph-ansible 工具安裝一個 ceph 叢集，使用的環境是 ubuntu 18.04 LTS ，一個最簡單的 Ceph 儲存叢集至少要一台 Monitor 與三台 OSD 。而 MDS 是當需要使用到 CephFS 的時候才需要部署。1.環境準備本次安裝會擁有 5 台 node，叢集拓樸圖如下所示：Tips: 每一台 server 都需要安裝，NTP package。$ apt install -y ntp 每一台 hostname 必須先設定成未來 host 連接時一樣的名稱。$ hostnamectl set-hostname {你想要的 hostname} Ansible Node 先設定好可以直接不須密碼連進其他台 node(server) .$ ssh-keygen$ ssh-copy-id root@ceph-node[1-4]2.安裝 Ansible 在 Ceph-Ansible 節點上安裝 ansible 工具。root@ceph-ansible:~# apt-get install -y software-properties-common git cowsay 在 Ceph-Ansible 節點中，輸入以下內容。 透過 ansible ping 指令，檢查是否有設置正確 hosts。root@ceph-ansible:~# ansible all -m ping3.透過 Ceph-Ansible 部屬 Ceph 叢集 下載 ceph-aisible，轉 branch 到 stable-5.0，安裝必要檔案。root@ceph-ansible:~# git clone \"https://github.com/ceph/ceph-ansible.git\"root@ceph-ansible:~# cd ceph-ansibleroot@ceph-ansible:~/ceph-ansible# git checkout -b origin/stable-5.0 ，將 sample 檔轉成 yaml 檔案。root@ceph-ansible:~/ceph-ansible# cp site.yml.sample site.ymlroot@ceph-ansible:~/ceph-ansible# cp group_vars/all.yml.sample group_vars/all.ymlroot@ceph-ansible:~/ceph-ansible# cp group_vars/osds.yml.sample group_vars/osds.yml 修改 group_vars/all.yml 中的配置。50: mon_group_name: mons51: osd_group_name: osds65: configure_firewall: False103: ntp_daemon_type: ntpd125: ceph_origin: repository132: ceph_repository: community146: ceph_mirror: http://download.ceph.com147: ceph_stable_key: https://download.ceph.com/keys/release.asc148: ceph_stable_release: octopus149: ceph_stable_repo: \"{{ ceph_mirror }}/debian-{{ ceph_stable_release }}\"---# Variables here are applicable to all host groups NOT roles# This sample file generated by generate_group_vars_sample.sh# Dummy variable to avoid error because ansible does not recognize the# file as a good configuration file when no variable in it.dummy:# You can override vars by using host or group vars############ GENERAL ################################################### Releases name to number dictionary ########################################ceph_release_num:# dumpling: 0.67# emperor: 0.72# firefly: 0.80# giant: 0.87# hammer: 0.94# infernalis: 9# jewel: 10# kraken: 11# luminous: 12# mimic: 13# nautilus: 14# octopus: 15# pacific: 16# dev: 99# Directory to fetch cluster fsid, keys etc...#fetch_directory: fetch/# The 'cluster' variable determines the name of the cluster.# Changing the default value to something else means that you will# need to change all the command line calls as well, for example if# your cluster name is 'foo':# \"ceph health\" will become \"ceph --cluster foo health\"## An easier way to handle this is to use the environment variable CEPH_ARGS# So run: \"export CEPH_ARGS=\"--cluster foo\"# With that you will be able to run \"ceph health\" normally#cluster: ceph# Inventory host group variablesmon_group_name: monsosd_group_name: osds#rgw_group_name: rgws#mds_group_name: mdss#nfs_group_name: nfss#rbdmirror_group_name: rbdmirrors#client_group_name: clients#iscsi_gw_group_name: iscsigws#mgr_group_name: mgrs#rgwloadbalancer_group_name: rgwloadbalancers#grafana_server_group_name: grafana-server# If configure_firewall is true, then ansible will try to configure the# appropriate firewalling rules so that Ceph daemons can communicate# with each others.configure_firewall: False# Open ports on corresponding nodes if firewall is installed on it#ceph_mon_firewall_zone: public#ceph_mgr_firewall_zone: public#ceph_osd_firewall_zone: public#ceph_rgw_firewall_zone: public#ceph_mds_firewall_zone: public#ceph_nfs_firewall_zone: public#ceph_rbdmirror_firewall_zone: public#ceph_iscsi_firewall_zone: public#ceph_dashboard_firewall_zone: public#ceph_rgwloadbalancer_firewall_zone: public# Generate local ceph.conf in fetch directory#ceph_conf_local: false############# PACKAGES ##############debian_package_dependencies: []#centos_package_dependencies:# - epel-release# - python3-libselinux#redhat_package_dependencies: []#suse_package_dependencies: []# Whether or not to install the ceph-test package.#ceph_test: false# Enable the ntp service by default to avoid clock skew on ceph nodes# Disable if an appropriate NTP client is already installed and configured#ntp_service_enabled: true# Set type of NTP client daemon to use, valid entries are chronyd, ntpd or timesyncdntp_daemon_type: ntpd# This variable determines if ceph packages can be updated. If False, the# package resources will use \"state=present\". If True, they will use# \"state=latest\".#upgrade_ceph_packages: False#ceph_use_distro_backports: false # DEBIAN ONLY#ceph_directories_mode: \"0755\"############ INSTALL #############ceph_repository_type: dummy# ORIGIN SOURCE## Choose between:# - 'repository' means that you will get ceph installed through a new repository. Later below choose between 'community', 'rhcs', 'dev' or 'obs'# - 'distro' means that no separate repo file will be added# you will get whatever version of Ceph is included in your Linux distro.# 'local' means that the ceph binaries will be copied over from the local machine# repository: 使用 Ceph upstream 的 repository# distro: 使用 Linux distro 中包好的 Ceph# local: 使用本地編譯好的 Ceph binaryceph_origin: repository#valid_ceph_origins:# - repository# - distro# - localceph_repository: community#valid_ceph_repository:# - community# - rhcs# - dev# - uca# - custom# - obs# REPOSITORY: COMMUNITY VERSION## Enabled when ceph_repository == 'community'#ceph_mirror: http://download.ceph.comceph_stable_key: https://download.ceph.com/keys/release.ascceph_stable_release: octopusceph_stable_repo: \"{{ ceph_mirror }}/debian-{{ ceph_stable_release }}\"#nfs_ganesha_stable: true # use stable repos for nfs-ganesha#nfs_ganesha_stable_branch: V3.2-stable#nfs_ganesha_stable_deb_repo: \"{{ ceph_mirror }}/nfs-ganesha/deb-{{ nfs_ganesha_stable_branch }}/{{ ceph_stable_release }}\"# Use the option below to specify your applicable package tree, eg. when using non-LTS Ubuntu versions# # for a list of available Debian distributions, visit http://download.ceph.com/debian-{{ ceph_stable_release }}/dists/# for more info read: https://github.com/ceph/ceph-ansible/issues/305#ceph_stable_distro_source: \"{{ ansible_distribution_release }}\"# This option is needed for _both_ stable and dev version, so please always fill the right version# # for supported distros, see http://download.ceph.com/rpm-{{ ceph_stable_release }}/#ceph_stable_redhat_distro: el8# REPOSITORY: RHCS VERSION RED HAT STORAGE (from 5.0)## Enabled when ceph_repository == 'rhcs'## This version is supported on RHEL 8##ceph_rhcs_version: \"{{ ceph_stable_rh_storage_version | default(5) }}\"#valid_ceph_repository_type:# - cdn# - iso#ceph_rhcs_iso_path: \"{{ ceph_stable_rh_storage_iso_path | default('') }}\"#ceph_rhcs_mount_path: \"{{ ceph_stable_rh_storage_mount_path | default('/tmp/rh-storage-mount') }}\"#ceph_rhcs_repository_path: \"{{ ceph_stable_rh_storage_repository_path | default('/tmp/rh-storage-repo') }}\" # where to copy iso's content# REPOSITORY: UBUNTU CLOUD ARCHIVE## Enabled when ceph_repository == 'uca'## This allows the install of Ceph from the Ubuntu Cloud Archive. The Ubuntu Cloud Archive# usually has newer Ceph releases than the normal distro repository.###ceph_stable_repo_uca: \"http://ubuntu-cloud.archive.canonical.com/ubuntu\"#ceph_stable_openstack_release_uca: queens#ceph_stable_release_uca: \"{{ ansible_distribution_release }}-updates/{{ ceph_stable_openstack_release_uca }}\"# REPOSITORY: openSUSE OBS## Enabled when ceph_repository == 'obs'## This allows the install of Ceph from the openSUSE OBS repository. The OBS repository# usually has newer Ceph releases than the normal distro repository.###ceph_obs_repo: \"https://download.opensuse.org/repositories/filesystems:/ceph:/{{ ceph_stable_release }}/openSUSE_Leap_{{ ansible_distribution_version }}/\"# REPOSITORY: DEV## Enabled when ceph_repository == 'dev'##ceph_dev_branch: master # development branch you would like to use e.g: master, wip-hack#ceph_dev_sha1: latest # distinct sha1 to use, defaults to 'latest' (as in latest built)#nfs_ganesha_dev: false # use development repos for nfs-ganesha# Set this to choose the version of ceph dev libraries used in the nfs-ganesha packages from shaman# flavors so far include: ceph_master, ceph_jewel, ceph_kraken, ceph_luminous#nfs_ganesha_flavor: \"ceph_master\"#ceph_iscsi_config_dev: true # special repo for deploying iSCSI gateways# REPOSITORY: CUSTOM## Enabled when ceph_repository == 'custom'## Use a custom repository to install ceph. For RPM, ceph_custom_repo should be# a URL to the .repo file to be installed on the targets. For deb,# ceph_custom_repo should be the URL to the repo base.##ceph_custom_key: https://server.domain.com/ceph-custom-repo-key.asc#ceph_custom_repo: https://server.domain.com/ceph-custom-repo# ORIGIN: LOCAL CEPH INSTALLATION## Enabled when ceph_repository == 'local'## Path to DESTDIR of the ceph install#ceph_installation_dir: \"/path/to/ceph_installation/\"# Whether or not to use installer script rundep_installer.sh# This script takes in rundep and installs the packages line by line onto the machine# If this is set to false then it is assumed that the machine ceph is being copied onto will already have# all runtime dependencies installed#use_installer: false# Root directory for ceph-ansible#ansible_dir: \"/path/to/ceph-ansible\"####################### CEPH CONFIGURATION ######################### Ceph options## Each cluster requires a unique, consistent filesystem ID. By# default, the playbook generates one for you and stores it in a file# in `fetch_directory`. If you want to customize how the fsid is# generated, you may find it useful to disable fsid generation to# avoid cluttering up your ansible repo. If you set `generate_fsid` to# false, you *must* generate `fsid` in another way.# ACTIVATE THE FSID VARIABLE FOR NON-VAGRANT DEPLOYMENT#fsid: \"{{ cluster_uuid.stdout }}\"#generate_fsid: true#ceph_conf_key_directory: /etc/ceph#ceph_uid: 167# Permissions for keyring files in /etc/ceph#ceph_keyring_permissions: '0600'#cephx: true## Client options##rbd_cache: \"true\"#rbd_cache_writethrough_until_flush: \"true\"#rbd_concurrent_management_ops: 20#rbd_client_directories: true # this will create rbd_client_log_path and rbd_client_admin_socket_path directories with proper permissions# Permissions for the rbd_client_log_path and# rbd_client_admin_socket_path. Depending on your use case for Ceph# you may want to change these values. The default, which is used if# any of the variables are unset or set to a false value (like `null`# or `false`) is to automatically determine what is appropriate for# the Ceph version with non-OpenStack workloads -- ceph:ceph and 0770# for infernalis releases, and root:root and 1777 for pre-infernalis# releases.## For other use cases, including running Ceph with OpenStack, you'll# want to set these differently:## For OpenStack on RHEL, you'll want:# rbd_client_directory_owner: \"qemu\"# rbd_client_directory_group: \"libvirtd\" (or \"libvirt\", depending on your version of libvirt)# rbd_client_directory_mode: \"0755\"## For OpenStack on Ubuntu or Debian, set:# rbd_client_directory_owner: \"libvirt-qemu\"# rbd_client_directory_group: \"kvm\"# rbd_client_directory_mode: \"0755\"## If you set rbd_client_directory_mode, you must use a string (e.g.,# 'rbd_client_directory_mode: \"0755\"', *not*# 'rbd_client_directory_mode: 0755', or Ansible will complain: mode# must be in octal or symbolic form#rbd_client_directory_owner: ceph#rbd_client_directory_group: ceph#rbd_client_directory_mode: \"0770\"#rbd_client_log_path: /var/log/ceph#rbd_client_log_file: \"{{ rbd_client_log_path }}/qemu-guest-$pid.log\" # must be writable by QEMU and allowed by SELinux or AppArmor#rbd_client_admin_socket_path: /var/run/ceph # must be writable by QEMU and allowed by SELinux or AppArmor## Monitor options## You must define either monitor_interface, monitor_address or monitor_address_block.# These variables must be defined at least in all.yml and overrided if needed (inventory host file or group_vars/*.yml).# Eg. If you want to specify for each monitor which address the monitor will bind to you can set it in your **inventory host file** by using 'monitor_address' variable.# Preference will go to monitor_address if both monitor_address and monitor_interface are defined.monitor_interface: ens3#monitor_address: x.x.x.x#monitor_address_block: subnet# set to either ipv4 or ipv6, whichever your network is using#ip_version: ipv4#mon_host_v1:# enabled: True# suffix: ':6789'#mon_host_v2:# suffix: ':3300'########### CEPHFS ############ When pg_autoscale_mode is set to True, you must add the target_size_ratio key with a correct value# `pg_num` and `pgp_num` keys will be ignored, even if specified.# eg:# cephfs_data_pool:# name: \"{{ cephfs_data if cephfs_data is defined else 'cephfs_data' }}\"# pg_num: \"{{ osd_pool_default_pg_num }}\"# pgp_num: \"{{ osd_pool_default_pg_num }}\"# rule_name: \"replicated_rule\"# type: 1# erasure_profile: \"\"# expected_num_objects: \"\"# application: \"cephfs\"# size: \"{{ osd_pool_default_size }}\"# min_size: \"{{ osd_pool_default_min_size }}\"# pg_autoscale_mode: False# target_size_ratio: 0.2#cephfs: cephfs # name of the ceph filesystem#cephfs_data_pool:# name: \"{{ cephfs_data if cephfs_data is defined else 'cephfs_data' }}\"# pg_num: \"{{ osd_pool_default_pg_num }}\"# pgp_num: \"{{ osd_pool_default_pg_num }}\"# rule_name: \"replicated_rule\"# type: 1# erasure_profile: \"\"# expected_num_objects: \"\"# application: \"cephfs\"# size: \"{{ osd_pool_default_size }}\"# min_size: \"{{ osd_pool_default_min_size }}\"# pg_autoscale_mode: False#cephfs_metadata_pool:# name: \"{{ cephfs_metadata if cephfs_metadata is defined else 'cephfs_metadata' }}\"# pg_num: \"{{ osd_pool_default_pg_num }}\"# pgp_num: \"{{ osd_pool_default_pg_num }}\"# rule_name: \"replicated_rule\"# type: 1# erasure_profile: \"\"# expected_num_objects: \"\"# application: \"cephfs\"# size: \"{{ osd_pool_default_size }}\"# min_size: \"{{ osd_pool_default_min_size }}\"# pg_autoscale_mode: False#cephfs_pools:# - \"{{ cephfs_data_pool }}\"# - \"{{ cephfs_metadata_pool }}\"## OSD options##is_hci: false#hci_safety_factor: 0.2#non_hci_safety_factor: 0.7#osd_memory_target: 4294967296#journal_size: 1024 # OSD journal size in MB#block_db_size: -1 # block db size in bytes for the ceph-volume lvm batch. -1 means use the default of 'as big as possible'.public_network: \"10.1.3.0/24\"#cluster_network: \"{{ public_network | regex_replace(' ', '') }}\"#osd_mkfs_type: xfs#osd_mkfs_options_xfs: -f -i size=2048#osd_mount_options_xfs: noatime,largeio,inode64,swallocosd_objectstore: bluestore# Any device containing these patterns in their path will be excluded.#osd_auto_discovery_exclude: \"dm-*|loop*|md*|rbd*\"# xattrs. by default, 'filestore xattr use omap' is set to 'true' if# 'osd_mkfs_type' is set to 'ext4'; otherwise it isn't set. This can# be set to 'true' or 'false' to explicitly override those# defaults. Leave it 'null' to use the default for your chosen mkfs# type.#filestore_xattr_use_omap: null## MDS options##mds_max_mds: 1## Rados Gateway options##radosgw_frontend_type: beast # For additionnal frontends see: http://docs.ceph.com/docs/nautilus/radosgw/frontends/#radosgw_civetweb_port: 8080#radosgw_civetweb_num_threads: 512#radosgw_civetweb_options: \"num_threads={{ radosgw_civetweb_num_threads }}\"# For additional civetweb configuration options available such as logging,# keepalive, and timeout settings, please see the civetweb docs at# https://github.com/civetweb/civetweb/blob/master/docs/UserManual.md#radosgw_frontend_port: \"{{ radosgw_civetweb_port if radosgw_frontend_type == 'civetweb' else '8080' }}\"# The server private key, public certificate and any other CA or intermediate certificates should be in one file, in PEM format.#radosgw_frontend_ssl_certificate: \"\"#radosgw_frontend_ssl_certificate_data: \"\" # certificate contents to be written to path defined by radosgw_frontend_ssl_certificate#radosgw_frontend_options: \"{{ radosgw_civetweb_options if radosgw_frontend_type == 'civetweb' else '' }}\"#radosgw_thread_pool_size: 512# You must define either radosgw_interface, radosgw_address.# These variables must be defined at least in all.yml and overrided if needed (inventory host file or group_vars/*.yml).# Eg. If you want to specify for each radosgw node which address the radosgw will bind to you can set it in your **inventory host file** by using 'radosgw_address' variable.# Preference will go to radosgw_address if both radosgw_address and radosgw_interface are defined.#radosgw_interface: interface#radosgw_address: x.x.x.x#radosgw_address_block: subnet#radosgw_keystone_ssl: false # activate this when using keystone PKI keys#radosgw_num_instances: 1# Rados Gateway options#email_address: foo@bar.com## Testing mode# enable this mode _only_ when you have a single node# if you don't want it keep the option commented#common_single_host_mode: true## Handlers - restarting daemons after a config change# if for whatever reasons the content of your ceph configuration changes# ceph daemons will be restarted as well. At the moment, we can not detect# which config option changed so all the daemons will be restarted. Although# this restart will be serialized for each node, in between a health check# will be performed so we make sure we don't move to the next node until# ceph is not healthy# Obviously between the checks (for monitors to be in quorum and for osd's pgs# to be clean) we have to wait. These retries and delays can be configurable# for both monitors and osds.## Monitor handler checks#handler_health_mon_check_retries: 10#handler_health_mon_check_delay: 20## OSD handler checks#handler_health_osd_check_retries: 40#handler_health_osd_check_delay: 30#handler_health_osd_check: true## MDS handler checks#handler_health_mds_check_retries: 5#handler_health_mds_check_delay: 10## RGW handler checks#handler_health_rgw_check_retries: 5#handler_health_rgw_check_delay: 10# NFS handler checks#handler_health_nfs_check_retries: 5#handler_health_nfs_check_delay: 10# RBD MIRROR handler checks#handler_health_rbd_mirror_check_retries: 5#handler_health_rbd_mirror_check_delay: 10# MGR handler checks#handler_health_mgr_check_retries: 5#handler_health_mgr_check_delay: 10## health mon/osds check retries/delay:#health_mon_check_retries: 20#health_mon_check_delay: 10#health_osd_check_retries: 20#health_osd_check_delay: 10################ NFS-GANESHA ################# Confiure the type of NFS gatway access. At least one must be enabled for an# NFS role to be useful## Set this to true to enable File access via NFS. Requires an MDS role.#nfs_file_gw: false# Set this to true to enable Object access via NFS. Requires an RGW role.#nfs_obj_gw: \"{{ False if groups.get(mon_group_name, []) | length == 0 else True }}\"############## MULTISITE ############### Changing this value allows multisite code to run#rgw_multisite: false# If the desired multisite configuration involves only one realm, one zone group and one zone (per cluster), then the multisite variables can be set here.# Please see README-MULTISITE.md for more information.## If multiple realms or multiple zonegroups or multiple zones need to be created on a cluster then,# the multisite config variables should be editted in their respective zone .yaml file and realm .yaml file.# See README-MULTISITE-MULTIREALM.md for more information.# The following Multi-site related variables should be set by the user.## rgw_zone is set to \"default\" to enable compression for clusters configured without rgw multi-site# If multisite is configured, rgw_zone should not be set to \"default\".##rgw_zone: default#rgw_zonemaster: true#rgw_zonesecondary: false#rgw_zonegroup: solarsystem # should be set by the user#rgw_zonegroupmaster: true#rgw_zone_user: zone.user#rgw_zone_user_display_name: \"Zone User\"#rgw_realm: milkyway # should be set by the user#rgw_multisite_proto: \"http\"#system_access_key: 6kWkikvapSnHyE22P7nO # should be re-created by the user#system_secret_key: MGecsMrWtKZgngOHZdrd6d3JxGO5CPWgT2lcnpSt # should be re-created by the user# Multi-site remote pull URL variables#rgw_pull_port: \"{{ radosgw_frontend_port }}\"#rgw_pull_proto: \"http\" # should be the same as rgw_multisite_proto for the master zone cluster#rgw_pullhost: localhost # rgw_pullhost only needs to be declared if there is a zone secondary.#################### CONFIG OVERRIDE ##################### Ceph configuration file override.# This allows you to specify more configuration options# using an INI style format.## When configuring RGWs, make sure you use the form [client.rgw.*]# instead of [client.radosgw.*].# For more examples check the profiles directory of https://github.com/ceph/ceph-ansible.## The following sections are supported: [global], [mon], [osd], [mds], [client]## Example:# ceph_conf_overrides:# global:# foo: 1234# bar: 5678# \"client.rgw.{{ hostvars[groups.get(rgw_group_name)[0]]['ansible_hostname'] }}\":# rgw_zone: zone1##ceph_conf_overrides: {}############## OS TUNING ###############disable_transparent_hugepage: \"{{ false if osd_objectstore == 'bluestore' else true }}\"#os_tuning_params:# - { name: fs.file-max, value: 26234859 }# - { name: vm.zone_reclaim_mode, value: 0 }# - { name: vm.swappiness, value: 10 }# - { name: vm.min_free_kbytes, value: \"{{ vm_min_free_kbytes }}\" }# For Debian &amp; Red Hat/CentOS installs set TCMALLOC_MAX_TOTAL_THREAD_CACHE_BYTES# Set this to a byte value (e.g. 134217728)# A value of 0 will leave the package default.#ceph_tcmalloc_max_total_thread_cache: 0########### DOCKER ############ceph_docker_image: \"ceph/daemon\"#ceph_docker_image_tag: latest#ceph_docker_registry: docker.io#ceph_docker_registry_auth: false#ceph_docker_registry_username:#ceph_docker_registry_password:## Client only docker image - defaults to {{ ceph_docker_image }}#ceph_client_docker_image: \"{{ ceph_docker_image }}\"#ceph_client_docker_image_tag: \"{{ ceph_docker_image_tag }}\"#ceph_client_docker_registry: \"{{ ceph_docker_registry }}\"#ceph_docker_enable_centos_extra_repo: false#ceph_docker_on_openstack: false#containerized_deployment: False#container_binary:#timeout_command: \"{{ 'timeout --foreground -s KILL ' ~ docker_pull_timeout if (docker_pull_timeout != '0') and (ceph_docker_dev_image is undefined or not ceph_docker_dev_image) else '' }}\"# this is only here for usage with the rolling_update.yml playbook# do not ever change this here#rolling_update: false###################### Docker pull retry #######################docker_pull_retry: 3#docker_pull_timeout: \"300s\"############## OPENSTACK ###############openstack_config: false# When pg_autoscale_mode is set to True, you must add the target_size_ratio key with a correct value# `pg_num` and `pgp_num` keys will be ignored, even if specified.# eg:# openstack_glance_pool:# name: \"images\"# pg_num: \"{{ osd_pool_default_pg_num }}\"# pgp_num: \"{{ osd_pool_default_pg_num }}\"# rule_name: \"replicated_rule\"# type: 1# erasure_profile: \"\"# expected_num_objects: \"\"# application: \"rbd\"# size: \"{{ osd_pool_default_size }}\"# min_size: \"{{ osd_pool_default_min_size }}\"# pg_autoscale_mode: False# target_size_ratio: 0.2#openstack_glance_pool:# name: \"images\"# pg_num: \"{{ osd_pool_default_pg_num }}\"# pgp_num: \"{{ osd_pool_default_pg_num }}\"# rule_name: \"replicated_rule\"# type: 1# erasure_profile: \"\"# expected_num_objects: \"\"# application: \"rbd\"# size: \"{{ osd_pool_default_size }}\"# min_size: \"{{ osd_pool_default_min_size }}\"# pg_autoscale_mode: False#openstack_cinder_pool:# name: \"volumes\"# pg_num: \"{{ osd_pool_default_pg_num }}\"# pgp_num: \"{{ osd_pool_default_pg_num }}\"# rule_name: \"replicated_rule\"# type: 1# erasure_profile: \"\"# expected_num_objects: \"\"# application: \"rbd\"# size: \"{{ osd_pool_default_size }}\"# min_size: \"{{ osd_pool_default_min_size }}\"# pg_autoscale_mode: False#openstack_nova_pool:# name: \"vms\"# pg_num: \"{{ osd_pool_default_pg_num }}\"# pgp_num: \"{{ osd_pool_default_pg_num }}\"# rule_name: \"replicated_rule\"# type: 1# erasure_profile: \"\"# expected_num_objects: \"\"# application: \"rbd\"# size: \"{{ osd_pool_default_size }}\"# min_size: \"{{ osd_pool_default_min_size }}\"# pg_autoscale_mode: False#openstack_cinder_backup_pool:# name: \"backups\"# pg_num: \"{{ osd_pool_default_pg_num }}\"# pgp_num: \"{{ osd_pool_default_pg_num }}\"# rule_name: \"replicated_rule\"# type: 1# erasure_profile: \"\"# expected_num_objects: \"\"# application: \"rbd\"# size: \"{{ osd_pool_default_size }}\"# min_size: \"{{ osd_pool_default_min_size }}\"# pg_autoscale_mode: False#openstack_gnocchi_pool:# name: \"metrics\"# pg_num: \"{{ osd_pool_default_pg_num }}\"# pgp_num: \"{{ osd_pool_default_pg_num }}\"# rule_name: \"replicated_rule\"# type: 1# erasure_profile: \"\"# expected_num_objects: \"\"# application: \"rbd\"# size: \"{{ osd_pool_default_size }}\"# min_size: \"{{ osd_pool_default_min_size }}\"# pg_autoscale_mode: False#openstack_cephfs_data_pool:# name: \"manila_data\"# pg_num: \"{{ osd_pool_default_pg_num }}\"# pgp_num: \"{{ osd_pool_default_pg_num }}\"# rule_name: \"replicated_rule\"# type: 1# erasure_profile: \"\"# expected_num_objects: \"\"# application: \"cephfs\"# size: \"{{ osd_pool_default_size }}\"# min_size: \"{{ osd_pool_default_min_size }}\"# pg_autoscale_mode: False#openstack_cephfs_metadata_pool:# name: \"manila_metadata\"# pg_num: \"{{ osd_pool_default_pg_num }}\"# pgp_num: \"{{ osd_pool_default_pg_num }}\"# rule_name: \"replicated_rule\"# type: 1# erasure_profile: \"\"# expected_num_objects: \"\"# application: \"cephfs\"# size: \"{{ osd_pool_default_size }}\"# min_size: \"{{ osd_pool_default_min_size }}\"# pg_autoscale_mode: False#openstack_pools:# - \"{{ openstack_glance_pool }}\"# - \"{{ openstack_cinder_pool }}\"# - \"{{ openstack_nova_pool }}\"# - \"{{ openstack_cinder_backup_pool }}\"# - \"{{ openstack_gnocchi_pool }}\"# - \"{{ openstack_cephfs_data_pool }}\"# - \"{{ openstack_cephfs_metadata_pool }}\"# The value for 'key' can be a pre-generated key,# e.g key: \"AQDC2UxZH4yeLhAAgTaZb+4wDUlYOsr1OfZSpQ==\"# By default, keys will be auto-generated.##openstack_keys:# - { name: client.glance, caps: { mon: \"profile rbd\", osd: \"profile rbd pool={{ openstack_cinder_pool.name }}, profile rbd pool={{ openstack_glance_pool.name }}\"}, mode: \"0600\" }# - { name: client.cinder, caps: { mon: \"profile rbd\", osd: \"profile rbd pool={{ openstack_cinder_pool.name }}, profile rbd pool={{ openstack_nova_pool.name }}, profile rbd pool={{ openstack_glance_pool.name }}\"}, mode: \"0600\" }# - { name: client.cinder-backup, caps: { mon: \"profile rbd\", osd: \"profile rbd pool={{ openstack_cinder_backup_pool.name }}\"}, mode: \"0600\" }# - { name: client.gnocchi, caps: { mon: \"profile rbd\", osd: \"profile rbd pool={{ openstack_gnocchi_pool.name }}\"}, mode: \"0600\", }# - { name: client.openstack, caps: { mon: \"profile rbd\", osd: \"profile rbd pool={{ openstack_glance_pool.name }}, profile rbd pool={{ openstack_nova_pool.name }}, profile rbd pool={{ openstack_cinder_pool.name }}, profile rbd pool={{ openstack_cinder_backup_pool.name }}\"}, mode: \"0600\" }############## DASHBOARD ###############dashboard_enabled: True# Choose http or https# For https, you should set dashboard.crt/key and grafana.crt/key# If you define the dashboard_crt and dashboard_key variables, but leave them as '',# then we will autogenerate a cert and keyfile#dashboard_protocol: http#dashboard_port: 8443#dashboard_admin_user: admin#dashboard_admin_user_ro: false# This variable must be set with a strong custom password when dashboard_enabled is Truedashboard_admin_password: password# We only need this for SSL (https) connections#dashboard_crt: ''#dashboard_key: ''#dashboard_rgw_api_user_id: ceph-dashboard#dashboard_rgw_api_admin_resource: ''#dashboard_rgw_api_no_ssl_verify: False#dashboard_frontend_vip: ''#node_exporter_container_image: \"docker.io/prom/node-exporter:v0.17.0\"#node_exporter_port: 9100#grafana_admin_user: admin# This variable must be set with a strong custom password when dashboard_enabled is Truegrafana_admin_password: password# We only need this for SSL (https) connections#grafana_crt: ''#grafana_key: ''# When using https, please fill with a hostname for which grafana_crt is valid.#grafana_server_fqdn: ''#grafana_container_image: \"docker.io/grafana/grafana:5.4.3\"#grafana_container_cpu_period: 100000#grafana_container_cpu_cores: 2# container_memory is in GB#grafana_container_memory: 4#grafana_uid: 472#grafana_datasource: Dashboard#grafana_dashboards_path: \"/etc/grafana/dashboards/ceph-dashboard\"#grafana_dashboard_version: master#grafana_dashboard_files:# - ceph-cluster.json# - cephfs-overview.json# - host-details.json# - hosts-overview.json# - osd-device-details.json# - osds-overview.json# - pool-detail.json# - pool-overview.json# - radosgw-detail.json# - radosgw-overview.json# - rbd-overview.json#grafana_plugins:# - vonage-status-panel# - grafana-piechart-panel#grafana_allow_embedding: True#grafana_port: 3000#prometheus_container_image: \"docker.io/prom/prometheus:v2.7.2\"#prometheus_container_cpu_period: 100000#prometheus_container_cpu_cores: 2# container_memory is in GB#prometheus_container_memory: 4#prometheus_data_dir: /var/lib/prometheus#prometheus_conf_dir: /etc/prometheus#prometheus_user_id: '65534' # This is the UID used by the prom/prometheus container image#prometheus_port: 9092#alertmanager_container_image: \"docker.io/prom/alertmanager:v0.16.2\"#alertmanager_container_cpu_period: 100000#alertmanager_container_cpu_cores: 2# container_memory is in GB#alertmanager_container_memory: 4#alertmanager_data_dir: /var/lib/alertmanager#alertmanager_conf_dir: /etc/alertmanager#alertmanager_port: 9093#alertmanager_cluster_port: 9094---略 修改 group_vars/osds.yml 中的配置。18: copy_admin_key: true36: devices:37: - /dev/vdb50: osd_scenario: collocated---# Variables here are applicable to all host groups NOT roles# This sample file generated by generate_group_vars_sample.sh# Dummy variable to avoid error because ansible does not recognize the# file as a good configuration file when no variable in it.dummy:############ GENERAL ############# Even though OSD nodes should not have the admin key# at their disposal, some people might want to have it# distributed on OSD nodes. Setting 'copy_admin_key' to 'true'# will copy the admin key to the /etc/ceph/ directorycopy_admin_key: true############### CEPH OPTIONS############### Devices to be used as OSDs# You can pre-provision disks that are not present yet.# Ansible will just skip them. Newly added disk will be# automatically configured during the next run.## Declare devices to be used as OSDs# All scenario(except 3rd) inherit from the following device declaration# Note: This scenario uses the ceph-volume lvm batch method to provision OSDs# 因目前使用 openstack VM 所以只有一個 disk # devices: 用來儲存的裝置，可以定義多個，如果每個 node 並不相同的話，可以嘗試使用 'osd_auto_discovery'，將其設為 true。devices: - /dev/vdb# - /dev/sdc# - /dev/sdd# - /dev/sde# osd_scenario: OSD 的部署方式，有 collocated、non-collocated、lvm 三種選項。# collocated: 將 ceph data, ceph block, ceph block.db, ceph block.wal 放在同一個裝置上。# non-collocated: 會將 ceph data 跟 ceph block 放在 devices 上，並且將 ceph block.db 跟 ceph block.wal 放在額外設定的 dedicated_devices 上。# lvm`: 需要設定 data, wal 跟 db 的 lv name 跟 vg group，只有 data 為必填選項osd_scenario: collocated---略 修改 group_vars/mgrs.yml 中的配置。26: ceph_mgr_modules: [status]---# Variables here are applicable to all host groups NOT roles# This sample file generated by generate_group_vars_sample.sh# Dummy variable to avoid error because ansible does not recognize the# file as a good configuration file when no variable in it.dummy:########### GLOBAL ############ Even though MGR nodes should not have the admin key# at their disposal, some people might want to have it# distributed on MGR nodes. Setting 'copy_admin_key' to 'true'# will copy the admin key to the /etc/ceph/ directory#copy_admin_key: false#mgr_secret: 'mgr_secret'############ MODULES ############# Ceph mgr modules to enable, to view the list of available mpdules see: http://docs.ceph.com/docs/CEPH_VERSION/mgr/# and replace CEPH_VERSION with your current Ceph version, e,g: 'mimic'ceph_mgr_modules: [status] 修改 site.yml 中的配置。 完成上述的編輯以後，透過下面指令進行部署。root@ceph-ansible:~/ceph-ansible# ansible-playbook site.yml完成 !Post converted from Medium by ZMediumToMarkdown." }, { "title": "使用 CephFS 代替 HDFS", "url": "/posts/5416e0765d5f/", "categories": "Jackycsie", "tags": "ceph, hdfs, hadoop", "date": "2020-04-08 11:44:17 +0800", "snippet": "使用 CephFS 代替 HDFS本文將介紹，如何透過 CephFS，替換掉 hadoop 生態系的 HDFS，那為什麼要替換呢？下面將會列出為什麼我會使用 CephFS 代替 HDFS 的原因。 Ceph 在硬體的擴建來的更加彈性以及更有架構化。 可以直接對 CephFS 內容進行修改。 CephFS 不存在單點控制的訪問入口，全部的 user 皆可以透過各 mon 去訪問 file...", "content": "使用 CephFS 代替 HDFS本文將介紹，如何透過 CephFS，替換掉 hadoop 生態系的 HDFS，那為什麼要替換呢？下面將會列出為什麼我會使用 CephFS 代替 HDFS 的原因。 Ceph 在硬體的擴建來的更加彈性以及更有架構化。 可以直接對 CephFS 內容進行修改。 CephFS 不存在單點控制的訪問入口，全部的 user 皆可以透過各 mon 去訪問 file system。 當儲存空間有限無法切割出各部門所需要的環境。 Ceph 提供完整的 POSIX API。 Ceph 可以切割冷熱數據與設備，如 SSD pool, HDD pool。 若是想要公司環境想要有雲服務與 hadoop 生態鏈的計算儲存服務，那 CephFS 會來的比 HDFS 更彈性，以及更容易控管硬體設備。1.必備條件替換 HDFS 成 CephFS 時必須先有幾個必要條件。已經安裝完下面兩個環境。 CephFS cluster Hadoop clusterCephFS statusHadoop clusterCeph and Hadoop version Ceph 13.2.8 Hadoop 2.10.0 CephFS 的 Hadoop 套件版本cephfs-hadoop-0.80.6.jar Java 1.8 python 2Ceph and Hadoop deploy article Ceph Deploy ( webiste ) CephFS Deploy ( website ) Hadoop Deploy ( website )2.安裝必要插件前提概要 ，以下的示範我們拿的是 hadoop cluster 的 master node 作為示範，若是想要全部的 node 都有此功能，每個 node 跟著 2. 3. 步驟做即可。 在 hadoop master 安裝 CephFS dependes packages.root@master:~# wget -q -O- 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc' | sudo apt-key add -root@master:~# echo deb https://download.ceph.com/debian-luminous/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.listroot@master:~# apt updateroot@master:~# apt-get install libcephfs-jni libcephfs-javaroot@master:~# cd $HADOOP_HOME/lib/native/root@master:~# ln -s /usr/lib/jni/libcephfs_jni.so . Deploy CephFS hadoop plug-inroot@master:~# git clone https://github.com/ceph/cephfs-hadoop.gitroot@master:~# cd cephfs-hadooproot@master:~# apt install mavenroot@master:~# mvn -Dmaven.test.skip=true packageroot@master:~# cp /root/cephfs-hadoop/target/cephfs-hadoop-0.80.6.jar /usr/lib/hadoop/libroot@master:~# cp /usr/share/java/libcephfs.jar /usr/lib/hadoop/libroot@master:~# vim /usr/lib/hadoop/etc/hadoop/hadoop-env.sh# 放剛剛下載的 package 到 hadoop-env.shexport HADOOP_CLASSPATH=\"/usr/lib/hadoop/lib/cephfs-hadoop-0.80.6.jar\"export HADOOP_CLASSPATH=\"/usr/lib/hadoop/lib/libcephfs.jar:$HADOOP_CLASSPATH\" 在 Ceph 中建立 Hadoop 所需要用的 poolroot@ceph-node0:~# ceph osd pool create hadoop1 100root@ceph-node0:~# ceph osd pool set hadoop1 size 1root@ceph-node0:~# ceph fs add_data_pool cephfs hadoop1 透過 ceph-deploy 讓 hadoop master 有 admin 權限# master 安裝 pythonroot@master:~# /usr/lib/hadoop/lib# apt install python-pip# 透過 ceph-deploy 使 master 安裝 cephroot@ceph-node0:~# ceph-deploy install masterroot@ceph-node0:~# ceph-deploy config push masterroot@ceph-node0:~# ceph-deploy admin master# 更改 master /etc/ceph/ 權限root@master:~# chown -R root:root /etc/ceph/ 修改 hadoop core-site.xml 將內部設置為連結 ceph 的 configroot@master:~# vim /usr/lib/hadoop/etc/hadoop/core-site.xml&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/app/hadoop/tmp&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;ceph://10.1.3.175/&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;ceph.conf.file&lt;/name&gt; &lt;value&gt;/etc/ceph/ceph.conf&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;ceph.auth.id&lt;/name&gt; &lt;value&gt;admin&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;ceph.auth.keyring&lt;/name&gt; &lt;value&gt;/etc/ceph/ceph.client.admin.keyring&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;ceph.data.pools&lt;/name&gt; &lt;value&gt;hadoop1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;fs.ceph.impl&lt;/name&gt; &lt;value&gt;org.apache.hadoop.fs.ceph.CephFileSystem&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;完成 ! 看一下 file system 內部資訊root@master:~# hadoop fs -ls /3. 將檔案放進 CephFSroot@master:~# hadoop fs -copyFromLocal hadoop-2.10.0.tar.gz /4. 可能會失敗的原因 因為 deploy CephFS plug-in 時我們的路徑是在 /usr/lib/hadooop, 但若是你的 hadoop 路徑在 /usr/local/hadoop，建議換位置即可成功。5. 如何修改 CephFS 資料 若是您想要在 master 中修改資料，直接拿到 key ， mount 到任意 folder ，即可修改資料，若是其他 server 想要拿到資料，就必須拿到 key 與建立 client.user 才可以。root@master:~# mount -t ceph ceph-node0:6789:/ /mnt/cephfs -o name=cephfs,secret=AQBAsYJeEC5fHxAAATzV0cLBQgmihe96HZY8CA==參考文獻 https://docs.ceph.com/docs/mimic/cephfs/hadoop/ https://discourse.juju.is/t/integrating-hadoop-with-cephfs/571/2 http://www.kai-zhang.com/cloud-computing/Running-Hadoop-on-CEPH/心得本身難度不高，但沒人寫文章，可能太簡單，大家都照官網做就成功了吧。Post converted from Medium by ZMediumToMarkdown." }, { "title": "使用 ceph-deploy 部署 ceph 叢集", "url": "/posts/1a780e62cd26/", "categories": "Jackycsie", "tags": "ceph, hdfs, hadoop", "date": "2020-03-30 15:04:46 +0800", "snippet": "使用 ceph-deploy 部署 ceph 叢集本篇會介紹如何透過 ceph-deploy 工具安裝一個 ceph 叢集，使用的環境是 ubuntu 18.04 LTS ，一個最簡單的 Ceph 儲存叢集至少要一台 Monitor 與三台 OSD 。而 MDS 是當需要使用到 CephFS 的時候才需要部署。。環境準備本次安裝會擁有 5 台 node，叢集拓樸圖如下所示：Tips: 每一...", "content": "使用 ceph-deploy 部署 ceph 叢集本篇會介紹如何透過 ceph-deploy 工具安裝一個 ceph 叢集，使用的環境是 ubuntu 18.04 LTS ，一個最簡單的 Ceph 儲存叢集至少要一台 Monitor 與三台 OSD 。而 MDS 是當需要使用到 CephFS 的時候才需要部署。。環境準備本次安裝會擁有 5 台 node，叢集拓樸圖如下所示：Tips: 每一台 server 都需要安裝，python 與 NTP package。$ apt install -y ntp$ apt install -y python-pip Deploy Node 先設定好可以直接不須密碼連進其他台 node(server) .$ ssh-copy-id root@ceph-node[1-4] 每一台 hostname 必須先設定成未來 host 連接時一樣的名稱。$ hostnamectl set-hostname {你想要的 hostname}透過 ceph deploy 做叢集 我們透過 deploy node 安裝，在這裡我們透過 pip install ceph-deploy。root@ceph-node0:~# apt install ceph-deploy# 建立在 deploy noderoot@ceph-node0:~# mkdir /etc/ceph ; cd /etc/ceph 使用 ceph-deploy 在所有節點中安裝 ceph。root@ceph-node0:~# ceph-deploy install ceph-node0 ceph-node1 ceph-node2 ceph-node3 ceph-node4 安裝完成後，查看 ceph 的版本。建立 monitorroot@ceph-node0:~# ceph-deploy mon create-initial 若是您在 作上述 command 時，發生錯誤，可以去下列網址 debug，極推。https://blog.whsir.com/post-4604.html 安裝完後，在 /etc/ceph ，應該會看到多了一些檔案。 接著我們看一下目前 ceph cluster 的健康狀態與其他配置細節。root@ceph-node0:~# ceph -s 可以看到目前不管是 pools, PG, OSD，都是空的 mon 目前也只有 1 個 ，下面將會介紹如何將這些加入進去。建立 OSD (object storage daemon) 首先在建立 OSD 前，確認哪些是我們想要變成 OSD 的 硬碟。root@ceph-node0:~# ceph-deploy disk list ceph-node0 [node-name]# 格式化硬碟，使用的是 xfs 格式root@ceph-node0:~# ceph-deploy osd prepare ceph-node0:/dev/vdb &lt;other_nodes&gt; &lt;data_disk&gt;# 也可以使用這種方法格式化root@ceph-node0:~# ceph-deploy disk zap ceph-node0 /dev/vdb 查看一下剛剛建立的 OSD 有沒有加入到 ceph 中。 另外把目前全部的 OSD 列出來設定 PG (placement groups) PG 與 OSD 的有密不可分的關係，設定 PG的多寡會直接影響到 CEPH 使用時的效能。那如何設定正確的 PG 範圍呢 ? 少於 5 個OSD， 建議設為 128。 5 到 10 個OSD，建議設為 512。 10 到 50 個OSD，建議設為 4096。 50 個 OSD 以上，就需要有更多的權衡來確定 PG 數目。root@ceph-node0:~# ceph osd pool set rbd pg_num 256root@ceph-node0:~# ceph osd pool set rbd pgp_num 256 若是沒有 pool 可以建立一個。# pool name = rbdroot@ceph-node0:~# ceph osd lspoolsroot@ceph-node0:~# ceph osd pool create rbd 256root@ceph-node0:~# ceph osd pool set rbd pgp_num 256root@ceph-node0:~# ceph osd lspools建立 ceph mgr (ceph Manager daemon) 可以看到 health 的狀態都是 HEALTH_WARN，主要的原因在於沒有給 ceph 建立 mgr ，因此建立一個 mgr.root@ceph-node0:~# ceph healthroot@ceph-node0:~# ceph-deploy mgr create ceph-node0 ceph-node1 ceph-node2 ceph-node3 ceph-node4額外多建立 monitor 在一個 Ceph 集群中至少有一個 monitor 集群才能運行，但為了集群的高可用，以及避免單點故障，一般情況下 Ceph 集群中 3 至 5 個Monitor。root@ceph-node0:~# ceph-deploy mon create ceph-node2root@ceph-node0:~# ceph -s 若有出現錯誤我們必須在 /etc/ceph/ceph.conf ，撰寫 public network。root@ceph-node0:/etc/ceph# echo \"public_network = 10.1.3.0/24\" &gt;&gt; ceph.confroot@ceph-node0:~# ceph-deploy --overwrite-conf config push ceph-node1 ceph-node2 ceph-node3 ceph-node4 成功 ! ceph monitor 變成兩個了。在一個 ceph cluster 中 mon 為基數較佳參考資料 圖文並茂 ( 網址 ) 白哥的寫得很聰明 ( 網址 ) 白哥 ceph 介紹 ( 網址 ) ceph mon 錯誤介紹 ( 網址 ) 李睿的博客 ( 網址 ) 超詳細介紹 ceph ，推 ! ( 網址 )Post converted from Medium by ZMediumToMarkdown." }, { "title": "Deploy slurm on ubuntu 18.04", "url": "/posts/33abb6e902c4/", "categories": "Jackycsie", "tags": "schedule, netflow, gene, workflow, tools", "date": "2020-03-11 15:30:03 +0800", "snippet": "Deploy slurm on ubuntu 18.04因工作需求，開始研究 slurm，那什麼是 slurm 呢 ? 他是一個跨平台的流程管理系統，其主要的功能其實只有一個就是 FIFO，那下面是介紹如何透過兩台以上的 machine 將 slurm 建立起來。環境配置Controller Node jacky97Compute Node jacky98因機器數量有限就拿 2 台做實驗...", "content": "Deploy slurm on ubuntu 18.04因工作需求，開始研究 slurm，那什麼是 slurm 呢 ? 他是一個跨平台的流程管理系統，其主要的功能其實只有一個就是 FIFO，那下面是介紹如何透過兩台以上的 machine 將 slurm 建立起來。環境配置Controller Node jacky97Compute Node jacky98因機器數量有限就拿 2 台做實驗吧。Tips: 若是可以先將 ssh 設定為輸入 ip or hostname 就可以登入較佳。 目前自己實測 slurm 的版本必須相同，另外也是使用相同的 Ubuntu 18.04，不同的 OS version，盡量改成相同的 slurm tool version。 在未來的 slurm.conf 與 munge 的 key 都是所有的 machine 都要有相同的一份，並且啟動與關閉時，確保每一台 machine 都有相同 state，較不容易出錯。Install Munge ( every node, machine) M UNGE is an authentication service for creating and validating credentials. It is designed to be highly scalable for use in an HPC cluster environment. It allows a process to authenticate the UID and GID of another local or remote process within a group of hosts having common users and groups. These hosts form a security realm that is defined by a shared cryptographic key. Clients within this security realm can create and validate credentials without the use of root privileges, reserved ports, or platform-specific methods. Download mungewget https://github.com/dun/munge/releases/download/munge-0.5.14/munge-0.5.14.tar.xz 解壓縮, installtar xJf munge-0.5.14.tar.xz \\ &amp;&amp; cd munge-0.5.14 \\ &amp;&amp; ./configure \\ --prefix=/usr \\ --sysconfdir=/etc \\ --localstatedir=/var \\ --runstatedir=/run \\ &amp;&amp; make \\ &amp;&amp; make check \\ &amp;&amp; sudo make install 這步非常的重要，首先我們先在 controller node, create key 然後將 key 複製到每一個 node 的資料夾。jacky97: create-munge-key 複製 key 到每一個 node 上，我們這邊是透過 scp 的方式。jacky97: scp /etc/munge/munge.key root@jacky98:/etc/munge/ 接著在每一個 node 上啟動 munge/etc/init.d/munge start 啟動成功後，來驗證一下本機是否成功使用 munge。jacky97: munge -n | unmunge 接著驗證一下 compute node (jacky98)jacky97: munge -n | ssh jacky98 unmungeSTATUS: Success (0)ENCODE_HOST: jacky98 (127.0.1.1)ENCODE_TIME: 2020-03-11 14:11:20 +0800 (1583907080)DECODE_TIME: 2020-03-11 14:11:21 +0800 (1583907081)TTL: 300CIPHER: aes128 (4)MAC: sha256 (5)ZIP: none (0)UID: root (0)GID: root (0)LENGTH: 0正常的模式應該要每一個 node 都可以驗證到另一個 node ，才合理。Install slurm安裝 slurmsudo apt update -ysudo apt install slurm-wlm slurm-wlm-doc -y 安裝完成後 check slurm 在每一個 node 保持一致。jacky97: slurmd -Vslurm-wlm 17.11.2 透過 slurmd -C 指令了解這個 node 的硬體配置。jacky97: slurmd -CNodeName=jacky97 CPUs=32 Boards=1 SocketsPerBoard=2 CoresPerSocket=8 ThreadsPerCore=2 RealMemory=128900UpTime=0-04:46:35 設定 slurm configure設定 config 有兩種方式: 透過 官方提供的 https://slurm.schedmd.com/configurator.html ，選擇你想要的內容案送出就會有 slurm.conf ，接著 copy 到每一個 node 就可以了。 使用自己的 config，然後在每一個 node 上放入 slurm.conf。放入的位置是 /etc/slurm-llnl/ 。 這裡要注意的地方在 ControlMachine 就是 control node 設你的 hostname， NodeName，有幾個 control node 就設幾個 Node name，最後最重要的是 PartitionName 的 Nodes 必須把想要 run 的 node 都寫進去，不然會偵測不到。# slurm.conf file generated by configurator easy.html.# Put this file on all nodes of your cluster.# See the slurm.conf man page for more information.#ControlMachine=jacky97#ControlAddr=##MailProg=/bin/mailMpiDefault=none#MpiParams=ports=#-#ProctrackType=proctrack/pgidReturnToService=1SlurmctldPidFile=/var/run/slurm-llnl/slurmctld.pid#SlurmctldPort=6817SlurmdPidFile=/var/run/slurm-llnl/slurmd.pid#SlurmdPort=6818SlurmdSpoolDir=/var/spool/slurmdSlurmUser=root#SlurmdUser=rootStateSaveLocation=/var/spool/slurm-llnlSwitchType=switch/noneTaskPlugin=task/none### TIMERS#KillWait=30#MinJobAge=300SlurmctldTimeout=3600SlurmdTimeout=300BatchStartTimeout=3600PropagateResourceLimits=NONE### SCHEDULINGFastSchedule=1SchedulerType=sched/backfillSelectType=select/linear#SelectTypeParameters=### LOGGING AND ACCOUNTING#AccountingStorageType=accounting_storage/noneClusterName=cluster#JobAcctGatherFrequency=30#JobAcctGatherType=jobacct_gather/none#SlurmctldDebug=3#SlurmctldLogFile=#SlurmdDebug=3#SlurmdLogFile=## AcctAccountingStorageEnforce=1AccountingStorageLoc=/opt/slurm/acctAccountingStorageType=accounting_storage/filetxtJobCompLoc=/opt/slurm/jobcompJobCompType=jobcomp/filetxtJobAcctGatherFrequency=30JobAcctGatherType=jobacct_gather/linux## COMPUTE NODESNodeName=jacky97 CPUs=32 State=UNKNOWNNodeName=jacky98 CPUs=32 State=UNKNOWNPartitionName=debug Nodes=jacky97,jacky98 Default=YES MaxTime=INFINITE State=UP 與配置文件對應，創建相應的目錄和權限設置rm -rf /var/spool/slurm-llnlmkdir /var/spool/slurm-llnlchown -R slurm.slurm /var/spool/slurm-llnlrm -rf /var/run/slurm-llnl/mkdir /var/run/slurm-llnl/chown -R slurm.slurm /var/run/slurm-llnl/ 啟動和設置開機自啟systemctl start slurmdsystemctl enable slurmdsystemctl start slurmctldsystemctl enable slurmctld Check 目前 slurm 的狀態jacky97: sinfo slurm 關閉systemctl stop slurmdsystemctl disable slurmdsystemctl stop slurmctldsystemctl disable slurmctld Slurm FIFO Experiment執行 Slurm 可以透過三種方法，最常用的是 sbatch, srun, salloc，後面可以接 shell script, command 都可以。sbatch bwa_script.sh在這裡可以看到，當平常我們使用 slurm 時，check squeue他的狀態，納線圖就可以知道他的狀態。 JOBID : 目前您的 job 排的序位。 ST : R 代表執行中, PD 代表 hold 住 等目前的跑完就換他了。 TIME : 表示目前跑了多久。 NODELST (REASON) : 代表正在跑的 node 有誰。jacky97: squeue若有時候 slurm 出現問題，可以重新啟動 slurm/etc/init.d/slurmd stop參考文獻 Deploy slurm on Ubuntu 18.04 ( single machine ) .2. Multiple server deploy ( website ) .3. Ubuntu 18.04/Mint 19 单机安装Slurm .4. Munge 安裝檔 .5. 超完整 slurm 介紹，不推不行 ( website ) .6. Slurm 中文官方網站 ( website ) .7.Slurm 英文官方網站 ( website ) .8. Slrum 框架介紹圖 ( website ) .9. 簡單小筆記 ( website ) .10. Partition introduce ( website ) .11. slurm execute job log record ( website ) .這篇專注在安裝跟基礎教學，非常推薦看第 5 個連結，非常容易懂，若有問題可以在下方留言，或者想要看甚麼類型的 Demo 都可以 17 討論喔。Post converted from Medium by ZMediumToMarkdown." }, { "title": "訂閱經濟 Subscribed", "url": "/posts/7d5262a49041/", "categories": "Jackycsie", "tags": "business-model-innovation, subscription, data-science, scrum, life", "date": "2020-03-01 21:51:40 +0800", "snippet": "訂閱經濟 SubscribedTL;DRBook:今天跟大家分享的書，名叫訂閱經濟，而為什麼想看這本書呢，是因為最近看 youtube 時，常常聽到 youtuber 在開頭說的第一句話，“別忘了觀看影片前，先記得按讚，訂閱，加分享”，這句話這 1, 2 年時常出現在我們的生活中，因此當我在逛書店時，看到這本書，就知道是它的，我決定透過這本書去了解，目前的科技是如何試圖顛覆過往舊有的傳統價值...", "content": "訂閱經濟 SubscribedTL;DRBook:今天跟大家分享的書，名叫訂閱經濟，而為什麼想看這本書呢，是因為最近看 youtube 時，常常聽到 youtuber 在開頭說的第一句話，“別忘了觀看影片前，先記得按讚，訂閱，加分享”，這句話這 1, 2 年時常出現在我們的生活中，因此當我在逛書店時，看到這本書，就知道是它的，我決定透過這本書去了解，目前的科技是如何試圖顛覆過往舊有的傳統價值觀念的一本書，而念完後的感想的確，他做到了，這本書試著透過自身過往的經歷，去闡述訂閱經濟是如何改變你我的生活的，而接下來我們總共會分 3 個段落，去初探，如何做到訂閱經濟的新商業思維。訂閱經濟：如何用最強商業模式，開啟全新服務商機 推薦序 台灣企業數位轉型契機就在其中… www.books.com.twOn your left :1. 訂閱思維2. 陷入沼澤3. D-day訂閱思維當你進入一家車商，你聽到你想買的車子只能是黑色，並且所有款式都一樣時完全無法選擇時，你有什麼想法？過往的工業革命，讓我們因為工廠的自動化而便利了不少，而這 200 多年的時間，從全自己手工的農業時代，轉成日本達人類型的各司其職的工業時代，這緩慢地成長中，讓我們學會了壓低成本，增加生產線，加大產量，提升通路，將家具提升到了萬物通電的層級；有感覺了嗎 ？你會跟朋友講誒我家有電燈誒，不是蠟燭喔～ 未來的 20 年，你還會說你的產品有加入 AI 嗎 ？這就是數位轉型，現在我們天天喊 Siri 打給我女朋友，然後你根本沒有女朋友，就像是劉銘傳在那邊玩路燈，一直開關一樣有趣，然後那些蠟燭燈籠的業者呢？ 然後就沒有然後了，幾千年的傳統，就在劉銘傳玩開關燈的過程中慢慢消失。 我們需要的是那光芒，還是需要那蠟燭或 AC, DC？產品皆能訂閱這就是訂閱經濟的核心，買東西的核心目標，想要的是什麼，我們是想要看一個高品質的電影，還是想要去百視達租 ＤＶＤ？或者是我們想要聽很多的最新音樂還是想要大家去五大唱片行，把想聽的專輯都買起來？我們是想要廚房，還是想要好吃的食物，是想要快速的從Ａ點到Ｂ點還是我想要一台車…，等等；這就是我們做訂閱服務時最先會思考的問題，那有什麼東西是不能做的呢，還是貧窮限制你的思考？個人化既然我們擁有了燈，那這燈能做什麼，在我想睡覺前 1 小時的時候，能否轉換成暖色舒服的光，當六日在家打電動時，知道我的習慣，透過燈光保護我的眼睛，變成舒服的光線，在一段時間房間沒有動靜時，能否幫我自動關燈達到環保節能的功用？這就是物聯網，可以透過收集習慣達到更多的個人化的服務。這就是訂閱經濟另外一個核心重點，也是命脈，你覺得像大苑子賣一杯飲料多難，做一個像 youtube 一樣的框架頁面非常難嗎 ，甚至是跟 Uber 一樣，當個小黃司機的開發平台，是否真的非常困難，那為什麼他們總有一些地方不一樣？那這些做的事產品，還是服務？是一次性的，還是永續不斷，這讓我們想到了台灣的經營之神永慶哥，他的白米怎麼可以宇宙不同，知道別人什麼時候快吃完米了，需要在買新的一抖了？那怎樣才能做到個人化，作者做了一個非常好的圖片，如下：過往我們去全聯買吃的以後，店員跟您說，您好歡迎光臨，逼逼逼結束了，付錢，全聯知道您喜歡吃買什麼嗎？那現在呢？店員：您好有福利卡嗎？阿罵：我沒有福利熊誒？店員：阿罵是福利卡拉～店員：逼逼逼有福利點數需要扣掉嗎？阿罵：喔喔，省錢的都好啊。店員：阿最近沙拉油買一送一誒，要不要帶個兩罐回去比較划算。阿罵：五星級(閩南語)，賺！那這過程，他出了多了一隻福利熊還多了什麼？全聯知道您喜歡什麼了？在未來的ＡＰＰ當中，可以推播您喜歡的產品，了解您對哪些東西很討厭，知道您對什麼東西有興趣，這就是數位轉型，透過搜集更多的習慣，幫助會員做好的消費體驗，這就是本節的核心， 訂閱服務的核心在於數位轉型，數位轉型的核心在於數據處理。大家的重點一定在那隻熊熊。陷入泥濘我們是否偶爾在看知識型 youtube channel 或者是書時，學到很多覺得很不錯的東西想要執行，但為什麼跟看起來差的那麼多呢？本章 “陷入泥濘”，就是在這描述書中你會轉型失敗的地方，讓你可以看過去去避免失敗，就跟 ＡＩ一樣，這裡列舉了 5種最常失敗的原因。 數位轉型資料整合失敗(資訊部門) 過往的成就，變成未來的負擔 (ERP) 公司部門溝通失敗(行銷，會計，資訊) 硬體公司訂閱經濟轉型 無法拿到執行權(董事會)1. 數位轉型資料整合失敗(資訊部門)訂閱服務的兩大重點，資料、敏捷，缺一不可。 不要用戰術上的忙碌，掩蓋戰略上的懶惰資料：但其實目前非常多的技術人才就是如此，在我們這邊也喜歡稱為碼農，從資料的收集到如何使用的整個 workflow ，是需要經過精心策劃，若是東放一塊，西丟一塊，沒有完整的ＥＴＬ，Dashboard，Report，沒有對內外明確地公開，以及如何使用，並且每個月提供報告給所有可能需要使用到資料的人，讓大家對資料提出更多一件，而只從在幾個人的頭腦裡，那這些技術人才，是非常不負責任的，但不能怪他們，只能將資料知識傳授給大家，因為沒見過車的人，只會想要一匹更快的馬。敏捷：近幾年非常熱門的思維，但基礎要打好就是上述的數位思維，快速回饋，快速整合、佈建，這有多快取決於你的公司部門有多新，如果你有 ERP 系統，目標又是客戶關係服務資料探勘，我也只能說辛苦你了。2. 過往的成就，變成未來的負擔 (ERP)相對於新創公司，根本不會有這問題恭喜你們，因為你們打的就是游擊隊，那建立在過往成功的大公司為什麼他變成一個移動緩慢的巨人？答案是他無法敏捷，因為公司有太多套的系統使用，每一套是功能導向，而不是人為導向，這就是工業時代跟個人化服務最大的差別，我們也想敏捷但每做一個 ＭＶＰ需要的時程太常，沒有快速回快更改，甚至關鍵期已過，假設我們是頂好超市的資料科學家，我們要知道誰最常買我們的東西，買的內容是什麼？又或者是我們是捷安特腳特車資料科學家，ubike 整年度每一季掉的貨的特色會是什麼 ？我們有可能知道嗎？不可能，那新的服務模式呢？會的，會的。所以本書的作者在這裡提到了 B2B, B2C, B2A 未來會越來越模糊，因為從資料科學街的核心價值看都是一樣的，只有商業策略跟服務會是不同。 我們賣的再也不是產品，是服務是你提供的價值。3. 公司部門溝通失敗(行銷，會計，資訊，市場)相信許多大公司在公司過去的工業模式非常的猛，員工各司其職，互不干擾，專注自己專長的工作，但訂閱服務的開始，行銷也需要看報表，會計也需要看報表，市場也需要看報表，大家最後的回饋點都是一個地方，資料科學家，但資料科學家又會心裡罵三字經，這我哪知道要跟會計，市場討論完才能跟你說，而這就會早成打世足賽一樣，球踢不完，問題呢還是沒解決。之後會寫作者怎麼解決的。4. 硬體公司訂閱經濟轉型硬體公司，該如何轉型，我們常常因為單價高，過去大眾購入時都時常容易卻步，近年來千禧年與訂閱新思維的轉變，讓產品越來越難賣，但是為什麼我們改成用訂閱的想法，成長還是十分緩慢，甚至負成長呢？我們是否有沒有思考到的環節呢？之後會寫作者怎麼解決的。5. 無法拿到執行權(董事會, 高層)如果你已經解決了上述的 4 大常見問題，非常恭喜你，跟作者一樣變成超級猛的大大，但你用了洪荒之力，改變了所有商業模式接下來，還有一個難題，你該如何量化你的數據，因為我們都知道過去的幾百年，會計模式都是依照同一種方法，我們過去賺到的利潤扣除掉成本剩下多少，但這些利潤並沒有計算到每個月如果你是獲取長時間的估定利潤時該怎麼計算，自己計算都不好計算，董事會高層又怎麼知道你的 business model 呢？所以就很容易被打槍。Ｄ- day 成功就是從失敗到失敗，也依然不改熱情。 — 邱吉爾我們看待訂閱經濟，就像是 你每 10 場比賽多打出 1 支安打，你的打擊率就會從 0.275 升高到 0.30 (11/40, 12/40), 我們並不需要一次把所有想要的事情做完，我們是游擊隊，透過後方完整的補給對，與前方戰術的彈性去將各個用戶會員一一了解。那我們的戰略是什麼，在本章節中我們會分 4 大目標進行探討，這四大目標，全部皆可以可大可小，這就是戰略的目的。 目標制定 內部改革 4P 定位 Money ball 沒有戰略的企業就像一艘沒有舵的船，只會在原地轉圈。 — 喬爾·羅斯1. 目標制定剛開始我們的目標不需要很大，而是很彈性，例如我們是電腦硬體商 Cisco，那我們該如何提升我們的客戶黏著度，因為硬體在 AWS, GCP, 的猛攻下，所需要的服務資源越來越少，我們透過硬體搭費免費的軟體應用如 Dashboard, workflow, schedule pipeline, 這些基礎的服務，讓購買 Cisco 的顧客，更聰明的使用這些服務，接著我們透過這些免費的軟體客戶，去打造付費的貼心小服務，如: optimizae SDN ML, 免 AD, 更多 machine 的 apply, 透過數據去達到會員與公司的雙向溝通，進而提供更多的個人化服務，未來這些免費的會員，去擴張更多的付費會員，接著再更後期可以做更多的加值服務與 upper selling.當失敗反應不好的服務可以透過數據改善，或者直接砍掉練新的，這就是敏捷開發跟快速佈建，每一季推出的產品的是測試版，都為了追求更好的服務品質而生，若是龐大的 ERP 系統，也沒關係，轉型慢慢來，我們以最小的 MVP(Ｐ是 prototype)，慢慢進步，收集數據，進而轉型，也可以，只是比較辛苦而已，就是要撐過下面的圖，可長可短，難的地方在於如何了解目前客戶的飽和度。2. 內部改革若是你是程式人員，你一定寫過，bug 測試，那 bug 測試是你要負責，還是測試部門負責，兩邊都負責一部份對吧？那若是有人的測 bug 能力很差，另一遍就會變得很累，就容易怪罪於另外一邊導致公司的環境氛圍越差，產品尚且如此更何況全新的訂閱服務團隊，那我們該怎麼解決上述的問題，最重要的地方我還是要專注在訂閱，透過訂閱的思維建立團隊，這團隊在未來人數提升時，你們才會有制度，才會知道如何討論，那作者提出了一個大膽的想法，如下圖。這時候一定會黑人問號，媽媽的這是一個資料團隊嗎？還是整個 RD 部門，年輕人終究是年輕人，這是訂閱服務式的新部門思維，每一個紅字都是一種服務，每一種服務裡面可能有 金融背景、行銷背景、資訊背景，對服務來說我們重視的是每筆資料近來後有不同的解讀方法及進步模式，這 service circle 改善了過往踢皮球的想法，明確定義出你所處的階段，就跟你是後衛跟人家搶籃板一樣，很怪，又或者說，你是 2 壘手，跑去中外野搶接球，不合理吧～這種 PADRE 或許是服務思維的新部門架構。3. 4P 定位 Product Price promotion place當 product 變成 service ，我們的整體通路商該如何改變，在本章中作者提到許多的核心做法，首先我們必須學會如何使用黃金圈進行交流，第二我們透過不同的會員採用更直覺更友善的定價方式，最後再透過兩大方法消費趨動性成長與能力驅動性成長，幫助會員得到更符合他的服務。軟體：從線上做到線下，將網路的個人化服務，轉換成個人的使用者體驗，因為服務容易學習，但體驗只有一次，就跟你做到藍寶傑尼的車時，或者是Tiffany的戒指，那興奮跟感動，無法用言語說明，但黃金圈已經幫你介紹完了。硬體：從線下，轉為線上，過往硬體容易失敗的原因在於，無法快速訂閱，快速解除，通路商少，長期壟斷，缺少更直覺的軟體操作，現在您在使用 uber，特斯拉的時候還會有這個困擾嗎，例如我想要做一台豪華的車載我從台北到機場捷運，需要多少個步驟？當我想要吃晚餐，懶得出去買叫 食物黑熊時所需要的步驟又需要多少？4. Money ball我們該如何計算每月每季或每年都會有的固定收入，這在過去 2,300 年中，是沒有這種計算方法的，因為過去我們的算法都是透過一季的結算去思考，我們花了多少成本在拿到我們所需的獲利，那我們要怎麼去計算新的記帳方式幫助投資者，董事會，更清楚洞悉知道我們的業務內容是什麼？下面有兩張圖可以快速的知道我們追求的是什麼。網址： 提勒簡報 詳細內容。簡而言之就是的比喻就是我們有 100 個人訂閱你的產品 10 % 是付費會員，你的人事成本佔了你的 5% ，那你剩下的 5 % 經常性收入你要怎麼去運用它，若是全部投入下去我們每次結算就會有 10 % 的成長，10 % 投入到未來就會有 20 % 的成長。在還未有平原期時，作者建議可以透過多項策略，盡量的 over pay 來獲取更多評論與使用者回饋，因為醒醒吧你不是在做產品了。那平原期的下一步呢，收購，來獲取到更廣泛的會員，接著透過 cross selling 達到更高的訂閱行為。訂閱服務的上一步…數位轉型:其實目前最難做的還是兩步數據使用與敏捷開發，許多人都是非資料科學相關系所，他們或許不知道什麼就做軟體開發，但正統出身的我們不跟我們團隊說明是我們的問題，DevOps, Data management, scrum, 許多的中老年們喜歡提醒我們千萬別眼高手低，我們也努力跟著前輩的教誨，幫助這社會盡一份心力，那沒有做上述這些的兩項核心內容，你就是眼高手低，因為你沒有數據文化，你只有工業文化。新創企業非常的幸運，因為跳入新的產業你不會用舊的東西，更容易搶到更多肥缺的服務，搶佔市場，等著被收購或者跨大，就跟你還會說菲律賓在過去比我們先進也創造過許多的 奇蹟 嗎？ 電的普及不是用愛，就跟資料的普及不是用想的。謝謝，耐心看完了整篇，當中我可能只寫了 10 % 的概念，把我吸收到想傳達給大家的寫下來，最後謝謝作者，讓我們又學到了一課。 If we try to play like the yankees here, we will lose to like yankees out there.Post converted from Medium by ZMediumToMarkdown." }, { "title": "Gene Workflow Platform (Galaxy)", "url": "/posts/48082a569d9/", "categories": "Jackycsie", "tags": "gene, workflow, automotive, netflow", "date": "2020-02-26 16:08:51 +0800", "snippet": "Gene Workflow Platform (Galaxy)為什麼想寫這篇 ?Galaxy 是一個基因流程平台，那他有分 online and offline，那為什麼想要寫本篇呢 ? 因為他跟 IGV tool 一樣複雜，另外 google 了一下，發現中文的資源超少，那就來當成筆記，順便跟大家 17 討論吧，本篇寫的是 offline 的安裝教步驟，一步步的跟大家做完 offline ...", "content": "Gene Workflow Platform (Galaxy)為什麼想寫這篇 ?Galaxy 是一個基因流程平台，那他有分 online and offline，那為什麼想要寫本篇呢 ? 因為他跟 IGV tool 一樣複雜，另外 google 了一下，發現中文的資源超少，那就來當成筆記，順便跟大家 17 討論吧，本篇寫的是 offline 的安裝教步驟，一步步的跟大家做完 offline 的方法。但本篇暫時不使用 docker 喔 ， 而是用 virtualenv。Step 0 Go to official website:Step 1 Download galaxy:因為 project 超大所以需要 clone 一段時間，目前用的版本是 19.09。git clone -b release_19.09 https://github.com/galaxyproject/galaxy.gitStep 2 set up envirment :我們需要建立一個虛擬環境，與真實環境隔離。apt install virtualenvvirtualenv .venv剛開始 config 的時間需又很久喔。cd galaxy/sh run.sh好了以後可以輸入 http://localhost:8080 就可以看我們進入到 galaxy 平台了。Step 4 修改 config 檔:因為 galaxy 需要設定的東西真的有點小多，啊我的目的就只是為了快速用平台阿，在那邊盧真的很煩，又不是要玩系統，有時候多覺得還不如自己下 command 來的快。cp config/galaxy.yml.sample config/galaxy.ymlstep 4-1 改成區域網路都可以連:vim config/galaxy.ymlstep 4–2 將帳戶改成最高權限:vim config/galaxy.ymlstep 4–3 重新 run 1次:sh run.sh成功了 ，狀態列上也有 Admin 的選項。step 5 下載工具 :拿到了最高權限後我們來示範怎麼下載工具。首先點 Admin &gt; Install or Uninstall &gt; 輸入 fastqc ，選擇你想要下載的 fastqc version 就可以囉。成功。Step 6 實驗使用 Fastqc :選擇您要的 fastq 檔案，然後按執行。執行完後我們會看到怎麼全部都是亂碼，因為我們並沒有將 fastqc 加入白名單，因此來跟大家介紹怎麼加入。首先點 Admin &gt; Manage whitelist&gt;尋找 FastQC ，點選後按 submit 。圖片都成功回來了。結束初步的介紹大概就是這樣，還是不懂明明應該要更容易使用的 GUI，卻把它說明文件卻搞得很複雜，好險做這個人的看不懂中文，不然他們關掉這個就少了一個好用的 GUI 了。之後還會陸續增加覺得很煩的設定，讓大家盡量專注在 gene 研究的身上比較好。Post converted from Medium by ZMediumToMarkdown." }, { "title": "2020 目標", "url": "/posts/d93b1dfb558c/", "categories": "Jackycsie", "tags": "life", "date": "2020-01-16 22:42:32 +0800", "snippet": "2020 目標2019 年就這樣過去了，今天時間過得超快，感覺沒做什麼事就過完了，剛好許多人最近都在寫 2020 的目標，那我也把原本在思考的方向寫出來，方便未來做紀錄吧！2019 回顧首先先來回顧 2019 年，這是我工作的第一年，當初的我對新的一年每有太多的目標，只希望過工作順利，並且能學到東西即可，而過了 2019 給自己的評分是 70 分，不算太滿意的一年，但是是剛好及格的一年。今年...", "content": "2020 目標2019 年就這樣過去了，今天時間過得超快，感覺沒做什麼事就過完了，剛好許多人最近都在寫 2020 的目標，那我也把原本在思考的方向寫出來，方便未來做紀錄吧！2019 回顧首先先來回顧 2019 年，這是我工作的第一年，當初的我對新的一年每有太多的目標，只希望過工作順利，並且能學到東西即可，而過了 2019 給自己的評分是 70 分，不算太滿意的一年，但是是剛好及格的一年。今年所得到的成長在金融，科技，行銷，溝通，商業，外語，而失望的地方在於技術，沒有大幅度有系統性的成長，在這部分扣了非常多的分數，台北的 workshop 只參加過 2 場，在工作上 feedback 太少，時常做了一些失望的表現，leadcode只寫 2 題就沒寫了，程式書籍只看了 1 本，回顧看了問題，一看就可以得知沒有訂定明確清楚的目標，另外一個原因是從高中開始都有1 個技術非常強的人在身旁，可以不斷的學習討論，但來了這新環境沒這個人的出現，使我的整個技術處於非常失望的一個結果，而這一切從外面看回來非常的荒腔走板，十分失望，好險有多元化的學習，拉了一點分數回來。2020 目標前言在 2020 年打算分幾個項目達成目標，而這幾個項目會先設定，全部都能達到目標的為主，所以看似可能簡單，但持之以恆才是核心。書籍關於書籍這項，目標是 2 個月 1 本資訊科技以外的書，1 本技術的書，然後每一本書都寫到 medium 中，將看完的思緒想法寫下來，其他月份還沒想好，等確定了開始看了在開始寫，而執行的方式就是每天看 30 分鐘的書。Jan. 訂閱經濟 , 無瑕的程式碼：敏捷軟體開發技巧守則Mar. 金錢心理學 , 無瑕的程式碼－整潔的軟體設計與架構篇May. 貝佐斯寫給股東的信 ,另外會不斷會一直重新看的有幾本。 華頓商學院最受歡迎的談判課 高績效人士都在做的8件事運動當完兵後，有嘗試維持一段時間每週四做有氧拳擊，但後來因為健身房價錢考量，跟本身自己懶惰的關係，大約半年沒有運動了，因此連走路都會喘 XD ，希望未來每週能去最少兩次的健身房，預計是禮拜一，禮拜三，若是可以星期四也會去一下(因為公司有時會請舞台劇來表演，會去看)。LeetCode我的 tech lead，在開會時，提及雖然目前我們做的技術並非以 coding 為主，但還是要持續保持 coding 開發的思緒，而目前能提供我持續提升程式能力的就是使用 LeetCode ，但為了避免寫到一半就會沒有熱情，今年預計寫 50 題就好，簡單少少的來換取複利的轉換。而預計將會如下面的配題，預計會分 12 個月，前面先培養感覺，再慢慢的培養習慣。1 2 2 3 4 5 6 5 6 5 6 5。生涯專業能力在過去的我，時常發現自己屬於什麼都略懂，什麼都不熟深入，但思考非常久的一段時間，總共有三個方向可以深入； 資料科學家(目前工作量最少，但卻是最喜歡，然而卻基礎最差的地方，就等於你喜歡唱歌不代表唱得好)。 資料工程師(覺得也不錯，但是目前工作上最沒有需求的地方，所以不好練功)。 infra 工程師(還不會定位，興趣普通，但成就感極高，能學習到的也非常多，因為部門團隊的前身就是以 infra service 為主)。從上述的分析後，顯而易見的可以得知，什麼方向對我的職場生涯是有幫助的，因此我會在這部分會去問資深的 tech lead，請他在未來生涯中有什麼建議方向(問完以後，再補上想法)。Github Co-work : 跟 Tech lead 討論後，發現需要快速熟悉暸解 git 的整個 pipeline，以及好的 coding style ，最好的方式就是透過有興趣的 github 一起 co-work ，因此在今年會以 1 個 Github project ，嘗試跟著整個流程，試著更佳清楚，整個 git flow 流程，對生涯的未來性比較高，也可以認識更多大大。技術文章就像我們 Tech lead 說的，自己認為會技術，沒有跟大家講述一遍，往往不容易發現自己的問題點，因此為了讓自己更佳清楚目前的技術是否真的都會了，會努力的寫相關的技術文章，另外寫文章也可以當作一種筆記，方便未來回顧當初的想法，是一個非常有趣的體驗；下面是對這年技術文章的目標。 中文的技術文章 10 篇(包含系列文章) 英文的 技術文章 5 篇英文思考了兩週，始終找不到可以有效提升英文程度的方法，所以為了持續的培養語感，就每天念空中英文雜誌 1 回，在邊唸邊找，有沒有對自己更有成效的方式唸英文吧。結論寫了這麼多，花了非常久的時間思考可行性，最重要的還是執行力，就跟棒球一樣，期待明年回顧的結果，以及看看自己成長了多少。Post converted from Medium by ZMediumToMarkdown." }, { "title": "How to download big file ?", "url": "/posts/c17688745a5d/", "categories": "Jackycsie", "tags": "technology, axel, multithread, ubuntu, tools", "date": "2019-12-04 10:09:10 +0800", "snippet": "How to download big file on ubuntu?If you use ubuntu systems, we usually use wget as a must-have tool for downloading files.However we know wget is a one process tools.So if we need download big fi...", "content": "How to download big file on ubuntu?If you use ubuntu systems, we usually use wget as a must-have tool for downloading files.However we know wget is a one process tools.So if we need download big file. wget will be very slow.I recommend a tool called axel.Install axelsudo apt updatesudo apt install axelaxel --helpDownload fileAs an example, I downloaded the 831 MB file.wget need use 4 hr when network speed have 1 Mbit/s.However when we use axel spent time only 5 minutes.The command will look like this:axel -n 80 -S5 http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.gz -n : use 80 thread accelerate download -S5 : This means that the file search engine filesearching.com is used to find the image file in order to increase the download speed -o : save file pathDownload FTP fileSometimes we need use ftp download files.So how to use axel download ftp file ?axel -n 32 -S5 \\\"ftp://gsapubftp-anonymous: @ftp.broadinstitute.org/bundle/b37/NA12878.HiSeq.WGS.bwa.cleaned.raw.subset.b37.vcf.gz\" ftp:// : you use protocol gsapubftp-anonymous : username :password : ftp password @ : we want to download urlEndNext time I will talk you how to download GCP file using mutlithread.Post converted from Medium by ZMediumToMarkdown." }, { "title": "Ubuntu 18.04 Deploy GPU Environment", "url": "/posts/196ebea555d3/", "categories": "Jackycsie", "tags": "ubuntu, gpu, v100, tensorflow, tools", "date": "2019-11-22 16:03:57 +0800", "snippet": "Ubuntu 18.04 Deploy GPU EnvironmentThis article will used smart way install GPU environment.Why so smart ? Because I used a rather stupid way.This method is stupid method “ deepvariant install”.Why...", "content": "Ubuntu 18.04 Deploy GPU EnvironmentThis article will used smart way install GPU environment.Why so smart ? Because I used a rather stupid way.This method is stupid method “ deepvariant install”.Why stupid. Because this way sometime will fall.This is very angry.Because we want focus deep learning or other professional field not environment issue.About machineWe are using the D52G provided by Quanta Computer.Environment inside: 80 vCPU 8 V100 GPU 768 GiB RAMEnvironmental configuration Ubuntu 18.04 LTS Nvidia driver 440.33.01 CUDA 10.0 cudnn 7.6.3 Tensorflow GPU 1.131. you need clean your environment.If your machine is not Re-irrigation system.You can skip this step.apt-get remove --purge '^nvidia-.*'apt autoremoveapt update2. install Nvidia driverAdd repository to your aptwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.debdpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.debapt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pubapt updatewget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb3. Check your ubuntu-driversIf ubuntu-drvers have nvidia drivers you can direct download.ubuntu-drivers devicesYou can choose autoinstall or own want version.#autoinstallubuntu-drivers autoinstallReboot your systemrebootTestingnvidia-smiYou can see this figure show CUDA version:10.2.It’s ok. because It’s wrong.Finally, it will still be based on your installation.4. Install CUDA 10 and Cudnn 7.6.3First you need go this page download cudnn.Then, go to you download folder.This command will be easy to install cudnn.apt install --no-install-recommends cuda-10-0Add environment variablesvim ~/.bashrcexport PATH=$PATH:/usr/local/cuda/bin/;source ~/.bashrcTestingnvcc -VInstall Cudnndpkg -i libcudnn7_7.6.3.30-1+cuda10.0_amd64.debdpkg -i libcudnn7-dev_7.6.3.30-1+cuda10.0_amd64.debdpkg -i libcudnn7-doc_7.6.3.30-1+cuda10.0_amd64.deb5. install tensorflow-gpu 1.13.1pip install tensorflow-gpu==1.13.1pip show tensorflow-gpuTestingimport tensorflow as tftf.test.gpu_device_name()It’s work.FinishReference https://www.tensorflow.org/install/gpu#software_requirements Chinese article https://developer.nvidia.com/nvidia-tensorrt-6x-download https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/ http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/Post converted from Medium by ZMediumToMarkdown." }, { "title": "Deepvariant Deploy", "url": "/posts/d73d983e62b2/", "categories": "Jackycsie", "tags": "deepvariant, gene, tools", "date": "2019-11-19 16:28:14 +0800", "snippet": "Deepvariant DeployThis article will tell you how to use deepvariant from nvidia driver.Sponsor:We are using the D52BV provided by Quanta Computer.Environment inside: 80 vCPU 4 T4 GPU 512 GiB RAM...", "content": "Deepvariant DeployThis article will tell you how to use deepvariant from nvidia driver.Sponsor:We are using the D52BV provided by Quanta Computer.Environment inside: 80 vCPU 4 T4 GPU 512 GiB RAMhttps://www.qct.io/product/index/Server/rackmount-server/GPGPU-Xeon-Phi/QuantaGrid-D52BV-2UEnvironmental configuration Ubuntu 18.04 LTS Nvidia driver 410.129 CUDA 10.0 cudnn 7.6.3 Bazel 0.19.2 Tensorflow 1.13 DeepVariant 0.8.0 Python2.7 Java 1.8Install Nvidia driverBefore you install nvidia driver. You need go to this page download program.sudo chmod u+x NVIDIA-Linux-x86_64-410.129-diagnostic.runsudo ./NVIDIA-Linux-x86_64-410.129-diagnostic.runIf you install fail or you lack some tools, this command is uninstall driver.sudo ./NVIDIA-Linux-x86_64-410.129-diagnostic.run --uninstallThen, follow below command.nvidia-smiYou can see this figure.Congratulations, you did it.By the way. I delete GPU UUID, so you can’t see it.Install CUDA 10.0First you need go to this page download CUDA 10.0And follow below command.sudo dpkg -i cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.debsudo apt-key add /var/cuda-repo-10-0-local-10.0.130-410.48/7fa2af80.pubsudo apt updateAdd environment variablesvim ~/.bashrcexport PATH=$PATH:/usr/local/cuda/bin/;source ~/.bashrcThen, input this command you can see CUDA version.This means that CUDA was successfully installed.nvcc -VInstall Cudnn 7.6.3You need go to this page download three file. libcudnn7_7.6.3.30–1+cuda10.0_amd64.deb libcudnn7-dev_7.6.3.30–1+cuda10.0_amd64.deb libcudnn7-doc_7.6.3.30–1+cuda10.0_amd64.debsudo dpkg -i libcudnn7_7.6.3.30-1+cuda10.0_amd64.debsudo dpkg -i libcudnn7-dev_7.6.3.30-1+cuda10.0_amd64.debsudo dpkg -i libcudnn7-doc_7.6.3.30-1+cuda10.0_amd64.debInstall Bazel 0.19.2Install required packagessudo apt-get install pkg-config zip g++ zlib1g-dev unzip pythonDownload bazel 0.19.2https://github.com/bazelbuild/bazel/releases/tag/0.19.2bazel-0.19.2-installer-linux-x86_64.shchmod +x bazel-0.19.2-installer-linux-x86_64.sh4./bazel-0.19.2-installer-linux-x86_64.sh --uservim ~/.bashrcexport PATH=”$PATH:$HOME/bin”Install Tensorflow GPU 1.13First you need use Python2If you are use Python3 use can change python version.ls /usr/local/python*Python --versionInstall tensorflow-gpu 1.13.1sudo apt-get install python3-distutilspip install tensorflow-gpu==1.13.1pip install setuptoolsDownload DeepVariantYou need to go to deepvariant github page.Download the deepvariant version you want.Github . We use deepvariant 0.8.0wget https://github.com/google/deepvariant/releases/download/v0.8.0/deepvariant.zipDeepvariant have three steps: Make examples Call Variants Postprocess Varianthttps://www.lizenghai.com/archives/27764.htmlMake exampleYou need go to this path:You can find make example.deepvariant/binaries/DeepVariant/0.8.0/DeepVariant-0.8.0/make_examples.zipHow to use it ?python2 deepvariant/binaries/DeepVariant/0.8.0/DeepVariant-0.8.0/make_examples.zip \\--mode calling \\--ref {fasta file} \\--reads \"{bam file}\" \\--examples \"{output_make_example}\"Call VariantsCall variants path:deepvariant/binaries/DeepVariant/0.8.0/DeepVariant-0.8.0/call_variants.zipcommand:python2 /deepvariant/binaries/DeepVariant/0.8.0/DeepVariant-0.8.0/call_variants.zip \\--num_readers 1 \\--batch_size 505 \\--outfile \"{call_variant_output} \\--examples \"{output_make_example}\" \\--checkpoint \"/deepvariant/deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model.ckpt\"Postprocess VariantPostprocess Variant path:deepvariant/binaries/DeepVariant/0.8.0/DeepVariant-0.8.0/postprocess_variants.zipcommand:python2 /deepvariant/binaries/DeepVariant/0.8.0/DeepVariant-0.8.0/postprocess_variants.zip \\--ref \"{fasta file}\" \\--infile \"{call_variant_file}\" \\--outfile \"{Postprocess_Variant_file}\"FinishPlease kindly review it and any question is welcomeThanks,JackyOne More thingIf you feel that you don’t understand how to deploy environment ?You can use this approach.This method could be use.However I didn’t try it. Maybe you can try.git clone https://github.com/google/deepvariant.gitcd deepvariantBecause very tools always using Java 1.8.However, if you use Ubuntu 18.04 defaults version is 1.11 so you need change version.update-alternatives --config javajava -versionReference https://github.com/google/deepvariant Bazel download Version check https://www.lizenghai.com/archives/27764.html D52BVPost converted from Medium by ZMediumToMarkdown." }, { "title": "機器也是色弱嗎?", "url": "/posts/99ede0ac1f33/", "categories": "Jackycsie", "tags": "deep-learning, color-blindness, tools", "date": "2019-08-27 15:08:20 +0800", "snippet": "機器也是色弱嗎?本篇文章用彩色的 MNIST 訓練，來判斷我們訓練出來的 model 是不是也是色弱 。而做這個小實驗的原因是因為前陣子體檢報告出爐，我被判定為色弱( 紅綠色盲)，因此就想說看看能不能訓練一個 model 以後幫我判斷色弱的問題，讓我測驗都能滿分 XDD。食神Step 1: 準備資料因為過往我們測試的 MNIST 都是黑白的字體，而我們要辨別的色盲測驗是屬於彩色的所以我們必須...", "content": "機器也是色弱嗎?本篇文章用彩色的 MNIST 訓練，來判斷我們訓練出來的 model 是不是也是色弱 。而做這個小實驗的原因是因為前陣子體檢報告出爐，我被判定為色弱( 紅綠色盲)，因此就想說看看能不能訓練一個 model 以後幫我判斷色弱的問題，讓我測驗都能滿分 XDD。食神Step 1: 準備資料因為過往我們測試的 MNIST 都是黑白的字體，而我們要辨別的色盲測驗是屬於彩色的所以我們必須解決這個問題。首先我們先下載 MNIST 並且把它轉為彩色，而網路上相關的文章已經非常的多了，在這個地方我就不自己手刻。https://github.com/pumpikano/tf-dann上面的這個網址是大大已經將 MNIST 轉換成彩色的文字了，跟著 github超做就會有 mnistm_data.pkl 這個 pickle檔案產生，結果圖如下這樣就可以變彩色的囉。Step 2: 訓練資料我們將從黑白變彩色的資料拿進來模型做訓練，我們所選擇的 model 是 CNN，下面的程式碼是我們所建立的 model describe 。我們的 epoch 設為 12，batch_size 設 128訓練集資料 55000 張照片。測試集資料 5000 張照片。model = Sequential()model.add(Conv2D(32, kernel_size=(5, 5), strides=1, activation='relu', padding=\"same\", input_shape=input_shape))model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2),padding=\"same\"))model.add(Conv2D(48, (5, 5), strides=1, activation='relu'))model.add(BatchNormalization())model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2),padding=\"same\"))model.add(Flatten())model.add(Dense(128, activation='relu'))model.add(Dense(num_classes, activation='softmax'))model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])Step 3: 辨別 model 是否為色盲或者色弱 那現在我們拿測試色弱的圖片餵入我們的 model 中。1. 我們拿簡單的照片來讓 model 練練手。答案出爐，第一關 model 辨識正確。2. 直接進入重頭戲，這張圖是判斷你是否有色弱。機器判斷也是為 7，如果有色弱的人會判斷為 2，就是我. . . .下圖這張若是有色弱的則會判斷成1，正常人會判斷成 4，我怎麼看它都像 1。結論在這場激烈的競爭中，機器的正確率為 50%，代表它是陰陽眼XDD，另外的意思是我不能拿它來下一次的色弱測試 QQ，看來還需要找個方法把 model 變得更好才行。下面影片是今天看到的有趣 video 。Post converted from Medium by ZMediumToMarkdown." }, { "title": "Nextflow 系列 1 (環境建置)", "url": "/posts/c016b4fae311/", "categories": "Jackycsie", "tags": "netflow, gene, biology, multiprocess, tools", "date": "2019-08-19 16:36:22 +0800", "snippet": "Nextflow 系列 1 (環境建置)本篇內容，主要介紹的是 Nextflow 如何透過 docker 安裝在自己的電腦中，並且提供一個快速簡單的範例，讓大家可以快速的學會如何建置 Nextflow。我的環境是使用 Docker 作為虛擬環境，並在 container 中使用 nextflow， container 的環境是 ubuntu 18.04。1. 建置一個 container為了...", "content": "Nextflow 系列 1 (環境建置)本篇內容，主要介紹的是 Nextflow 如何透過 docker 安裝在自己的電腦中，並且提供一個快速簡單的範例，讓大家可以快速的學會如何建置 Nextflow。我的環境是使用 Docker 作為虛擬環境，並在 container 中使用 nextflow， container 的環境是 ubuntu 18.04。1. 建置一個 container為了要確保環境是乾淨的，所以才必須得自己建立一個全新的環境。而我的 Dockerfile 內容在下方。FROM ubuntu:18.04RUN apt-get update &amp;&amp; apt-get install -y \\ vim \\ htop \\ default-jre \\ curlWORKDIR /opt/projectADD . .RUN curl -fsSL get.nextflow.io | bash在這當中最需要注意的是 default-jre 需要這個 package 才能夠執行 nextflow，因此要特別注意。接下來 build images 以及 container。docker build -t nextflow .docker run --name jacky_nextflow -it [IMAGE-ID] bash若是之後要改 container name。docker rename jacky_nextflow nextflow_new_name2. 測試 Nextflow這邊快速的測試看看我們的環境是否能夠使用docker run -it [container ID] bash進入 container 後，建立一個檔案。vim tutorial.nf輸入下面程式#!/usr/bin/env nextflow params.str = 'Hello world!'process splitLetters { output: file 'chunk_*' into letters mode flatten \"\"\" printf '${params.str}' | split -b 6 - chunk_ \"\"\"}process convertToUpper { input: file x from letters output: stdout result \"\"\" cat $x | tr '[a-z]' '[A-Z]' \"\"\"}result.subscribe { println it.trim()}# 要用下述方法才可以，印出結果。./nextflow run -process.echo true tutorial.nf成功了 ! ! !3. 結論這篇文章，只提供最基礎的安裝環境教學，並未提供任何其他 Nextflow 的相關知識，在後續的幾篇文章中，將會提到 Nextflow 有哪些基礎架構框架，以及 Nextflow 主要的目的、功用，另外也會提到它 popular 的原因。4. 參考文獻 Nextflow 官網 Nextflow github nextflow, 一種用於數據驅動計算管道的DSL 初识Nextflow (系列之一) NextFlow用法1 — 基本概念Post converted from Medium by ZMediumToMarkdown." }, { "title": "Jetson Nano", "url": "/posts/9d89cbf2fc18/", "categories": "Jackycsie", "tags": "jetson-nano, tensorrt, edge-computing, tools", "date": "2019-05-28 17:20:03 +0800", "snippet": "Jetson Nano本篇內容跟大家介紹的是 jetson nano 從灌系統到測試效能以及做些 TensorRT 的應用。首先先讓大家看一下 Nano 長怎麼樣。各位觀眾 ! ! !Sorry… 放成 ipod nano. .讓我們從重看一下真正的 Nvidia Jetson Nano 長怎樣。剛開始拿起來比想像中的還要重一點，可能是本身玩板子的時間太少所以不夠專業，這塊板子最酷的地方在於...", "content": "Jetson Nano本篇內容跟大家介紹的是 jetson nano 從灌系統到測試效能以及做些 TensorRT 的應用。首先先讓大家看一下 Nano 長怎麼樣。各位觀眾 ! ! !Sorry… 放成 ipod nano. .讓我們從重看一下真正的 Nvidia Jetson Nano 長怎樣。剛開始拿起來比想像中的還要重一點，可能是本身玩板子的時間太少所以不夠專業，這塊板子最酷的地方在於支援 4K 高畫質，看石原可以更清楚了 XDD。Jetson Nano 規格這裡值得提的地方在推薦 storage 建議使用 32GB ，因為灌個 OS 就要 12GB；另外電源也推薦 DC jack，如果像我一樣懶得去買的話，那 micro USB 推薦使用 5V (2.5A~5A) 的電源，不然如果跑複雜太高的 model，還沒跑完就會先斷電了. . .讓 nano 重生 透過官方提供的 SD format 將SD Card 裡面的資訊 format 。 官方 提供三種 OS 做 format(windows, mac, linux) .2. 下載 官方 提供的 image 檔 (Ubuntu 18.04 LTS)，下載檔案約 5.3 GB，解壓縮後大概 12 GB。3. 透過官方提供的燒錄器 Etcher ，將 image 檔案燒進去，最後記得燒完以後要退出記憶卡，不然很有可能會發生錯誤。4. 將 mirco SD 放入 nano 的插槽中，插入電源開機。5. 裝 HDMI 的時候不建議使用轉接頭，有可能會無法顯示。6. 經過一些跟 ubuntu 一樣的系統安裝就會像下圖一樣安裝完成。超帥 ! ! ! 因為接了 HDMI，讓他感覺非常的潮，SSH Jetson Nano使用 ssh 的好處有兩個 可以降低在 model 時候 4K 高畫質還不斷的吃著你的 GPU 資源。 電流不夠的風險又可以降低了一點。htop, nvidia-smi 結合在 Jetson 中有一個非常好用的工具就是 jtop，可以同時查看 CPU 資源與 GPU 資源，另外也可以看目前 CPU 與 GPU 的溫度與功耗，另外他還有貼心的服務，就是將你目前的 library show 出來。sudo apt-get install python-pip python-dev build-essential sudo pip install --upgrade pipsudo -H pip install jetson-statssudo jtop安裝必備套件因 image 本身就有安裝 CUDA 以及 cuDNN 和 openCV 所以讓我們省去了非常多的麻煩，首先我們先安裝 python3 相關套件。Virtualenvsudo apt-get install virtualenv -ymkdir envscd envsvirtualenv -p /usr/bin/python3 env_examplesource ~/envs/env_example/bin/activateOpenCV 與 Virtualenv 做連結因為在虛擬環境以外 opencv 已經裝好了，所以我們需要做的目標只要把 virtualenv 跟 opencv 結合就可以了。首先先知道 opencv 的路徑以及需要裝numpy。pip install numpysudo find / -name \"cv2*\"知道路徑以後將我們說需要的路徑 link 到我們的虛擬環境中。cd ~/envs/env_example/lib/python3.6/site-packages/ln -s /usr/lib/python3.6/dist-packages/cv2.cpython-36m-aarch64-linux-gnu.so成功拉~安裝 Tensorflow-GPU萬事俱備，只欠東風，如果像過去安裝 server 一樣裝 tensorflow-gpu 那在執行上會出現很多問題，因此官方建議使用他們專門給 Jetston 系列裝的 tensorflow-gpu，首先我們要先去官網查看一下目前的版本，我個人裝的是中間的版本不新也不舊，這樣就不用擔心 bug 太多。下面這個是同一行，拿去 command 按 enter 即可，可能會很久喔~要有一點耐心 . . .pip install --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v42 tensorflow-gpu==1.13.1+nv19.4增加 SWAP好險有 大神 提供需要建立 swap，不然每次超過 4G RAM 就當機真的是一件很麻煩的事情，下述的指令是大神提供，小弟也只是複製貼上。# 這裡設 4G 你也可以設8G 等等sudo fallocate -l 4G /swapfilesudo chmod 600 /swapfilesudo mkswap /swapfilesudo swapon /swapfilesudo swapon -show自從用了 swap 以後，就會比較不容易下 command 的時候就比較不容易 delay 了，另外有使用 model 訓練時，若是那偵測可以吃得下 free swap + free mem 那就不會 break up 出來，不然很容易無法執行。切換高低功率當訓練的時候，有可能因為你的電力無法負荷而馬上跳電，nvidia 在這裡算是很親民的提供兩種方式讓你轉換，5w, 10w，降成 5w 後許多 model 就跑得動了如: Resnet50，下面是轉換低功率與正常功率的指令。# 知道目前是哪一個 modesudo nvpmodel -q# 將目前的瓦數 降為5瓦sudo nvpmodel -m1改成 5w 後，會自動的將兩個 cpu 關閉，只使用 cpu 1, 2。測試效能因為到寫完這篇文章時，我的 micro usb 電源線都還沒到貨，所以 10w 的測試之後再補上，先做 5w 的。我們拿 tensorflow 官方測試 的 project 來進行驗證，我們共測了 4 種 model，resnet50, inception3, vgg16, alexnet，並且透過不同的 batch size進行測試， batch size 有 8, 16 兩種選擇。VGG16 為 0 的意思是 他跑到一半就 OOM 了，為了公平起見，這邊也不特別在增加 swap 來嘗試是否可以成功。Jetson-inferenceNvidia 的 大大 提供了一些 model 供大家嘗試，所以我也挑了幾個來玩一下，另外有 大神 寫了這篇應用的文章可以參考一下，這些 model 有趣的地方在它們都支援了 TensorRT 據說性能可以大幅提升，安裝步驟，中間過程可能會有一點久，聽個幾首 Bruno mars 的歌，應該就好了。cd ~sudo apt-get install git cmakegit clone https://github.com/dusty-nv/jetson-inferencecd jetson-inferencegit submodule update --initmkdir buildcd buildcmake ../makesudo make install全部的 model 都會放在 ~/jetson-inference/build/aarch64/bin，所以先 cd 過來，所有的 model 在第一次執行時都會比較慢，所以建議可以邊聽周杰倫的音樂 run 起來會比較快。cd ~/jetson-inference/build/aarch64/binImageNet 的檢測# cat.jpg 是我自己放進去的./imagenet-console cat.jpg cat_detect.jpg原圖辨識後的圖片結果出來囉，該算是對還是不對呢 XDDDDetectnet 檢測因為我們並沒有清楚的說明需要分類什麼因此，這次我們來清楚的定義吧。./detectnet-console subway.jpg subway_detect.jpg facenet這次我們想要將圖片中有人臉的 mark 起來。酷吧~其實他有提供許多可以檢測的方式，供大家使用。感謝謝謝本次的 Sponsors Vic 讓我有機會可以這麼快可以玩到這片轟動武林的 Nano 板，也感謝 Charles 的大力幫忙，最後感謝 ZhgChgLi ，是因為你才讓我繼續想要寫這篇的原因。心得這塊板子感覺是一個里程碑，可以去思考未來 AI 的趨勢以及 AI 真的慢慢走入你我的生活中了。參考文章 Deploying Deep Learning Jetson Nano — Use More Power! Jetson Nano Developer Kit (official) Jetson Nano Developer Kit NVIDIA Jetson Nano學習筆記 NVida Jetson Nano 初體驗（一）安裝與測試 分類模型辨識北極熊與即時影像人臉辨識Post converted from Medium by ZMediumToMarkdown." }, { "title": "我跳進去又跳出來了，打＿", "url": "/posts/26cb3038ca0e/", "categories": "Jackycsie", "tags": "numba, python, cprofile, tools", "date": "2019-05-24 08:52:54 +0800", "snippet": "我跳進去又跳出來了，打＿本篇文章介紹的是透過 cprofile 分析 python program ，哪些 function, module 值得優化。之前有寫一篇關於 numba 提升 python program 速度的文章( 魯蛇變蟒蛇 )，而這篇使用 time.time( ) 來作為評斷程式快慢的依據，但是這種方法不是這麼的專業，因此我們在本篇中透過 cprofile 來簡單的跟大家...", "content": "我跳進去又跳出來了，打＿本篇文章介紹的是透過 cprofile 分析 python program ，哪些 function, module 值得優化。之前有寫一篇關於 numba 提升 python program 速度的文章( 魯蛇變蟒蛇 )，而這篇使用 time.time( ) 來作為評斷程式快慢的依據，但是這種方法不是這麼的專業，因此我們在本篇中透過 cprofile 來簡單的跟大家聊聊，如何透過這個工具可以找出整個 project 中最耗時的部分，這樣可以讓大家透過演算法改進或者 numba 快速優化 module。其實網路找了一下，發現其實很多人寫過這種文章了 XDD，其實我主要也是給自己筆記用所以還 OK，survey 了一下發現大家最常使用測試程式效率的有 time.time( ), timeit.timeit( ), cprofile，而 cprofile 是這三種方法中，最清楚，另外 cprofile 在大型專案中用起來是比較方便的因為可以清楚地了解整個流程，以及用綜觀的方式，讓你知道專案的走向以及那些 function call 了幾次，花費的時間等等…，最重要的是可以視覺化。範例程式首先我們先建立主要的程式，再建立需要 call 外部 function 的 程式。from extend_function import cal_sum def foo(): result = 0 for i in range(100000): result += i for i in range(10000): cal_sum() return resultif __name__ == '__main__': foo()def cal_sum(): result = 0 for i in range(1000): result += i return result接著執行以下這段， -s 代表的是再 cumtime 進行 sort。python -m cProfile -s cumtime cprofile.py ncalls : 代表的是 cprofile 總共 call 了幾次 function，若有像分數的產生代表的是， 前者 2 表示總調用次數，後者 1 表示主要調用次數。 tottime : 代表的是本身 function 所花費的時間，不包含子函數花費的時間。 percall : 第一個 percall，等於 tottime / ncalls。 cumtime : 全部的時間，包含子函數。 percall : 第二個 percall，等於 cumtime / ncalls。 filename : lineno(function) : 首先是檔案名，括號的是 function name。畢卡索時間畢竟數字是難以真正了解我們整個 project 的流程，沒圖沒真相我們來看看上述的程式它們畫出圖的結果吧。在這個地方我們使用 gprof2dot 來畫圖。pip install gprof2dotsudo apt-get install graphviz首先將剛剛輸出的文字檔轉換成 * .pstatspython -m cProfile -o cprofile.pstats cprofile.py接著再將剛剛輸出的 cprofile.pstats 轉換成 .png 的圖。python -m gprof2dot -f pstats cprofile.pstats | dot -T png -o flow.png結果如下若是有些 code 比較花的時間比較寫的話 就會是下面的圖。當然因為上述是簡單的測試，若是一個小專案那會向是下面的圖。從這張圖可以清楚地得知，那些流程是值得優化的，越靠近紅色，橘色的代表就是整體時間花費非常久的 function。各 function 效能測試上述說的都是對整個專案或者是整體架構程式做時間評比，那我要如何有像time.time( ) 一樣可以分析小程式呢 ?在這邊共提出兩種做法 cProfile.run( ) cProfile.profile( )cProfile.run( )在這個部分是快速地對某一行 code 進行判斷，去研究這行 code 所花費的時間極其細節。from extend_function import cal_sum import cProfiledef foo(): result = 0 for i in range(100000): result += i for i in range(10000): cal_sum() return resultif __name__ == '__main__': #cProfile.runctx('foo()',globals(),locals()) cProfile.runctx('foo()')另外也有提供下述的 function可以使用，如果你的分析需要加入其他參數，可以使用這種方式。 cProfile.runctx(command, globals, locals, filename=None)，filename 是將輸出內容轉成二進位，cProfile 區塊測試上述的這種方法，主要是針對單行程式做計算，下面的方式則是可以想time.time( ) 一樣，針對某一個區塊進行計算。from extend_function import cal_sum import cProfiledef foo(): result = 0 for i in range(100000): result += i for i in range(10000): cal_sum() return resultif __name__ == '__main__': spent_time = cProfile.Profile() spent_time.enable() foo() spent_time.disable() spent_time.print_stats() enable( ) : 開始分析 disable( ) : 結束分析 print_stats( ) : 印出分析結果 create_stats( ) : 停止分析且內部記錄其分析結果 dump_stats(fimename) : 輸出分析結果於檔案透過這種簡單的例子可以看到，我們將許多不必要的資訊，都 remove 掉了，讓我們可以透過 cprofile 專住在分析程式的內容中。感謝本篇文章感謝中正的學弟【 宇彤】 ，跟我介紹了這個 tool，讓我又多學到了一點點，另外感謝主管 Vic, small 讓我有時間做簡單的 cprofile 研究。大概寫完簡單的介紹了….剩下的就是有多的需求再去找了參考內容 The Python Profilers [Python] cProfile 教學 Python实践58-性能调优之cProfile Python优化第一步: 性能分析实践Post converted from Medium by ZMediumToMarkdown." }, { "title": "魯蛇變蟒蛇記", "url": "/posts/41e9c047e8e5/", "categories": "Jackycsie", "tags": "numba, big-data, cpython, tools", "date": "2019-05-15 17:08:40 +0800", "snippet": "魯蛇變蟒蛇本篇文章介紹的是如何透過 “Numba” 讓 Python 在大量的數據運算中，可以提升它的效率。Python，時常在處理大量的數據資料時，會被人嫌處理的速度太慢，因而最近，許多人開始在聊或許可以使用 juila 來加快，但畢竟在公司或是研究所的專案中，不可能將所有的 project 轉成 juila，因此我們將會介紹如何透過Numba 提升 python 的執行效率，而提升後甚至...", "content": "魯蛇變蟒蛇本篇文章介紹的是如何透過 “Numba” 讓 Python 在大量的數據運算中，可以提升它的效率。Python，時常在處理大量的數據資料時，會被人嫌處理的速度太慢，因而最近，許多人開始在聊或許可以使用 juila 來加快，但畢竟在公司或是研究所的專案中，不可能將所有的 project 轉成 juila，因此我們將會介紹如何透過Numba 提升 python 的執行效率，而提升後甚至有機會比原本的速度快到將近百倍之多，但使用的方法還是會導致不同的結果產生，因此還是因人而異。在做更多的介紹前，讓客官看一下在 medium 上有一位 大神 已經把它簡單的測試結果圖做出來了，可以看出當我們的時間複雜度到達 N^3，jit 能夠幫我們加速到多麼驚人的程度。如果對加速還是有興趣的話那就繼續看下去吧~是否要入坑？如果有以下情形. . . . 若是原本專案大部分的程式碼都是以 python 完成，這個時候為了加速，換語言重刻 code，工程浩大. . 目前只想專精一種程式語言，並且讓你的 co-worker 不需要再學其他類型的語言的話。 你的程式碼有非常多的 for-loop 或者有大量的數學運算。歡迎你加入”Numba” 的行列。安裝pip install numba新手上路首先我們做簡單的從 1 加到 100000000 需要多少的時間。import timedef sum_cal(x,y): sum_num = 0 for i in range(x,y): sum_num += i return sum_numstart_time = time.time()print(sum_cal(1,100000000))print('Time used: {} sec'.format(time.time()-start_time))結果Time used: 10.194484949111938 sec那我們再試試看使用 Numba @jit的效果，來看看效果如何。可以從下圖看到非常簡單我們只要在 def 上加上一行的 @jit 即可。import timefrom numba import jit@jitdef sum_cal(x,y): sum_num = 0 for i in range(x,y): sum_num += i return sum_numstart_time = time.time()print(sum_cal(1,100000000))print('Time used: {} sec'.format(time.time()-start_time))結果Time used: 0.0987863540649414 sec各位觀眾朋友，從這個結果可以看出 numba 的能力是多麼的驚人，我們只是簡單的加入一行 @jit，就可以讓本段程式快了 100 倍. .是不是覺得超酷啊~不能不注意從上面的簡單小範例就可以知道 Numba 的加速能力是非常驚人的，但是numba 在使用上會有一些小問題需要解決，因此我們在下面跟大家介紹一下，在使用上有那些注意事項。 Numba 不支援許多的第三方 package, ex: Pandas，因 為numba 主軸在於加速數據的運算，而非做資料的清理或者篩選，因此在若是程式當中有使用到第三方套件的，那建議拉出來做，把最核心的數據運算，交給 numba 加速。 本身在執行 numba 時，時常發現 numba 在做 numpy運算時，並不會報錯，主要報錯的時間點在於，格式的不同或者不支援，才會報錯。 因第二點的關係，確保numba產生的結果，是你想要的結果。 有時候在 function 中傳入 list 會出現錯誤，因此個人推薦將list轉成np.array 會是一個比較好的選項，也可以減少 numba 做多餘的操作。 numba 熱機模式，numba 跑第一次資料時，會來的較慢，因此建議若是要測試跑速，建議多跑幾筆資料，數據會來的更加客觀。Numba 1 2 3在本段將介紹上手3小步，教大家如何快速的上手 numba，並且快速地達到加速的目的。 分析自己的 code，看看區段花費的時間最長，以及這些區段是否適合 @jit 做加速。 將不能使用@jit 的 code 分開寫，讓純粹做運算的發揮到它能加速的極致。 做完上述辛苦的兩個步驟後，就享受執行後的速度感吧。深入Numba如果想要使用更深入的 numba 技巧，那就歡迎大家繼續看下面的介紹吧。Numba 其實不只提供@jit裝飾器，另外它還提供了@vectorize，@njit，@jitclass等等幾種裝飾器，而這 4 種裝飾器都有提供下列的四種參數供大家使用。接下來讓我們看一下主要提供的四種參數。 nopython：加速的效果非常的 powerful 但是限制非常的多，@jit 在第一次執行時就會嘗試執行 nopython，如果執行才會轉換成 object 執行，也因為太好用了，之後甚至開發了 @njit，它的功用就像 @jit(nopython=true) 一樣喔。 nogil：寫過 python threading 都了解，gil 時常會造成執行上的困擾，因為它並不是真的同時同步的執行多件事情，因此 @jit(nogil=True) 的目的就是將這些被 lock 住的 thread，讓他們自由變成可以同步執行。 cache：為了避免每次調用 Python 程序時的編譯時間，可以指示 numba 將函數編譯的結果寫入基於文件的緩存中。這是通過傳遞@jit(cache=True) 這段是直接從官網翻譯來的，因為本身在實測這上面並沒有特殊的功效，因此無法客觀的跟大家解釋。 parallel：就如字面上的意思一樣 XDD，它就是平行化的功用，但在這裡不推薦每次都使用這個參數，而必須依照實測結果做評估，因為也試過平行化的大大們都知道，有時候做平行化反而只會提升 I/O 的交換時間，而這個時間反而會降低了運算的速度；@jit(parallel=True)。Numba 在一般運行總共分為兩種模式一種是nopython mode，另一種是 object mode。 nopyhon mode:在執行第二次會直接忽略 python C API，這種方式的好處在於加速的非常快，會比 object mode 快大概 20~30 倍之多，但壞處就是限制非常多。 object mode: 在執行時雖然比 nopython 來的慢，但是錯誤率較低，不需要花大量的時間 debug，因此若是專案有時間的限制，可先考慮使用這種版本。Numba(nogil, nopython) 搭配 multi threadsimport mathfrom concurrent.futures import ThreadPoolExecutorimport numba as nbimport numpy as npimport time@nb.jit(nopython=True, nogil=False)def kernel1(result, x, y): sum_num = 0 for i in range(x,y): sum_num += i #print(sum_num) result = sum_num return result@nb.jit(nopython=True, nogil=True)def kernel2(result, x, y): sum_num = 0 for i in range(x,y): sum_num += i #print(sum_num) result = sum_num return resultdef make_single_task(kernel): def func(length, *args): #result = np.empty(length, dtype=np.float32) result = kernel(length, *args) #print(result) return result return funcdef make_multi_task(kernel, n_thread): def func(length, *args): all_len = args[1] single = round(all_len/4) count_num = 0 with ThreadPoolExecutor(max_workers=n_thread) as executor: executor.submit(kernel, length, 1, single) executor.submit(kernel, length, single, single*2) executor.submit(kernel, length, single*2, single*3) executor.submit(kernel, length, single*3, all_len) executor.shutdown(wait=True) return funcnb_func1 = make_single_task(kernel1)nb_func2 = make_multi_task(kernel1, 4)nb_func3 = make_single_task(kernel2)nb_func4 = make_multi_task(kernel2, 4)result = np.array(0)no_gil = 0 multi_no_gil = 0yes_gil = 0multi_yes_gil = 0step = 50 for i in range(1, step): start_time = time.time() nb_func1(result, 1, 100000000) #print('no gil: {} sec'.format(time.time()-start_time)) no_gil += (time.time() - start_time) start_time = time.time() nb_func2(result, 1, 100000000) #print('muti no gil: {} sec'.format(time.time()-start_time)) multi_no_gil += (time.time() - start_time) time.sleep(0.5) start_time = time.time() nb_func3(result, 1, 100000000) #print('Have gil: {} sec'.format(time.time()-start_time)) yes_gil += (time.time() - start_time) start_time = time.time() nb_func4(result, 1, 100000000) #print('mutli gil: {} sec'.format(time.time()-start_time)) multi_yes_gil += (time.time() - start_time)print(\"no_gil : \\n\",no_gil/step)print(\"multi_no_gil : \\n\",multi_no_gil/step)print(\"yes_gil : \\n\",yes_gil/step)print(\"multi_yes_gil :\\n\",multi_yes_gil/step)no_gil : 0.004972248077392578multi_no_gil : 0.001708517074584961yes_gil : 0.0011373043060302734multi_yes_gil : 0.0016107177734375從上面的結果可以看的出來，多執行緒在細節切分來加速效果還是相當不錯的，另外搭配 nogil 的使用可以有些許的進步，沒有進步幅度很大的原因有兩個第一運算的複雜度不高，第二 step 在加更多會看得出更大的效果，這就是為什麼 numba 適合用在大量複雜的運算的關係。使用Numba (vectorize, parallel, nopython)import numba as nbimport numpy as npimport time@nb.vectorize(\"float32(float32, float32, float32)\", target=\"parallel\", nopython=False)def clip_with_parallel(y, a, b): if y &lt; a: return a if y &gt; b: return b return y@nb.vectorize(\"float32(float32, float32, float32)\", nopython=False)def clip(y, a, b): if y &lt; a: return a if y &gt; b: return b return yy = np.random.random(10**5).astype(np.float32)print(y)assert np.allclose(np.clip(y, 0.1, 0.9), clip(y, 0.1, 0.9), clip_with_parallel(y, 0.1, 0.9))start_time = time.time()clip_with_parallel(y, 0.1, 0.9)print(format(time.time() - start_time))start_time = time.time()clip(y, 0.1, 0.9)print(time.time() - start_time)start_time = time.time()np.clip(y, 0.1, 0.9)print(time.time() - start_time)有設 parallel: 0.00099921226501464843750沒有設 parallel: 0.0009975433349609375沒有設 parallel 但是使用 np Ufunc: 0.0009732246398925781這次實做的是 parallel 的參數，因為我們當初假設的計數 1 到 1 億次對電腦來說太過簡單，所以我實測的時候幾乎為 0，為了有更好的呈現給大家的方式，大陸有一位 大大 將它寫出來了，而實測效果雖然跑的也很快但至少，有數字可以呈現給大家看。因自身專案有需求所以有使用過 parallel 的參數，在這邊只能跟大家心得分享，parallel 真的必須要小心用因為 I/O 的關係，有可能會因為 I/O 轉換所花費的時間，降低了原有的效率大家必須注意一下。Decorator介紹完上面的幾種應用，你們一定會好奇剛剛使用的@vectorize是甚麼？這個裝飾器是用來支援 numpy 的 Ufunc 功能下面就會介紹這種功能的實作方式，另外其他常使用的裝飾器還有@jit, @njit, @jitclass這幾種是我常用的裝飾器。Numba @vectorizefrom numba import jitimport numpy as npimport timeimport numba as nb@nb.vectorize(nopython=True)def add_with_vec(a, b): for i in range(1, 10000): a * b + b for j in range(1, 10000): a * b + b for k in range(1, 10000): a * b + b for l in range(1, 10000): a * b + b return a*a + by = np.random.random(10**5).astype(np.float32)one = 0two = 0add_with_vec(y, 100)add_with_vec(y, 100.)for i in range(1, 100): start_time = time.time() add_with_vec(y, 100) one += time.time() - start_time start_time = time.time() add_with_vec(y, 100.) two += time.time() - start_timeprint(one,\"\\n\", two)沒有浮點數input: 0.014963150024414062有浮點數input: 0.004987239837646484從結果可以得知，有支援 vectorize 效果又會來的更好一點，另外 vectorize 有支援上面所說的平行運算的功能。 當只有使用 vectorize 沒有加參數時，那它只會有一個 cpu 再跑。 當vectorize加入 parallel 後，可以有多個核心再跑。 另外 vectorize 也加入了 CUDA，讓你跑GPU也可以跑得順順順(聽說可以CUDA 加上 CPU) 17 跑，但家裡窮錢都拿去玩 MLB The show 19 了沒有錢錢買顯卡，所以要靠大家自己試試看了。另外當你確定你的 input 是甚麼輸入格式時，在 vectorize後面打入你要將會輸入的格式，那效率又會來的更高一些。@nb.vectorize(\"float32(float32, float32)\", nopython=True)def add_with_vec(a, b): .... #跟上面程式碼一樣 return a * a + bfloat32(float32, float32)是你想要輸入的格式另外不確定自己輸入的格式還可以有另外一種寫法。@nb.vectorize([ \"float32(float32, int32)\", \"float32(float32, float32)\"], nopython=True)def add_with_vec(a, b): .... #跟上面程式碼一樣 return a * a + b這種寫法要注意，有時候若是沒有按照32, 64的大小從小寫到大numba可能會出錯的。沒有浮點數input: 0.010974407196044922有浮點數input: 0.003989219665527344Numba @jit這個地方就不多贅述了，就是最基礎的jit，一打字上去即可使用非常方便。Numba @njit在前文中已經有提過，若是當你使用 @jit又不想打nopython=true，那@njit是你旅行居家必備良藥。Numba @jitclass這算是一個比較新的東西，我在網路survey也沒有許多資料，但它的主要目的在於當你不知道@vectorize你想要input的類型是甚麼的時候，那就先寫個class起來吧~因為用的機會非常的低，大部分在使用numba都已經會將資料拆得很細了，因此需求不高，有興趣的可以看一下下面的網址。 用numba为python写高性能C扩展 python — 如何嵌套numba jitclassNumba 支援的 Ufunc如果想要使用 numpy 卻不知道 numba 提供哪一些數學式的話，那這邊提供您 網站做查詢 。透過 np.add( ) 或者是其他你想用的 math 都可以。感謝本篇文章感謝部門四位大大，Vic, Small, Charles, Weien，一起討論完成。參考文章 Dask + Numba for Efficient In-Memory Model Scoring Numba — Making Numpy 50x Faster Introduction to Numba Compilation Modes 加速python运行-numba Python · numba 的基本应用 一行代码让你的python运行速度提高100倍 关于Numba你可能不了解的七个方面Post converted from Medium by ZMediumToMarkdown." }, { "title": "Do You Have Free Style? (ECG)", "url": "/posts/48643a2b1bbf/", "categories": "Jackycsie", "tags": "ecg, paper", "date": "2019-04-03 21:24:16 +0800", "snippet": "Do You Have Free Style? (ECG)因為平常看論文時，會有許多的ECG症狀，而因為短時間不容易記起來，那就做個筆記吧，在本篇文章中會將論文常用到的幾個症狀寫出，在這當中也有Burt大的筆記，非常謝謝你。WARNING另外以下有錯的話請馬上通知，因為知識傳導的錯誤會影響人的認知，進而對病人的性命有非常嚴重的後果，非常感謝您Sinus Rhythm(竇性節律)竇性節律指的是...", "content": "Do You Have Free Style? (ECG)因為平常看論文時，會有許多的ECG症狀，而因為短時間不容易記起來，那就做個筆記吧，在本篇文章中會將論文常用到的幾個症狀寫出，在這當中也有Burt大的筆記，非常謝謝你。WARNING另外以下有錯的話請馬上通知，因為知識傳導的錯誤會影響人的認知，進而對病人的性命有非常嚴重的後果，非常感謝您Sinus Rhythm(竇性節律)竇性節律指的是心臟去極化的起始點來自竇房結，這裡會介紹6種節律。 Normal Sinus Rhythm (NSR) Sinus Arrhythmia：完整P-QRS-T波，P-P節律不規則(節奏變換一陣子)，但心跳仍然在60~100次之間，一般不需治療。 Sinus Bardycardia(竇性心搏過緩)：&lt;60次/分去極化，其餘波型正常。 Sinus Tachycardia(竇性心搏過速)：&gt;100次/分去極化，其餘波型正常， Sinus Block(竇性阻斷)：竇房結有時不發生去極化衝動，或太弱，原因為竇房結發生缺血現象，一般不需治療，除非過於頻繁或阻斷時間過長(大於2秒以上)，停滯為RR的兩倍(或倍數)。 Sinus Arrest(竇性心律暫停)：竇房結受抑，延緩發出去極化之衝動，停滯與RR倍數不同。Atrail Rhythm(心房性節律)心房性節律是指心肌去極化之起使來自於心房，但不一定來自於竇房結，這裡介紹5種節律。 Premature Atrial Contraction(PAC 心房早發性收縮)：指一個或數個心肌去極化之起始在心房某處，PAC之出現節律不規則外(一個P-QRS-T波)，P波有時比較平有時缺角，診斷方式：相鄰的心率呈現不規則，且有P波，P波提早出現。 Atrial Tachycardia (AT 心房性心搏過速)：心肌去極化衝動來自心房， &gt;100次/分去極化，每次衝動均傳至房室結，繼而引起心室去極化。P波形狀異於竇性結律，有時埋於T波中。 Atrial Flutter ; AF(心房撲動): 心房以250~300次/分產生去極化(心室因為有房室結60~150次/分去極化)；在去極化的過程時，殼能房室結屬於在極化過程之不反應期，因此不見得新房去極化之衝動均會傳至心室，引起心室去極化，因此會看到很多P波才會有一個QRS波，P波鋸齒狀。 Atrial Fibrillation ; Af(心房纖維顫動): 心房&gt;350次/分產生去極化(心室因為有房室結60~100次/分去極化)。P波難以辨認。 Wandering Pacemaker(遊走性律點): 引發心肌去極化衝動之結律點不提改變(遊走)，每個P波樣子均不相同。Junctional Rhythm(交界性節律) or Nodal Rhythm(結性節律)指自房室連結處發出極化之電衝動，引起整個心臟去極化之節律。P波不易辨識，可能出現位置有三： 高交界性: P波與QRS波距離較短 中交界性: P波常埋在QRS波內 低交界性: QRS波後產生倒立P波 Junctional Escape Beat ; JEB(交界型逃脫性收縮(一次) ) : 40~60次/分，若交界性節律之QRS波與前一個QRS撥之R-R間隔比原來節律的R-R間隔長時，那就稱為JEB。 Junctional Escape Rhythm ; JER(交界型逃脫性節律(一系列) ) : 40~60次/分 Premature Junctional Rhythm(早發性交界性節律) : 嚴重時需才治療，不規則 Accelerated Junctional Rhythm(加速型交界性節律) : 60~100次/分，規則，不需治療 Paraxysmal Junctional Tachycardia; PJT( [陣發性]交界性心搏過速) : 100~240次/分，規則※ 若交界性節律QRS波與前一個QRS波的RR間隔較短，為PJR；若較長，則為JEB※ Premature Junctional Rhythm(早發性交界性節律)與PAC有相同特性，但無P波測得※ Sinus Tachycardia + PAT + PJT = Paraxysmal Supraventricular Tachycardia; PSVT(陣發性心室上心搏過速)※ Junctional Rhythm通常定義為一類Ventricular Rhythm(心室性節律)心室性節律指的是引取心室去極化的電衝動源自於心室，而非竇房結、房室結或心房上的任何一點， 寬大QRS波，T波與QRS波方向相反，P波難判斷。 Idioventricular Rhythm ; IVR(心室自身節律):寬且巨大QRS波，節律通常&lt;40 Accelerated Idoioventricular Rhythm ; AIVR(加速型心室自身節律): 寬且巨大QRS波，40 &lt; 節律 &lt; 100 Premature Ventricular Contraction ; PVC(早發性心室收縮): n個sinus Rhythm後，一個PVC。 Ventricular Tachycardia VT(心室性心搏過速)，在心電圖上會發現許多寬大且巨大之QRS波羅列，波數超過每分鐘100次。 Ventricular Flutter VF(心室撲動)，心電圖上之QRS波一個個以不好辨認，但扔可看出一個個的波連接交疊，150&gt;心搏&gt;300。 Ventricular Fibrillation Vf(心室纖維顫動)，心電圖上之QRS波不意辨認，已呈現雜亂高高低低之小波連接不斷。Post converted from Medium by ZMediumToMarkdown." }, { "title": "論文筆記 — 吳恩達 2019 ECG", "url": "/posts/a5b03aed9cb8/", "categories": "Jackycsie", "tags": "deep-learning, ecg, cnn, resnet, paper", "date": "2019-03-20 15:56:07 +0800", "snippet": "論文筆記 — 吳恩達 2019 ECGCardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network在本篇論文前吳恩達在2017年已經發過與ECG相關的paper也是使用CNN架構加以改變，下面連結是他的論文網址，以及...", "content": "論文筆記 — 吳恩達 2019 ECGCardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network在本篇論文前吳恩達在2017年已經發過與ECG相關的paper也是使用CNN架構加以改變，下面連結是他的論文網址，以及我念這篇2017 paper後的想法。心得: 論文筆記 — AI當醫生了!?原文: 2017 吳恩達 paper新文： 2019年 新的paper我們本篇的目標第一是將本篇文章的特色拿出來跟大家聊聊，第二是比較2019與2017年兩篇paper之間的差別？1. Dataset在這篇論文中，他共準備了53,877位病人的資料，共91,560份 200HZ 的ECG records，並且切割為training set與testing set，另外這些病人的年紀皆大於18歲。Training set: 病人共有53,549位病人，共有91,232份ECG records，年齡層介於 53~85歲之間，而女性占的比率約43%。Testing set: 病人共有328位病人，共有328份ECG records，年齡層介於53 ~87歲之間，女性比佔了38%。從Dataset的角度可以看出，paper的資料性非常的多樣化，可以在訓練時增加相對應的複雜度，讓model 可以有更好的效果產生，也可以從dataset看出女性在dataset的比例相對較低，可能的原因有很多種，如女生在醫界中，比男生更難收集到資料，那這樣的發展是否也會影響到model在testing 女生時，沒有男生來的精準呢!?2. Model在model中，他們使用的是透過ResNET的想法下去改造，ResNET的特色在於可以讓model學習更加複雜的pattern，ResNET是一個瘦長的神經網路，以數學的想法來說，就是得知即將餵入的資訊雖然有pattern，但是可能pattern的相似度很高，需要利用更加複雜的function才能包住它，並且加以判別。在這篇論文中，共有34層layer，33層的卷積層，1層的Softmax層(在16個residual blocks中，每一個residual blocks有兩個卷積層)，以及max pooling來增強它的學習效能(透過subsampling)，這就是ResNET的特性，不管是使用BatchNorm或者Relu，都是為了能在攏長的訓練過程中，加強他的學習效率。Model 所使用的filter是16 或者是 32 * 2的k次方個數量，k的數量從0開始並且每4層加1，以及在每一個residual blocks中進行下採樣的方式；在Dropout中它為了降低over-fitting的風險，把參數設為0.2；在optimizer所使用的是大家用過都說讚的Adam，β 1 = 0.9 and β 2 = 0.999：mini-batch也設為非常有sense的128；learning rate從1 * 10的負3次方，當2次epoch沒有改善時，就在將learning rate乘於0.1，有此可知對自己的model 非常的有信心。從這個model可以非常明顯的看出，他們是跑了非常多個model，以及透過各種認為合理的方法下去training完成(小窮舉法)，他們也有提到當residual blocks超過8個後，效果會非常的明顯，可以得知當複雜度非常高的data，build非常deep的model，是趨勢了。而在本篇中他們也嘗試使用過LSTM，以及bidirectional recurrence，效果它不是來的非常的好，另外一個重點是訓練的時間非常的久，因此放棄了其他兩種類型的發展。3. Evaluation在評估model時，本篇共拿了12種類型的label做比較(10 arrhythmias與sinus rhythm和noise)，而數據評比的方式有兩種，一種是 Sequence-level，另一種是Set-level。 Sequence-level：每30秒就會有23個輸出(30/1.3s)，並且去比對這23個輸出的答案是否正確。 Set-level： 將輸出結果與ture label比對，而在set level 中不會有時間錯位的懲罰。透過AUC與ROC的評判方式則是使用Sensitivity and specificity以及PPV(precision)這三種方式，敏感度跟特異性這種評斷方式在醫療期刊中非常的常拿來使用。下面有簡單的介紹，方便大家理解。 Sensitivity(敏感度, Recall)： 有病的人當中真正被驗出有病的百分比，例如100個有病的人90人檢測結果呈陽性即90% sensitivity，10人檢測結果呈陰性稱為10%偽陰性 10% false negative； 識別出來對的，佔全部對的幾%(未識別出也算進來)。TP/(TP+FN) Specificity(特異度)： 沒病的人當中真正被驗出沒病的百分比，例如100個沒病的人90人檢測結果呈陰性即90% specificity，10人檢測結果呈陽性稱為10%偽陽性 10% false positive。TN/TN+FP Precision：識別出來，有幾張是對的。 TP/(TP+FP)在test data中，他們共請了9位心臟學的專家，來進行測試，並且拿訓練好的model與這群專家比賽看誰的評斷比較正確，而標label的人則是找另外6位專家共同標註label，當成gold standard。下圖是model比較的表格，我們可以看出醫生在Junctional rhythm以及Ventricular tachycardia上，贏了我們的model，雖然都是贏的不多，但可以了解model，還有可以進步的空間，而整體model的sequence-level model 的平均正確率為0.807，Set-level 0.837，醫生的sequence-level為 0.753，set-level為0.780，可以得知model的整體實力真的有機會可以降低醫生未來的誤判率。另外我們來看一下 confusion matrix 可以得知，我們給model 的目標就是幾近完美，近乎苛求，在sinus rhythm可能是因為波型太過相似，讓這塊連醫生都無法有效的判斷出來。另外為了驗證model 的可行性，本篇論文還實作了MIT-BIH的data下去training以及testing，training set(n=8,528)以及拿10%的自己的data下去train目的在於希望可以提早收斂，testing set(n=3,658)，並且檢驗3種label後，下面為test後的結果圖，他們的驗證方式是使用Sequence-level作驗證的。看完上面的圖，個人的心得認為或許它有其他可以微調的方式，因為它在paper中也宣稱，它沒有特別調過任何的參數，目標是跟原始的training方式一樣，這方面是值得存疑的，需要去驗證看看而這個實驗最後它的F1 分數為0.83，也相當的不錯，或許是label少，容易認出？4. 結論本篇論文雖然將許多參數的細節寫出，但許多內容並未公布，如他們如何解決在set-level 30s中有多比record的算法，以及為什麼取這幾種label做為target；但從大方向來說，可以得知擁有好的資料集是非常重要的，但好處在於它也有提到他們利用MIT-BIH的training set加上10%自己的data可以也做rup w到不錯的結果。而它有提供測試集剛好是占整體資料的10%，另外我們自己再去抓MIT-BIH的資料即可， 測試集資料 。論文也提供了 github 供大家參考。5. 比較既然我們在前面有提到，在本篇論文前已經有出過2017年 ECG paper，那我們來比較他們的幾項差異。 Dataset：在2017年擁有的病人數接近為30,000，到了2019年共有了53,877人，並且非常詳細的紀錄了男女比例，以及他們選擇資料的標準。 model：總體的層數34 層layer，大致上優化的方法也都沒有改變，唯一有變的就是在residual layer中，都少了一個dropout，在2017中個residual layer都有兩個dropout，從此方法的改變可以看出，或許是因為資料變多了，over-fitting沒有那麼嚴重，因此可以少一個dropout，讓training的收斂速度來的更快。 label：在2019論文當中，將atrial fibrillation與atrial flutter合併為一個，另外也將2 second-degree AVB與third-degree AVB合併為AVB class，這樣就從2017的14個label，變成2019的12個label。接下來做的是2017 與 2019 testing set數據比較，左圖是2017年的，右圖是2019年的圖。從比較圖可以得知，大致上透過上面data、model、label的改進，的確在2019年的paper中，可以看出識別率的確變高了，但是在EAR、IVR、junctional rhythm，可以看出並沒有讓識別率更高，甚至有一些還降低了；另外透過合併後的的兩個label，大幅提高了辨識率；另外有趣的是因為這樣的合併讓心臟病專家，除了AVB以外，並沒有有提升非常多的辨識度。從上面的數據結果一步步的比較下來，2019 辨識度Set-Level 0.837，2017 辨識度Set-Level 0.809，或許真正提升辨識度的原因不在於data、model、label的改進？還是純粹只是將label合併之後所提升的整體辨識度。因此我們換個思考方式當2017年的4個label套用成2019的兩個label的數據，我們的model是否會提升，而實計算出來的結果是seq-level是0.8017，set-level是0.7765，因此可以得知model真的是有進步的。寫完了. . .看完大神的兩篇paper以後，可以得知雖然前面的路不容易，但是有機會。We are the champions. . .My friends~Post converted from Medium by ZMediumToMarkdown." }, { "title": "論文筆記 — AI當醫生了!?", "url": "/posts/d5cf73ef27c8/", "categories": "Jackycsie", "tags": "deep-learning, ecg, resnet, cnn", "date": "2019-03-18 20:56:38 +0800", "snippet": "論文筆記 — AI當醫生了!?Cardiologist-Level Arrhythmia Detection with Convolutional Neural Networks這篇論文主要的內容是吳恩達與史丹佛大學團隊在2017年針對ECG透過深度學習進行檢測。而寫本篇文章的目的在於，希望能夠將這篇論文的特色，以及為什麼會這麼powerful的地方寫出來，透過這篇文章，讓大家可以快速了解吳...", "content": "論文筆記 — AI當醫生了!?Cardiologist-Level Arrhythmia Detection with Convolutional Neural Networks這篇論文主要的內容是吳恩達與史丹佛大學團隊在2017年針對ECG透過深度學習進行檢測。而寫本篇文章的目的在於，希望能夠將這篇論文的特色，以及為什麼會這麼powerful的地方寫出來，透過這篇文章，讓大家可以快速了解吳恩達大神的神操作，以及若是之後忘記他的key idea 可以回來回憶一下。另外吳恩達在2019年也寫了一篇paper，我也做了paper的心得在下面連結。 論文筆記 — 吳恩達 2019 ECG那我們開始吧. .1. Dataset萬事起頭，皆從資料集，好的資料集可以讓你省去非常多的功夫，在這方面我覺得這是這篇paper會成功的原因之一，因為他們的資料量共擁有了30,000個不同的病人的心電圖剪輯，而普通的online open data，如非常有名的MIT-BIH，也只擁有了47位病人的心電圖，光是在資料的豐富性，這篇paper就展現了一個高度在。而他們的sensor用的是Zio Patch monitor所收集到的資訊，從這個角度看可以得知，他們的TA不是在於超大儀器的感測準確率，而是在各病房或者未來家中的病患照顧，去做思考。2. Model這是這篇論文第二個厲害的地方，他們的想法是基於ResNET的概念去實作，而為什麼要使用ResNET而沒透過其他VGG16相關的model呢？因為ResNET有個很重要的特性在於，他們將更多資訊帶入更深層的layer中，而這個model不只透過自身的forward傳遞，也可以將前幾層layer的特性一並餵入進後面的layer，讓更deep 的layer可以學習到資訊而不會無法傳遞下去，因此這是這篇論文的一大特色。下圖是這model圖，總共建立了34層的layer，33層卷積層，一層FCN，以及softmax，使用max pooling的目的在於降低運算量，將image subsampling，做特徵萃取。3. Training在這階段共使用了29,163個病人，共64,121個心電圖剪輯作為訓練，有趣的地方在於他們training的時候，以病人作為切割點，1個病人的心電圖剪輯不會同時出現在訓練集以及測試集，這是資料量大的好處，也是他們論文非常細心的地方，因為當我們使用MIT-BIH資料沒那麼大，那就可能只能以7:2:1的切割法作為切割，最後可能會導致，訓練結果不佳。Input進去的data為每個30秒 200 HZ的singal，輸出為14個label (12 heart arrhythmias, sinus rhythm and noise)4. Testing在測試資料集，他們共使用了328個病人，以及336個心電圖剪輯作為測試，而測試集的標籤共有6個心臟病的專家進行標籤；然後再請另外3位心臟病的專家來做benchmark，來跟model做PK，看看是屠龍刀好還是倚天劍強。在這個地方，這篇paper再次用不同的思維打破了我們的想法，與其用更多不同學習的dataset或者方法比較，還不如直接跟現實生活上的醫生比比看，來的更加實際。5. Result在這個地方論文共用了兩種方式進行比較。 Sequence Level Accuracy：這裡的意思是餵入30s後，每1.28秒slice或是256 samples就會output一次資訊，來比對是否與true label相符。 Set Level Accuracy：input 30s後，將輸出結果與ture label比對，而在set level 中不會有時間錯位的懲罰。從這兩個評比標準，我們可以思考，為什麼他要設這兩個比較方式，大概會有以下幾種原因。1. 隨身攜帶的sensor目的就是為了偵測那些不是時常有心律問題的人身上，這個目的在於希望發生事情時，能立即診斷出來。2. 當然sequence 一定比較難執行，因此我們才會需要再做一個set當作我們的目標，不但可以看宏觀，也可以找專注，非常好的想法。我們來看看下圖很明顯的可以看出，在14種label比較中，model大致上都贏了醫生，可以得知或許我們的model真的慢慢的輔助醫生做更多的判決，讓醫生專注在他們更想做的事情上了。看另外一張 confusion matrix可以看出在SINUS中，model很容易跟其他類型的label做混淆，或者是SVT AFL兩者之間也非常的相似，但整體上model 還是表現不錯的水準。最後看下圖，可以很明顯得知，model的實力已經真的比心臟科的專家來的好許多了，也可以看出Set明顯比sequence 來的更佳的好build，但是從醫生的角度來看sequence竟然也會輸Set這倒是比較意外。6. 結論雖然說2017這篇文章距離現在有一段時間了但從他們的做法，已經看到了醫學界的新高度，原因有以下幾個。 我們可以用deep learning去學習ECG的pattern了，雖然還不夠好，但是我們可以降低資源在rule-based當中。 這篇文章沒有刻意的透過任何取feature的方式，就做到了ECG的判讀，這說明在科技領域中，我們可以大大的降低對ECG完全的認識，因為研究ECG這麼多label的狀況下，還需要有跟心臟病專家一樣的水準，那花的時間是非常久的。 我們可以透過GAN，data generator等方式將資料複雜性不足的補齊，data generator的方式是相對容易多的，因為透過前面的方法得知，資料量是這篇論文的關鍵，所以在醫療ECG中誰擁有更多的data，誰就有機會有更好的model。7. 思考 從這篇文章中可以得知，醫生標的label，有可能會連帶影響我們model training的結果，那我們該如何解決？2. 當類型很像的ECG，我們該如何透過深度學習的方式解決？ (或許不用softmax)3. 有些label的資料量就是過小，我們只能使用oversampling嗎？4. 在論文中使用的皆以200 HZ，改成其他的HZ，是否會影響deep learning的判斷？5. 當一個30s的剪輯，裡面有多種label時我們該如何解決。論文參考：https://www.nature.com/articles/s41591-018-0268-3先寫到這有新的想法在打上…Post converted from Medium by ZMediumToMarkdown." }, { "title": "求職 5 4 3", "url": "/posts/649fd6c5e93f/", "categories": "Jackycsie", "tags": "life", "date": "2019-02-17 01:07:37 +0800", "snippet": "求職 5 4 3本篇文章主要是提供給資料科學家參考。在文章中，我會分三大部分跟大家介紹： 面試資料科學家的職缺中，我準備了哪些資料。 面試的公司他們的考試內容以及他們的優劣。 我選擇公司的基準跟想法。在這一個半月的面試過程中，我共面試了以下這幾間公司。聯電，鴻海，廣達，和碩，凌群，威盛，技嘉，大聯大，QNAP，世界先進，華碩。前言 背景中正大學資工所排名：系排14%，2次第1，1次第...", "content": "求職 5 4 3本篇文章主要是提供給資料科學家參考。在文章中，我會分三大部分跟大家介紹： 面試資料科學家的職缺中，我準備了哪些資料。 面試的公司他們的考試內容以及他們的優劣。 我選擇公司的基準跟想法。在這一個半月的面試過程中，我共面試了以下這幾間公司。聯電，鴻海，廣達，和碩，凌群，威盛，技嘉，大聯大，QNAP，世界先進，華碩。前言 背景中正大學資工所排名：系排14%，2次第1，1次第5名，還有1次50名。英文程度：中下，TOEIC落在500~600之間(看考運)。研究所數據分析專案：4個。 投遞公司類型以中大型公司產業為主(IT部門至少100人以上)，並且只有面試資料科學家相關職缺。 數據相關課程李弘毅老師線上課程吳恩達 coursera莫凡 線上教學Datacamp python 課程 履歷投遞主要以104作為我投遞公司的媒介，我共投出198次應徵紀錄，通知我面試的次數約51次，後期主要都是重複投遞履歷的情況，每當medium或者github有改版，就會再重新投一次，想要的職缺。但其實很推薦台灣人工智能學校做為投公司的地方，因為在這地方，碰到技術能力較差的人也來試試水溫的機率較低，並且主要面對的是主管，可以跳過ＨＲ直接跟主管做媒合。1041111台灣人工智能學校Linkedin一、準備資料 製作履歷在製作履歷我主要使用的是 cakeresume ，blink則是參考用，選擇cakeresume的原因在於，簡單明亮，快速套版，修改方便；而在撰寫履歷時我的想法有許多都是透過下列網址的啟發，讓我履歷品質來的更高，另外對自己履歷想要有更多明燈指點的我推薦可以花一點小錢在104上做履歷健檢，幫助自己。 履歷內容身為數據科學家，我們的目標有三個。 我們將我們做過的作品量化，例如我們幫助專案達到多少百分比的提升；或者化繁為簡，將專案條列式的顯示出來，並將專案中，您負責的內容清楚地寫下來，並且描述您在裡面所做的工作事情幫助到專案的地方在哪？ 將自己的優勢特色展現出來，例如做數據科學家，很重要的部分是團隊合作以及溝通，但是過往傳統的履歷不容易呈現這塊，可以在履歷中將參與過的計畫人數以及透過什麼方式，將你碰到的困難點解決，這部分就可以透過履歷去表達，讓面試者知道你是很友善的資料科學家。 因為資料科學家的層面太廣，能幫助到公司的內容很多，但我們可以透過應徵的內容更改我們履歷的寫法，例如是做ＣＶ為主的職缺，那我們在投職前，會先思考怎樣的model(YOLO、SSD、Mask R-CNN)，或者資料前處理可以幫助到公司，並且朝著那塊寫履歷，然後再寄出，這樣上的機率就會更高。 整理程式碼 push 到github資料科學家很重要的就是分析，不只是對自己分析，能夠讓面試者分析您，也是一種漂亮的視覺化，因此建議可以將自己過去的專案整理好push 到github分享給大家，會是一個非常好認可自己能力的機會，若是沒有好的內容，沒關係以下提供了幾個快速又簡單的方式讓自己的程式可以量化。Siraj Raval 是一名 Youtuber他的專案有非常多的學習計畫，一步步的學習並且push上去就可以慢慢累積自己的作品量。Avik Jain 最有名的就是 deep learning 100天，而這個計畫其實只持續50多天，就停止了，但是內容其實不難，但能重新吸收理解，當初的做法想法也是很不錯的。 撰寫文章在我自己的實驗中，寫了一陣的程式碼後，我發現github不是一個商品，是一個作品集，而沒有商品是無法有實質的回饋，因此經過 Z Realm — 解決問題的道路上你並不孤單 大大的介紹，開始踏入的medium的旅程，寫medium的好處有兩個。第一個在寫文章的過程中可以清楚地將自己的重新理一遍，並且可以思考如何才能幫助讀者看懂文章。第二實際嘗試過給面試官看文章的感想，那當下的感覺就會像你第一次拿iphone的感覺，因為你跟別人不一樣。 閱讀書籍因為認識了飛載國際的創辦人，讓我有機會再面試的閒暇時間可以有機會多讀幾本書，下面是我念的幾本書讓大家可以用不同的角度思考。自我介紹的技術這本書的好處在於，可以快速的讓面試者知道你會什麼。機率思考這本書很不錯，可以當成各個小故事慢慢看。富人不說這本書，教的是很多做事情的細節，可以應用於面試中。百面機器學習，真的很不錯裡面有教很多model的應用，而且是case by case，讓你覺得原來他也像故事書一樣有趣。 程式邏輯練習在面試的過程中，有非常多的程式是考leedcode相關的內容，或者是排序法，因此在這面試期間，我也有在youtube上看著大大們的邏輯練習。 考古題記得去網路上多google，公司的考古題，很多公司其實考試的內容跟軟體工程師內容很像，可以去參考看看。 英文能力提升主要我學習英文的地方有兩個第一個是 linguamarina ，這個youtuber是專門教英文的人，長得漂亮，又可以學她對英文的看法與細節很不錯。另一個是Airbnb上班的Luba她的channel Life of Luba ，因為她是軟體工程師，所以不但可以學英文，又可以學程式思維，人又長得漂亮，真的可以一兼多顧呢～二、面試我總共面試了聯電，鴻海，和碩，凌群，威盛，技嘉，大聯大，QNAP，世界先進，華碩。在這當中和碩是資料工程師，並非資料科學家。那我們開始一間間介紹面試內容以及感想。聯電因為當初還就讀中正，所以就面試南科聯電，當天面試時，共面了4關，第一關邏輯測驗，感覺聯電的邏輯測驗滿間單的，記得距離考試結束還有10鐘就寫完了，接下來考的是英文，然後是兩關的技術面試。第一個主管做的事機器學習的演算法開發，問我研究所做的專案，以及利用過哪些機器學習的模型做訓練，以及晶圓瑕疵品在深度學習中該如何找出來。第二個主管問比較偏平台佈建，面試的內容是hadoop與spark的佈建方式，以及他的運作內容，然後問使用什麼類型的資料庫(它們用的是oracle的)如果進來的時候可以要幫忙建置openstack與GPU database。然後就進入二面，結果二面當天大主管臨時有事沒來，所以撲空，只剩下ＨＲ面試我，問我為什麼要來這邊啊，為什麼不去北部要來南部，做事細不細心啊，最後跟我說可能要三面，傻眼貓咪。但最後跟他說因為要論文面試所以可能無法在面試他們也體諒了。結果：offer get心得：聯電是面試這麼多家公司服務態度最好的，他們的ＨＲ態度真的是誇張的優於平均線，主管也非常的開明，可惜是在南部，但如果南部的鄉親想試試聯電真的是你的好選擇。鴻海考英文、人格測驗、工業互聯網專業考試，多對一面試；工業互聯網考試真的是挺有趣的裡面的內容包山包海有些屬於機械相關的人才會，當然基礎的ＡＩ也是有的，面試的方式都是多對一，剛開始是一個個自我介紹，才知道大家都是台清交成的只有我小廢廢，然後讓大家介紹自己以前做過的專案，然後碰到訓練模型成效不好時可以怎麼解決，也有問進來後發現工作的內容跟想像中的不一樣的時候怎麼辦，另外也問大家都去哪裡學習ＡＩ領域相關的課程。在本次面試中完全沒有考資料分析或者機器學習細節的問題並且還趁機跟我們說他們部門有另外招人，可以用其他方式跟他聯絡。結果：沒上，英文太爛。心得：說實在的他的薪水開的真的非常高，可能這樣才招的到好人才吧，另外上班地點在大陸，對新鮮人來說這的確是一個挑戰，我個人去面試的目的其實是想要了解我們國家有哪些人在學相關領域的東西，以及去認識朋友，還有我們當初中正的技術落後程度大概多少，這份工作的缺點是你無法清楚的知道你之後的工作內容以及你的同事是誰，但優點是對事業有野心的可以去大陸了解他們的文化，甚至之後直接跳BANT四巨頭，都是更容易的。和碩因為剛當完兵沒有面試經驗，想說和碩找我試試資料工程師，我就去了，當天共分為性向測驗，程式測驗，面試，程式測驗對我來說偏難，因為要考很多C C+ +的內容，我主要是寫python，所以我只能寫虛擬碼ＸＤＤ，因為他們很多棟大樓，所以就走去其他棟大樓面試，面試我的是研發替代役的同事，人非常的好，大部分的時間都在互相聊天，聊佈建環境時遇到的甘苦談，他們負責的工作是維護spark 與 hadoop 並且做一些基礎的ETL或者統計分析，資料量非常的大，分析的內容以Apple的為主，因為主管去澳洲出差所以要等二面。二面前先請我寄之前寫過的專案程式碼，過幾天說因為我專長在數據分析，與他們的職缺不太吻合所以謝絕了。結果：沒有上。心得：這間公司面試的感覺非常好，整體環境感覺以禪風搭配有點科技感的味道，據所知上班時數可能會辛苦一點，但能拿到的運算資源會是非常的多，但壓力應該不輕，可惜的是沒有數據分析的職缺，不然這間真的可以來試試看。凌群一個過去還沒面試就上的公司… ，一進去主管就說我看過你的履歷內容，滿喜歡的，但主管感覺不太會深度學習的內容，工作地點也是外派，一直跟我說薪水很不錯，並且公司很穩定，感覺就不太知道外包工作的內容，只想要找一個人補上，考邏輯測驗跟文章記憶測驗，然後就上了….我都不知道我在做什麼…結果：offer get大聯大控股公司，做的內容以倉管系統整合搭配數據分析進行的專案，共有三關，第一關人資面試面試的內容是你為什麼要來，以及公司是否有初步了解，還有想要在這間公司3年後的展望，以及你喜歡理論還是實務，第二關是ＩＴ部門主管，因為他本身非這領域的出生，所以很希歡新鮮人進去嘗試，面試時不太以問答，而是以想法的方式作為依據，例如，倉儲的數據管理，有什麼想法可以做，那你要怎樣完成階段性的任務然後考python程式碼，考試考基礎的python題目，不難，但有些不常用真的容易忘記像是產生器，裝飾器這些的應用，接下來是第三關大主管，面試的內容以論文的問題為主，可能想要考思緒跟概念是否清楚，他會問得蠻仔細的並且問說你在這論文有確實幫助到你想要幫助的主題嗎 ？另外也有問你認為ＡＩ在這坡熱潮中能帶給我們什麼樣的幫助。結果：offer get心得：這個部門非常的開明，開明到感覺像是新創，各單位的主管跟人資態度也非常的友善，若是本身有能力想要一展長才的可以嘗試看看，薪水上給的不錯。QNAP一間很瞎的公司，感覺只是過客，人資愛理不理的(雖然很漂亮)，在裡面共分為性向測驗，邏輯測驗，專業測驗，部門主管面試，白板題，專業測驗考的全部都是深度學習的內容，例如監督式學習與非監督的差別，數據型資料與類別型資料如何做資料前處理，CNN RNN差別，梯度消失與爆炸會如何，當overfitting時可以透過什麼方式解決；考完後主管來了也是做醫療相關的內容，目前因數據分析已經達到不錯的結果，想問我轉平台是否ＯＫ，然後是我做的論文介紹，大概話8分鐘他介紹完就走了，接下來是白板題，考的是兩個字串如何透過程式找出全部不同的數字，不能用set並且只能是O(N)的解法。結果：offer get。心得：我覺得最後一個主管是我在最後整個面試中學習到最多的，透過交流與討論，讓我學會如何詢問需求，以及專案邏輯，甚至是程式邏輯，如果我是其他老闆知道有這個主管，我一定會花高薪挖過來，真心覺得能跟著他做事，技術能力一定會變超好，他是軟體工程師的部門，可惜了，如果是直屬主管我就過去了。威盛想說是以前的股王，就去面試了一下，有性向測驗，專業測驗，專業考的內容也是考基礎的CNN LSTM的應用在哪裡，SQL 語法，為什麼要做BP，cross entroy MISE的差別，部門主要做的是自走車，共有三位同時跟我面試，他們問的內容是有沒有駕照，能不能美國出差，會不會 object detection，以及tensorflow的能力如何，因為他們想要把model做在chip上，所以在性能中有很多的限制，問看看我在這方面有什麼想法，如何讓model降低運算資源，效果也可以不錯，還有問比賽的經驗，負責處理哪一塊內容，論文的優勢在哪裡。結果：offer get心得：這間公司其實真的不錯，感覺進去能學到很多東西，也有很多可以挑戰的項目，主管人也都非常好，有android ML可以寫，也有硬體的ＭＬ可以玩，看你想嘗試看看哪一個項目都可以接受。技嘉這個職缺蠻有趣的是全端工程師，進去前先確認資料然後考專業科目，最後才是面試，專業科目真的是從硬體底層到機器學習，網頁，資料庫全部考了一次，但都算基礎題，如果有相關知識的相信都不會太難，如資料庫建表，查詢，網頁考javascipt但我不會，考list, set, dict差別，vector, matrix, data frame差別，爬蟲，CNN，LSTM，RNN差別，硬體考arduino；接下來考完試後就有三位面試官坐電梯下來，先簡單的自我介紹，做過哪些專案，比賽的內容，寫android 的心得，GRU跟LSTM差別，LSTM為什麼可以解決梯度爆炸，機器學習這麼多種最喜歡哪一種，進來工作後可能很多東西都要會無法專注在一塊你這樣願意接受嗎？RELU跟sigmoid差別，有聽過VGG16嗎？大概是上述這些問題，他們公司主要目前是做內部的人臉辨識系統，全部都要自幹。結果：offer get心得：面試這間有很大的原因是信仰，加上好兄弟都在這裡上班，木曜四超完也較介紹過這裡，最重要的是電獺少女的其中一個當這裡的秘書，實際感想，我覺得他們還在摸索ＡＩ能做什麼事情，公司給人的感覺是硬體說話比較大聲，感覺能進來發揮的程度很低，不然真的很愛這間公司。世界先進共六關，性向測驗，英文測驗，專業測驗，主管面試，IT處長面試，人資面試，專業測驗考的內容是python加上機器學習的內容，考cross entropy的公式，overfitting如何解決，BP的流程，CNN model架構圖，不算是非常刁難的題目，但是算是一份有深度的考題，接下來是主管面試，共出了兩道題目第一晶圓瑕疵檢測，當我們沒有label，沒有明確正確或錯誤資料，可以如何解決？第二題我們現在有機器的溫度以及機房的溫度還有很多system log我們可以怎麼預測機器的零件是否需要換了？接下來是論文講解，比賽講解，做過的專案有哪些，AI在晶圓中的如何創造優勢？接下來是IT處長，是一個女生，問我和我同學的英文程度怎麼差這麼多，接下來她開始ＡＩ在世界先進的目的，然後問我細不細心，學東西快不快；最後是人資面試問我為什麼想來世界先進，在專案溝通中，碰到問題或者溝通不良會如何解決，職缺和自己想的不太一樣的時候會如何？壓力太大的時候都怎麼解決？三年後你的目標是什麼？為什麼這麼喜歡棒球？結果：offer get心得：這間工作的目的很多人是為了跳台積電，或者是薪水高，在這個職缺中我看得了一個極限，雖然天花板真的超高，也許這個極限我努力很久都還到不了，薪水上真的很不錯，主管跟人資對人也很好。華碩共面試了兩個地方的華碩，台北與新竹華碩。新竹華碩，主要做的事路由器搭配機器學習的應用，沒有考試，面試時快速的報告論文以及做過的專案比賽，並且跟主管們(兩個)聊機器學習，在路由器中，可以做什麼類型的應用以及他在機器學習學習pattern，還有當我面臨某寫狀況下我們可以透過怎樣的模型去解決這些問題，一個小時面試結束。結果：剛面試完不知道。心得：主管們對這塊算是陌生，但願意嘗試，但因為主管的氣場蠻強的所以不敢多聊。台北華碩，主要做的是語音引擎，面試當天邏輯測驗，專業能力測驗，兩個team 主管面試，大主管面試，專業能力測驗只有一題考CNN LSTM的內容，其餘有考android 是哪幾大元素組成，IPC機制介紹，字串重複地列出，泡沫排序法，以及任想排序法排序數字，語音辨識的專業(都不會)，做過的機器學習的專案有哪些，考完試後，走路到另一個大樓跟主管們碰面(跟和碩很像)，首先是兩個team面試，第一個做演算法開法的主管，問比賽經驗，專業經驗，第二個做應用的主管，問會不會NDK，以及程式能力平常都是如何練習的，接下來是大主管，華碩的語音系統五個創辦人他就是其中一個，人非常的nice，願意花時間跟我們聊天，從專案邏輯，程式效能速度，開發速度，行銷，市場趨勢，產業脈動都有聊，但只限於新鮮人，有經驗的人會問過往開發的程式開發邏輯跟流程圖，會問很細要注意。結果：剛面試完不知道。心得：台北華碩真的超級霹靂無敵推的，花個兩三年在裡面，真的可以學到很多，但壓力會很大，但這收穫一定會是職業生涯中很幫的過程，有個小壞處，公司硬體環境屬於克難中求生存，也許跟當兵差不多了。三、選擇公司的想法因為資料科學家這個職缺很新所以有幾個tips必須跟大家分享一下。 假分析，真噱頭，整個公司都不太懂這個領域在紅什麼，甚至連想做什麼都沒有清楚，因為別人做所以我也想做。 數據收集，平台建置，分析人員，研究人員，這是一個很基礎的整個過程，面試時可以多問他們上面這幾種內容他們的成熟度高低。 持久度，AI已經開始要面臨泡沫化了，這一兩年內，要從ＡＩ賺到錢真的很困難，但是若是整個公司有其他產線可以支撐AI團隊持續生存，那ＡＩ才有未來。 資源分配，一個公司很多部門做AI已經見怪不怪，但是若是你訓練model時，你的資源總是特別少，那成效一定會特別低。 跨部門的溝通，有時候資料的收集是需要更高層的人幫忙才能做到的，若是公司沒有相對應的決心，那收集資料往往是事倍功半，因此這方面也有去做思考。 是否有AI drive，主管不想信你的分析有兩種，一種是你做的真的不好，第二種是太對數據分析沒有信心，可以透過面試時試探性的詢問，了解公司的想法。目前就寫到這裡吧～ 有想法我在慢慢補上，有興趣可以互相聊聊新的概念，或者想要詢問細節可以 mail 我 jackycsie@gmail.com。祝大家找到好工作，謝謝。Post converted from Medium by ZMediumToMarkdown." }, { "title": "小數據 bang bang bang", "url": "/posts/de47a58680d5/", "categories": "", "tags": "", "date": "2019-01-24 10:03:36 +0800", "snippet": "小數據 Bang Bang Bang大家好我是佛系歐巴，Jacky。這堂操課是 刺槍術三部曲 上擊 衝擊 砍劈 的最後一集。今天我們來聊聊如何能夠將小數據蛻變成eagle讓它展翅高飛。我們在本文將會透過Deep learning的 transfer learning去實作，讓大家知道原來這就是玩轉數據。我會努力的把我寫文章的核心價值展現出來，那我們的核心價值是什麼呢？那就是任何人看完文章都會...", "content": "小數據 Bang Bang Bang大家好我是佛系歐巴，Jacky。這堂操課是 刺槍術三部曲 上擊 衝擊 砍劈 的最後一集。今天我們來聊聊如何能夠將小數據蛻變成eagle讓它展翅高飛。我們在本文將會透過Deep learning的 transfer learning去實作，讓大家知道原來這就是玩轉數據。我會努力的把我寫文章的核心價值展現出來，那我們的核心價值是什麼呢？那就是任何人看完文章都會覺得AI不難，大家都願意嘗試看看，如果真的有太艱深的詞，就當成看歐巴講故事就ＯＫ拉～那transfer learning( 遷移式學習 )它的功用是什麼，為什麼我們要在使用它，或者是他對資料有什麼幫助呢？我們歸類一下兩點來跟大家快速的簡單說明。 當有兩個類型差不多的dataset，一個已經是已經train好的model，另一個則是還沒train過，這樣我們許要重新在train一個model嗎，是不是太過浪費時間(因為train好一個model所花的時間成本非常高)，因此這就是使用transfer learning的時間點。例如：下方的兩張圖左邊的是已經train好的手寫識別dataset，而你想要預測的是右邊的資料集，你會願意花一倍的時間重新做白工嗎？這不科學啊～那你會想那直接將右邊的圖餵進去已經train好的model測試呢？孩子，很多歐巴們幫大家試完了，accuracy大概60~70%，這個效果，看了也只能笑笑對吧～這就是為什麼很多人說21世紀最迷人的職業是資料科學家，因為我們像攝影師一樣用不同的角度看世界。2. 另一個使用的時機在哪呢？就是我們本文的重點小數據；就如我們前兩篇文章看到的，不管是透過任何方法我們還是無法避免over-fitting的產生，就算我們已經努力的將驗證集accuracy提升到80~85%，我們始終缺了那關鍵的key idea。那我們若是發現過往的我們或者是網路上的歐巴們已經有人將類似圖片的big data訓練好，push在網路上，我們是否能夠將別人的智慧，放在自己的口袋中，就如同擁有智囊庫的stackoverflow，我們碰到的瓶頸是否就露出一線曙光了呢？本篇文章就會帶大家進入這個議題，我們在 kaggle 上有20,000張train好的model，透過transfer learning，帶入到我們的小數據，來看看我們的數據站在巨人的肩膀上，能否擁有巨人般的視野，那我們就來開始對付這個矯情的小三吧～不能不注意Transfer learning，有兩個tips，需要跟大家說一下。 若要使用transfer learning，必須是類型相近的資料集才能發揮功效，例如很多種類的動物去分析只有2,3種類的動物資料集，效果才會出現，下圖就是給大家看一下必須類似像下面這種接近的圖片，分析出來才會有好的accuracy。剛剛分析了gif，現在的jacky跟10年後的jacky accuracy 有87%像（誤）。2. 目前transfer learning大量的拿來使用在image，以及自然語言處理(NLP)這兩塊領域，而當用在圖片的時候，是要將前面conv以及pooling的幾層weights與bias凍結住只需要改變後面的幾層fully connection即可，而NLP則是train前面幾層layer，後面layer的weight與bias不train，大家記得要注意喔。詳細報告在 李宏毅老師的 上課內容 有說在 15:47有說到。那我們開始吧～Transfer learning在本次實驗中，我們會實驗遷移式學習的兩種做法一種是花無缺的移花接木(卷積基底:凍結 + 串接新的密集分類層)，另一種是微調 (fine-tuning)。那我們來開始吧 美系歐巴(馬) bang bang bang一、移花接木移花接木的意思是我們把下圖 Layer 1~3 的weight與bias全部留下來並且不進行train，而只把後面fully connected layer 刪除或者是修改，我們可以把它改成neural 數比較多或者是少，或者是在多個幾層Conv + BatchNorm + pooling＋ Ｄrop out都可以，重點是保留layer 1 ~ 3 前面大數據學習過後的特徵值。而本次實驗我們fully connected 在neural 數沒有改變，唯一改變的就是它的心 drop out，我們將它改成從原本的0.5改成0.3。下面是我們的code ，我們將別人訓練好的參數透過keras load到我們的model中。#load weight and biasmodel.load_weights('all_picture.h5')#刪除掉最後一層FCNmodel.pop()#低溫冷凍 layer 1 ~ layer 3 weight, biasmodel.trainable = FalseOK，就這樣牽著你的手，陪你到宇宙，靠著我肩膀baby根本不用想以後。經過一陣子的訓練後…不好意思剛剛睡著了. .下面是我們的結果圖，transfer learning比普通train model還快，原因在於我們並不需要train全部的weight, bias, 另外如果dataset的相似度很高的話，那model收斂的速度也會非常的快。我們可以很容易地看到，第一步validation acc就0.88以上開始，這意味著什麼？我們家的小數據轉大人了，前兩篇文章我們說了這麼多validation acc 最高0.85，而使用transfer learning效果，一鳴驚人，最高還在0.92呢，效果非常的不錯，連子瑜看到都說讚呢～나는 또한 강요 당한다~二、微調 (fine-tuning)講解完移花接木後，我們來聊聊fine-tuning，它的作用是什麼？它的彈性與思維跟剛剛移花接木的方法不同，它的終極目標在於只將原本訓練好的model，在某幾層進行微調，概念上不是可以add新的layer進去，例如你可以將layer2與fully connect layer做微調其他layer不動，這種方式進行訓練，但可想而知如果隨便train layer，沒有依照深度學習的概念思考那只是做白工。而在本次實驗我們將layer 3 與 FCN進行 fine-tuning，layer 1, 2不進行train，來嘗試看看實驗結果的結果如何。首先我們將train過的weight, bias匯入後，然後我們將我們想要train的layer解除封印，就可以開始train了。#load weight and biasmodel.load_weights('all_picture.h5')#然後我們將我們想要train的layer解除封印layers_frozen = ['dense_46','dropout_58', 'batch_normalization_61', 'dense_45, 'flatten_24', 'dropout_57', 'max_pooling2d_36', 'batch_normalization_60', 'conv2d_36']經過一陣的蹦蹦蹦，噠噠噠，啊啊啊，的訓練時間各位觀眾朋友！！！訓練完model後validation acc一樣是有相當不錯的實驗結果，但大部分都落在0.91, 0.92 之間，非常的穩定，loss 也猶如中華電股票一樣，維持在一定的想睡覺程度，相當不錯呢，可以證明fine-tuning是可以的不然就要了TT了。總結 從上圖的幾種做法，都可以看到這麼穩定的loss與accuracy，代表的只有一種說法，我們小資料照片複雜度不夠高，大致上都處於大家image型態都很類似，因此在train和vaildation的過程中不會有太大的浮動，對於目前的狀況單然是很好的結果，但是在生活中不可能照片的複雜度不可能的簡單。因此簡單的結論就是我們大數據train完以後的透過遷移式學習model真的可以使用，但若是真的應用是否可以用在複雜度相當高的生活中，還有待驗證。 比較移花接木與微調，可以看得出來移花接木可以來得更加的有彈性，透過接木我們可以接任何想要的layer，做出更人性化的產品，另外若是想要將移花接木與微調結合變成乾坤大挪移，其實也是可以的，但還是需要看目前的TA決定是否需要過於複雜的思維模式。3. 我們可以透過keras中許多已經train好的model來獲取我們想要的weight與bias，下面我們將會介紹我們透過更加龐大的dataset來看看我們的小數據是否有提高accuracy的機會。三、One more thing前兩種方法我們用的都是Kaggle的20,000張cat, dog數據集進行transfer learning，現在我們想要試試更大的數據集做遷移式學習，來試試是不是效果會更好，還是小數據已經盡力了？我們使用ImageNet數據集，這當中有140萬個標記的圖像和1000個不同的類別，而我們拿的也是己經預訓練好的VGG16模型。 ImageNet包含許多愛的動物類別，當然裡面也有我們想要的貓咪和狗狗，我們可以希望透過這次的遷移式學習可以讓我們的貓咪與狗狗分類問題上表現非常好。我們的VGG16 model我們拿最後的conv + pooling + FCN解鎖，進行微調，其他層的layer我們就把它鎖住，不做任何的weight與bias訓練。訓練結果出爐，透過自己實測超過10次，下圖是經過140萬image訓練後的結果，我們明顯的發現小數據最好的大概是0.9345，看得出來我們的小數據已經盡力了。延伸思考透過前三集我們可以知道，若是我們直接train小數據的話，accuracy 大概落在63%上下，用了資料增強(Data augmentation)可以達到78%，再加上Dropout、batch normalization可以達到85%，最後再透過transfer learning可以達到93%，我們一步不的帶著大家如何從手足無措的小孩，慢慢轉大人變成一個可以獨當一面的大人，而現實中真的有這麼輕鬆可以解決百病的方法嗎？因此下一篇我們會來聊聊 object detection，請讓大家用輕鬆的心理繼續看歐巴說故事吧～對der ~終於寫完了….感想，環境用的好，Training 沒煩腦，環境用不好，Training到你老。希望大家看完能對ＣＶ與ＡＩ有更加認識，如果你有看完全部的小數據介紹你真的是champion了～We are the champion~ my friend~終於寫完 刺槍術 全集了下一個寫的目標將會是 object detection.難易度會拉高很多但我會一樣用說故事的方法來聊這個議題因為我覺得ＡＩ不應該是B2B更應該貼近於 App store的 B2CPost converted from Medium by ZMediumToMarkdown." }, { "title": "小數據的逆襲", "url": "/posts/c04fee852539/", "categories": "", "tags": "", "date": "2019-01-19 16:06:03 +0800", "snippet": "小數據的逆襲 ?大家好我是母湯先生，Jacky。上一次跟大家介紹完上擊以後 =&gt; 小數據的大平台 。這集要跟大家聊聊做完資料增強以後(Data Augmentation)，我們怎麼把它套入到我們的深度學習模型中。在本篇文章將給大家三個大方向 示範小數據餵入到model以後，所造成的overfitting，我們可以如何解決。 實測透過data augmentation解決overfi...", "content": "小數據的逆襲 ?大家好我是母湯先生，Jacky。上一次跟大家介紹完上擊以後 =&gt; 小數據的大平台 。這集要跟大家聊聊做完資料增強以後(Data Augmentation)，我們怎麼把它套入到我們的深度學習模型中。在本篇文章將給大家三個大方向 示範小數據餵入到model以後，所造成的overfitting，我們可以如何解決。 實測透過data augmentation解決overfitting的效果表現。 過dropout, batch normalization, data augmentation後的輸出結果是否比前面兩者來得更好。那我們來開始吧！一、資料集我們使用的資料集是 kaggle 提供的資料，該原始數據集包含25,000張狗和貓的圖像(每個類別12,500個)，但是這是官方提供的資料，而我們為了模擬小數據集時所碰到的難題，我們訓練集只拿出2000張貓與狗的image(各1000張)，驗證集以及測試集各1000張，下圖是loading資料過後的結果。二、資料預處理 (Data Preprocessing)當我們得知我們圖片的內容無誤後，我們該如何將圖片轉換成模型能夠讀入的資料呢？我們透過下列四個步驟去完成我們想要的任務。 讀進image檔案。 將image內容解碼為RGB的像素。 將image轉換成浮點張量。 最後將像素值（ 0和255之間）重新縮放到[0,1]間隔，方便deep learning做運算。以下我們透過程式碼介紹我們是如何做到資料預處理的。Keras有一個圖像處理工具的module，位於 keras.preprocessing.image 。當中的 ImageDataGenerator 類別，可以快速的自動將disk上的圖像文件轉換成tensors。我們將在這裡使用這個工具。使用ImageDataGenerator的 好處 在於Keras並不是在記憶體中對整個圖像數據集執行圖像轉換操作以及儲存，而是設計為通過深度學習模型訓練過程進行迭代，從而為您動態地創建增強的圖像數據。這會減少您的記憶體開銷，但在模型訓練期間會增加一些額外的時間成本。from keras.preprocessing.image import ImageDataGenerator# 透過下列方式可以將所有的image做正規化。train_datagen = ImageDataGenerator(rescale=1./255)讓我們看一下當做完上述的程式碼後我們shape後出來的值會是多少。for data_batch, labels_batch in train_generator: print('data batch shape:', data_batch.shape) print('labels batch shape:', labels_batch.shape) break我們會出現每次20批次的大小130*130的相片。OK 做完了圖片的前處理後我們開始我們的重頭戲開始來建立model吧～三、建立模型沒錯～我們想要建立一個可以識別狗跟貓圖片的model，那這麼多的模型我們會選擇哪種呢？我想有基礎深度學習概念的大大們，第一個想法就是拿卷積神經網路(CNN)下去試試看，卷積神經網路有幾個優點。 共享卷積核，對高維資料處理無壓力。 不需要手動選取特徵，訓練好權重，即得特徵分類效果好。 對圖片識別特別有效，卷積的根本目的是從輸入圖片中提取特徵。卷積用一個小方格的數據學習圖像特徵，可以保留像素之間的空間關係。因上述幾種因素我們這次選擇的model也是用基於CNN概念的改良版，來看一下CNN的model長怎樣。對不起 XDD，放錯張不是這個model，也不是那個CNN，是下面這個。在本次實驗中我們使用的是High-level API keras，底層是tensorflow在運行，為什麼不直接使用tensorflow呢？簡單的原因在於我們沒有要實作非常困難或者需要自行定義loss function,GD等等複雜的運算，因此我們用keras簡單快速。下面是我們使用keras functional api 所建立出來的模型。Input以後會有兩層的convolutional layer提取feature，接下來做降維的max pooling，重複兩次動作以後，做flatten並且再接一個FCN最後output成為1維的輸出。下面是我們的程式碼。target_number = 130image_input = Input(shape=(target_number, target_number, 3), name='input')conv1 = Conv2D(3, kernel_size=3, activation='relu', name='conv1')(image_input)conv2 = Conv2D(10, kernel_size=3, activation='relu', name='conv2')(conv1)pool1 = MaxPool2D(pool_size=(2, 2), strides=(2,2), name='pool1')(conv2)conv3 = Conv2D(3, kernel_size=3, activation='relu', name='conv3')(pool1)conv4 = Conv2D(5, kernel_size=3, activation='relu', name='conv4')(conv3)pool2 = MaxPool2D(pool_size=(3, 3), strides=(2,2), name='pool2')(conv4)flatten1 = Flatten(name='flatten1')(pool2)既然寫完了build完我們就開始來跑跑看我們的model的效果如何吧～run~modelrun~沒有使用資料增強(Data augmentation)的dataset由下圖可以看到沒有使用資料增強的dataset，就像電影越來越愛你(la la land)兩個不回頭的情人，越走越遠了，因為它們overfitting了，validation data的acc都沒有超過65%以上，loss 也越來越高。使用資料增強(Data augmentation)的dataset說了這麼多，我們的重頭戲終於來了首先來看一下我們keras使用那些 ImageDataGenerator來做資料的加強。train_generator = ImageDataGenerator( rotation_range=15, rescale=1./255, shear_range=0.1, zoom_range=0.2, horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1)假設我們model.fit的steps_per_epoch設定為100,epoch設為50,batch_size設定為32，那32*100*50等於160,000，就等於了我們讓這個model訓練了160,000張照片，是不是一件很酷的事情，從原本只有2000張照片變成80倍的資料集，這就是 資料科學家迷人的地方 。而我們的model跟上一個model，一模模一樣樣，讓我們來看看只做了一個小小的改變，能變成怎樣的結果。各位觀眾！！！！從下圖可以容易地看出，我們的資料完全沒有如剛剛overfitting那種誇張的分離，並且我們的validation acc足足提升了10%以上，loss value也持續地跟train loss value一樣不段的在下降，這兩條原本看似永不相愛的戀人，變成了像真愛每一天(about time)的情人一樣緊緊相依，請各位掌聲加尖叫，謝謝。大家看到這裡就過癮了嗎？？我想我們可以試試更多不一樣的選擇來去解決overfitting的方式，在接下來我們會使用dropout，batch normalization, data augmentation，做結合，來看看結合這麼多的方法會不會像航海王的黑鬍子一樣變得這麼的powerful，讓我們繼續看下去。下集待續…被我騙到了吧～ 別慌下集不就來了嗎～使用dropout以及batch normalization提升accuracy上述這兩種方法是資料科學家常用來提升accuracy，或這是降低loss時常用的手法，另外其實還有L1 L2 regularization，但因為這對新手來說太難所以這次沒有放進去介紹。為了應映新的方法，我們改變了model的架構，下圖我們的model是參考Uysim大大寫的，用的方法就是很basic的Conv + BatchNorm + mac pooling + Dropout，做三次卻可以得到驚人的效果。下面的程式碼是透過keras functional api完成上述的model。mage_input = Input(shape=(target_number, target_number, 3), name='input')conv1 = Conv2D(32, kernel_size=3, activation='relu', name='conv1')(image_input)bn1 = BatchNormalization()(conv1)pool1 = MaxPool2D(pool_size=(2, 2), name='pool1')(bn1)drop1 = Dropout(0.25, name='drop1')(pool1)conv2 = Conv2D(64, kernel_size=3, activation='relu', name='conv2')(drop1)bn2 = BatchNormalization()(conv2)pool2 = MaxPool2D(pool_size=(2, 2), name='pool2')(bn2)drop2 = Dropout(0.25, name='drop2')(pool2)conv3 = Conv2D(128, kernel_size=3, activation='relu', name='conv3')(drop2)bn3 = BatchNormalization()(conv3)pool3 = MaxPool2D(pool_size=(2, 2), name='pool3')(bn3)drop3 = Dropout(0.25, name='drop3')(pool3)flatten1 = Flatten(name='flatten1')(drop3)建立完model以後來讓我們看看見證奇蹟的時刻吧！從上圖中我們一樣的看到了overfitting，就像浩子說的人一直走對的路也不是辦法，做機器學習當然也不是幾個簡單的方法套一套就可以做到有如夢幻般的神力，但還是有好事情發生的雖然我們的model依舊overfitting，但是我們的validation acc已經可以提升到將近80~85%以上的水準，比起過去的數據還是有相當不錯的水準，by the way 如果我們原本的dataset有20000筆資料我們的model在validation acc可以到90%的成績。另外看一下我們做的confusion matrix資料，雖然數據量還不夠客觀但可以讓大家看看漂亮的圖片～四、圖片視覺化圖片視覺化是做影像識別中最迷人也最著迷的部分，透過它你就像007 詹姆士龐德穿著西裝演動作片，而忘了你只是個清槍開始、清槍蹲下的二等兵。下圖是我們將圖片轉換成RGB以後的照片，那大家一定在想經過convolutional的貓咪它的特徵萃取是怎樣的呢？我們給大家看幾張照片。下圖是第一層layer再經過convolutional後的機器會去識別的輪廓，我知道大家看不太到所以放大了一張給大家喬個仔細。沒錯下圖就是放大後的照片，長得超像埃及壁畫一樣，但是還是蠻酷的實驗，其實還有很多有趣的視覺化可以試試，透過openCV可以還原當初深度學習訓練時的案發現場，這種感覺就像是貓吸到貓薄荷一樣有趣。五、總結透過了上述的幾種講解我們可以得知小數據的確還是有機會做到不錯的成果，但是仍然有它的限制在，所以資料還是盡可能的收集才是解決之道，當然機器學習的基礎學習也是必要的。寫完了上擊與衝擊後，接下來我們就是要朝向砍劈了，在這部分我還在思考大家比較想要看我們應用在Apple ios ML中，還是想要做遷移式學習(transfer learning)？下一集 =&gt; 小數據 bang bang bang 。謝謝大家總算寫完了…其實難的不是寫文章而是環境問題, train了大概3,40次有吧. .Post converted from Medium by ZMediumToMarkdown." }, { "title": "小數據的大平台", "url": "/posts/f555d1eb6e34/", "categories": "", "tags": "", "date": "2019-01-15 15:26:36 +0800", "snippet": "小數據的大平台 (上擊)大家好我是工科先生，Jacky。今天想跟大家聊聊的是，當數據集的量過小時，我們該如何在deep learning也有相當不錯的成績。希望大家，看完篇文章後，能在未來碰到這類問題時，能夠有更多的想法可以去解決。而本篇的重點將會放在圖片的數據中，但是在機器學習本身的領域也會稍微提到。這篇文章將會有兩大主軸 一、介紹增加數據的方法 二、在增加數據後，比較深度學習在資料量多寡...", "content": "小數據的大平台 (上擊)大家好我是工科先生，Jacky。今天想跟大家聊聊的是，當數據集的量過小時，我們該如何在deep learning也有相當不錯的成績。希望大家，看完篇文章後，能在未來碰到這類問題時，能夠有更多的想法可以去解決。而本篇的重點將會放在圖片的數據中，但是在機器學習本身的領域也會稍微提到。這篇文章將會有兩大主軸 一、介紹增加數據的方法 二、在增加數據後，比較深度學習在資料量多寡的的差別。本篇文章共分為三集第二集的名稱是 =&gt; 小數據的逆襲 。第三集的名稱是=&gt; 小數據 bang bang bang 。ㄧ、 增加數據量1. 爬蟲：透過爬蟲將google圖片爬下來，增加你的數據集，但壞處在於label會非常不容易標，而且圖片大小還是需要經過裁切才能使用，另外爬蟲過後的品質相當不穩定，非常有可能變成noise，反而造成訓練模型時，一個很大的絆腳石，但整體而言是相當有效的擴增數據集方式。 莫凡的爬蟲網站有聲書 ，將會非常好的介紹如何大量的爬取所需要的資料，而本篇文章將不會示範爬蟲的方式增加dataset。2. Data Augmentation (資料增強) :當資料量不足，或者資料複雜度不夠高時，往往可能會造成訓練模型時有over-fitting(過擬合)的產生，當然over-fitting可以使用 L1/L2 正規化，或者是dropout技術解決，而本篇文章決定使用data augmentation來做處理，常用的data Augmentation工具有3種，OpenCV，Keras，pillow，而目前常用的圖片augmentation有6種，以下我們將會介紹有哪七種方式，可以讓我們在圖片的資料量不足時做加強。圖片隨機翻轉(flip)：水平垂直翻轉，透過不同的180度翻轉可以得到更多的照片，透過keras ImageDataGeneratord可以隨機的180度翻轉照片。import keras ImageDataGenerator(horizontal_flip=True, vertical_flip=True)相片隨機旋轉(rotate)：透過這種功能可以有更多不同的選轉角度去生產所需要的資料量。import keras圖片裁剪(crop)：這種方法其實不是非常推薦，因為必須要到前提在於，你的整張圖片都是你想要的標籤主題，否則可能切不到你要的標籤，下圖是切割的範例。from PIL import Image圖片隨機偏移(shift)：透過圖片隨機，水平垂直偏移，可以讓機器訓練時，有夠多的想法，可以了解當label在不同地方時，是否一樣能夠辨別。import kerasImageDataGenerator(width_shift_range=200, height_shift_range=200)隨機推移錯切 (Random Shear)維基百科 — 圖像錯切 ，圖像錯切的想法，就如同用不同的角度拍照一樣，透過不同的角度去訓練機器，讓機器可以學習，當有一樣的照片不同的角度時仍然能夠識別，透過這種方式可以增加機器的可識別度與資料量。ZCA 白化轉換 (ZCA Whitening)ZCA白話轉換的想法是從PCA轉換過來，其目的在於減少圖片像素矩陣中的冗餘/去除相關性，若是需要在圖片中有更好的收斂效果那可試試看這個方法，去解決它。import kerasImageDataGenerator(zca_whitening=True)圖片產生雜訊(Noise)在機器學習中，若是data量太少，或者是資料不夠複雜容易產生過擬合(over- fitting)，因此增加noise的好處就是可以讓model學習複雜度提升，並且讓資料更豐富。3. 基於Deep learning model 產生資料目前最熱門用deep learning 產生資料的model就屬於生成對抗網路(GAN)莫屬了，不管是在醫學圖像，照片風格轉移，自動訓練剪裁放大所小增加noise,etc.，都可以看到他的身影，而網路有大大整理了 GAN的論文 ，在各項領域中所生成的圖像；而下圖就是透過GAN增加Data的另一種方式。這篇論文在 ＣＶ 界造成了大轟動呢。二 、不能不注意(Sampling)當我們產生了相當多的圖像數據後，開始要進入訓練，而在這方面許多新手會一些問題，那就是產生數據完，會發生不平衡資料(imbalanced data)。那我們來聊聊這狀況，什麼時候會發生這種問題呢？例如：狗貓比9:1，那餵入機器學習後模型後的思考比率就會過於傾向於狗的分類而忽略貓類。因此這時候我們有兩種做法，第一種透過上述的data augmentation，將貓的資料變成5:5平衡或者透過下採樣undersampling or 上採樣oversampling，下圖就是非常容易理解的示意圖，這樣的好處就可以讓model學習時更加的有效，不會太過偏向於哪一種label。三、下集待續跟大家說完如何解決小資料的問題後，我們將會實作deep learning CNN model，來試試看小數據跑model後的結果，以及做完data augmentation後我們的model，CP值大分析，下次見。預計本次小數據大平台，將會分為上擊 衝擊 砍劈，三小節跟大家聊聊。終於寫完了…Post converted from Medium by ZMediumToMarkdown." } ]

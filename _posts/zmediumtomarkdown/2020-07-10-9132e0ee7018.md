---
title: "Deploy Ceph cache tier & erasure code"
author: "黃馨平"
date: 2020-07-10T07:31:51.613+0000
last_modified_at: 2020-08-05T08:50:04.592+0000
categories: ["Jackycsie"]
tags: ["ceph","storage"]
description: "本篇文章主要紀錄的是如何應用 cache tier 與 erasure code 在 Cephfs 當中。"
render_with_liquid: false
---

### Deploy Ceph cache tier & erasure code


![](https://miro.medium.com/max/1400/1*e7-XWTx6MvNk_3K1SONq_Q.jpeg)


本篇文章主要紀錄的是如何應用 cache tier 與 erasure code 在 Cephfs 當中。

本篇文章將會分 4 個部分撰寫：
1\. 建立 cache pool，撰寫 crush map rule 將 SSD 與 HDD 分開。
2\. 將 cephfs pool 建立 ，並使用 erasure code 與 HDD rule\.
3\. 將 cache tier 與 cephfs pool 合併。
4\. 刪除 cache tier 與 cephfs 。
### Step 1\.

撰寫 crush map 將 SSD 與 HDD 分開
- Check SDD 與 HDD 的 OSD\.ID 編號

```
ceph osd tree | grep hdd
ceph osd tree | grep ssd
```
- 先將 crush map decode

```
ceph osd getcrushmap -o crushmapdump
crushtool -d crushmapdump -o crushmapdump-decompiled
```
- vim crushmapdump\-decompiled 開始撰寫 rule。


第一步區分 SSD block 跟 HDD block
```
root ssd {
        id -40          # do not change unnecessarily
        # weight 0.546
        alg straw2
        hash 0  # rjenkins1
        item osd.6 weight 0.182
        item osd.7 weight 0.182
        item osd.8 weight 0.182
}
```
```
root sata {
        id -41          # do not change unnecessarily
        # weight 20.019
        alg straw2
        hash 0  # rjenkins1
        item osd.0 weight 0.455
        item osd.1 weight 0.455
        item osd.2 weight 0.455
        item osd.3 weight 0.455
        item osd.4 weight 0.455
        item osd.5 weight 0.455
        item osd.9 weight 0.455
        item osd.10 weight 0.455
        item osd.11 weight 0.455
        item osd.12 weight 0.455
        item osd.13 weight 0.455
        item osd.14 weight 0.455
        item osd.15 weight 0.455
        item osd.16 weight 0.455
        item osd.17 weight 0.455
        item osd.18 weight 0.455
        item osd.19 weight 0.455
        item osd.20 weight 0.455
        item osd.21 weight 0.455
        item osd.22 weight 0.455
        item osd.23 weight 0.455
        item osd.24 weight 0.455
        item osd.25 weight 0.455
        item osd.26 weight 0.455
        item osd.27 weight 0.455
        item osd.28 weight 0.455
        item osd.29 weight 0.455
        item osd.30 weight 0.455
        item osd.31 weight 0.455
        item osd.32 weight 0.455
        item osd.33 weight 0.455
        item osd.34 weight 0.455
        item osd.36 weight 0.455
        item osd.37 weight 0.455
        item osd.38 weight 0.455
        item osd.39 weight 0.455
        item osd.40 weight 0.455
        item osd.41 weight 0.455
        item osd.42 weight 0.455
        item osd.43 weight 0.455
        item osd.44 weight 0.455
        item osd.45 weight 0.455
        item osd.46 weight 0.455
        item osd.47 weight 0.455
}        
```

第二步撰寫 tier 所需要的 rule。
```
rule ssd-pool {
        id 1
        type replicated
        min_size 1
        max_size 10
        step take ssd
        step chooseleaf firstn 0 type osd
        step emit
}
rule sata-pool {
        id 2
        type erasure # very import. if you want use erasure. You must set this type.
        min_size 1
        max_size 10
        step take sata
        step chooseleaf firstn 0 type osd
        step emit
}
```

Crush map 都寫完以後 encode 然後 set 到 system 中\.
```
crushtool -c crushmapdump-decompiled -o crushmapdump-compiled
ceph osd setcrushmap -i crushmapdump-compiled
```

第三步建立 cache\-pool
```
# create cache-pool
ceph osd pool create cache-pool 32 32
```
### Step 2\.

將 cephfs pool 建立 ，並使用 erasure code 與 HDD rule\.
```
# 建立 cold storage 的 pool，若不需要 erasure 可以刪除，但在 crush map 也需要更改。
ceph osd pool create cephfs_data 1024 1024 erasure
```
### Step 3\.

將 cache tier 與 cephfs pool 合併。
```
# 將兩個 pool 合併，cephfs_data 為 cold storage，cache-pool 為 hot storage。
ceph osd tier add cephfs_data cache-pool
# 設定 cache-pool 的模式為 writeback 模式(其實還有readonly 但不推)。
ceph osd tier cache-mode cache-pool writeback
```

因為本篇應用的方法是使用 cephfs 所以接著建立 cephfs。
```
# cephfs 還是需要 metadata 的 pool(雖然用不到)
ceph osd pool create cephfs_metadata 1024 1024
```
### 完成 \! \!

有興趣的可以玩一下，當中也有很多 cache\-tier 調優的方法。
```
ceph osd pool set cache-pool hit_set_type bloom
ceph osd pool set cache-pool hit_set_count 1
ceph osd pool set cache-pool hit_set_period 3000
ceph osd pool set cache-pool target_max_bytes 161061273600
ceph osd pool set cache-pool target_max_objects 1000000
ceph osd pool set cache-pool cache_min_flush_age 600
ceph osd pool set cache-pool cache_min_evict_age 1800
ceph osd pool set cache-pool cache_target_dirty_ratio .6
ceph osd pool set cache-pool cache_target_dirty_high_ratio .7
ceph osd pool set cache-pool cache_target_full_ratio .8
```

另外小 tips 可以透過指令觀察 cache tier 的變化。
```
watch -n1 ceph df 
```
### Step 4\.

刪除 cache\-tier 與 cephfs。

在做這個步驟時必須十分警慎，不然處理起來會複雜很多，自己實測按照下面的步驟可以減少很多冤枉路。
```
# Check cache-pool 中有沒有 dirty object
ceph df detail
# 如果有 dirty object 請使用，若沒有可忽略，會花一段時間。
rados -p cache-pool cache-flush-evict-all
# 將 writeback 模式先設定成 readproxy，再改成 none 模式，它無法像 readonly 一樣可以直接改成 none 模式。
# ceph osd tier cache-mod cache-pool forward(研究中)
ceph osd tier cache-mode cache-pool readproxy
ceph osd tier cache-mode cache-pool none
```
- 刪除 cephfs ，因為個人實測無法直接在 tier 刪除 cephfs\_data pool 必須先移除 cephfs 才能解除 cache\-tier 的相依性。

```
# 關閉 mds
systemctl stop ceph-mds@ceph-007
# 關閉 cephfs
ceph fs fail cephfs
```
- 移除 cache tier 的 cold\-storage 以及 cache tier

```
ceph osd tier remove-overlay cephfs_data
ceph osd tier remove cephfs_data cache-pool
```
### 結束了 \! \!

現在大家都回復自由之身了， cold storge pool 的內容都還保存，沒有問題。

大致上就結束了，有任何問題歡迎討論。


![](https://miro.medium.com/max/1400/1*G4p-DfRg6q3RFd9EJFzhLQ.jpeg)

### 參考資料 :
- [Ceph 官網](https://docs.ceph.com/docs/jewel/rados/operations/cache-tiering/){:target="_blank"}
- [Cache tier tuning](https://yq.aliyun.com/articles/606731){:target="_blank"}
- [管理 cache tier](https://www.cnblogs.com/breezey/p/11080532.html){:target="_blank"}
- [cache tier 介紹與基礎教學](https://yq.aliyun.com/articles/606731){:target="_blank"}
- [SUSE 教學](https://documentation.suse.com/zh-cn/ses/5.5/html/ses-all/cha-ceph-tiered.html){:target="_blank"}



_[Post](https://medium.com/jacky-life/deploy-ceph-cache-tier-erasure-code-9132e0ee7018){:target="_blank"} converted from Medium by [ZMediumToMarkdown](https://github.com/ZhgChgLi/ZMediumToMarkdown){:target="_blank"}._

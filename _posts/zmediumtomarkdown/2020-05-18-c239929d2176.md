---
title: "MLflow: 紀錄您 Train 步驟的平台"
author: "黃馨平"
date: 2020-05-18T12:08:38.996+0000
last_modified_at: 2020-05-18T12:19:52.078+0000
categories: ["Jackycsie"]
tags: ["mlflow","machine-learning","automation","jupyter","tools"]
description: "本文將初探，如何透過 MLflow，紀錄每一步 Train 的結果。"
render_with_liquid: false
---

### MLflow: 紀錄您 Train 步驟的平台


![](https://miro.medium.com/max/1400/1*WwtKhB-daG0tYG6gZjvYnw.jpeg)


過往，我們在做機器學習時，總會碰到以下幾個問題。
1. 我們在 train model 時，我們總是忘記選過那些超參數。
2. 剛剛前一個 model 訓練的效果不錯，它的參數為何 ，想保留 model 又遭到覆蓋，只能重 train 。
3. 過了幾週，我們想要重現前一次好的訓練結果，但是我們的資料集已經不見了。
4. 當團隊在 co\-work 時，是不是因為不同的環境導致有不同的結果出現 ？


這些問題，也困擾了我非常長一段段時間，而這些問題可能也困擾著你，或許這篇文章，會是您不錯的解決方案。

那這個工具的名字叫做 MLflow。

MLflow 是由 Databricks 開發而成，那 Databricks 旗下也開發了許多 amazing 的 工具，像是 Spark 也是由同一個創辦人開發而成的。
#### 1\. 環境 Demo

在本篇的分享中，將會在 **container** 中進行，另外也會在 jupyter 與 CLI 中分別分享，它們的結果。
#### 2\. 安裝 MLflow
```
pip3 install mlflow
```
#### 3\. 快速開始

首先我們使用官方建議的 code 來練練手。
```
import os
from mlflow import log_metric, log_param, log_artifact

if __name__ == "__main__":
    # Log a parameter (key-value pair)
    log_param("param1", 5)

    # Log a metric; metrics can be updated throughout the run
    log_metric("foo", 1)
    log_metric("foo", 2)
    log_metric("foo", 3)

    # Log an artifact (output file)
    with open("output.txt", "w") as f:
        f.write("Hello world!")
    log_artifact("output.txt")
```
- 使用 python 執行 quick\_start\.py file\.



![](https://miro.medium.com/max/1400/1*iruZzzcNePln8X2fTtogeQ.jpeg)

- 在 mnt 資料夾中，開啟 MLflow 服務。

```
mlflow server --host 0.0.0.0
```


![](https://miro.medium.com/max/1400/1*vZCD7hZkqsqKCy0aLYR2xQ.jpeg)

- 在網址列中輸入 server 的 ip 以及 port 號，例如 http://127\.0\.0\.1:5000，本文用的是 5555 是因為我們是 docker 環境。



![](https://miro.medium.com/max/1400/1*Pe4VkMb4cym10spLapL09w.jpeg)


有開啟的畫面以及資料的話，代表成功了，若是沒有剛剛的執行結果，可能是您開啟的服務在錯誤的資料夾，跟 quick\_start\.py 同一層即可。
#### 4\. 訓練手寫識別

在這節，我們透過訓練手寫識別模型，並且調整它的超參數，跟大家分享如何快速上手 MLflow，看完馬上可以直接使用。
- **變數參數化**


下面的步驟我們會將幾項超參數，改為 MLflow 可以儲存 log 的方式去撰寫。
1. Convolutional filter
2. Kernel\_size
3. Max pooling
4. Dropout
5. Dense
6. Batch\_size
7. Epochs

```py
from __future__ import print_function
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras import backend as K

######
# mlflow 紀錄的方法，先指定好超參數的變數
import mlflow.keras

mlflow.keras.autolog()

first_layer_conv = 32
first_layer_kernel_size = (3,3)

second_layer_conv = 64
second_layer_kernel_size = (3,3)

pool_size = (2,2)
dropout_rate = 0.2
dense_neural = 128

mlflow.log_param("first_layer_conv", first_layer_conv)
mlflow.log_param("first_layer_kernel_size", first_layer_kernel_size)

mlflow.log_param("second_layer_conv", second_layer_conv)
mlflow.log_param("second_layer_kernel_size", second_layer_kernel_size)

mlflow.log_param("pool_size", pool_size)
mlflow.log_param("dropout", dropout_rate)
mlflow.log_param("dense_neural", dense_neural)

batch_size = 128
num_classes = 10
epochs = 12
# 將這些變數放到 model 設置，超參數的地方
#####

# input image dimensions
img_rows, img_cols = 28, 28

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

if K.image_data_format() == 'channels_first':
    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    input_shape = (1, img_rows, img_cols)
else:
    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
    input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = Sequential()
model.add(Conv2D(first_layer_conv, kernel_size=first_layer_kernel_size,
                 activation='relu',
                 input_shape=input_shape))
model.add(Conv2D(second_layer_conv, second_layer_kernel_size, activation='relu'))
model.add(MaxPooling2D(pool_size=pool_size))
model.add(Dropout(dropout_rate))
model.add(Flatten())
model.add(Dense(dense_neural, activation='relu'))
model.add(Dropout(dropout_rate))
model.add(Dense(num_classes, activation='softmax'))

model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
```
- 訓練 model

```
python official_keras_mnist.py
```
- 訓練完，連到剛剛的網站，就可以看到多了一個訓練結果與我們剛剛設定 model 時，用到的參數。



![](https://miro.medium.com/max/1400/1*lCrxW1mEOyH9B6G9G3ckvg.jpeg)

- 點進去專案看實驗的細節。



![](https://miro.medium.com/max/1400/1*NZyyRtwKgn0vgsFCaWYJhw.jpeg)

- 可以看到列出了非常詳細的參數內容，方便進行回推\(回顧\)。



![](https://miro.medium.com/max/1400/1*eQYUR6vpPGsd--6gC1hRGw.jpeg)

- 在往下滑可以看到，剛剛計算的 accuracy, loss 等等資訊。



![](https://miro.medium.com/max/1400/1*MK9oHYWGNQH4UEGWzDPexQ.jpeg)

- 點進去看以後就可以查看剛剛訓練每一個步驟的細節。



![](https://miro.medium.com/max/1400/1*2NUUN-UTNnQmNBajB2RFDg.jpeg)

- 接著在回到上一頁，往下滑，可以看到環境設定，所使用的工具版本，儲存的 model ，model summary 細節資訊。



![](https://miro.medium.com/max/1400/1*Mqotwgo-eksTakdqoJ29rQ.jpeg)

### 5\. 比較模型

上述是我們的第一次 MNIST 訓練，在平常實戰中，我們相同的資料集會訓練非常多不同的超參數，但若本身沒有紀錄，根本不會確切地記得當初下的參數內容如何，本節來跟大家分享我對 mlflow 的心動的原因。

因此我們修改了參數 train 了第二個 model，可以從圖中看到我們的第二個結果明顯變差，但是我們只改了參數，這實我們可以回推到底是改了那些參數導致結果變差。


![](https://miro.medium.com/max/1400/1*rEr5Z0YJNm2IVurfJ35khg.jpeg)

- 點選\(兩個或多個\) checkbox，並且按 compare，即可做表格類型比較，黃色的就是我們跟前一個 model 不同的地方，透過這些不同的地方就可以快速回推，我們該如何重現或修改模型。



![](https://miro.medium.com/max/1400/1*K8T9wrC0I-iTs_U5pQD1sw.jpeg)

- 看數據細節結果，可以更快速的知道，雖然 accuracy 落差蠻大，但是 val\_accuracy 並無像 train data 差別如此之大。



![](https://miro.medium.com/max/1400/1*pg7_glRbdzF_uoloss7Efg.jpeg)

- 透過視覺化比較圖表。



![](https://miro.medium.com/max/1400/1*bkuHbXS5Mm9WAqA96wLyQg.jpeg)

#### Search bar
- 我們可以像 Kibana 一樣，在 search bar 中快速過濾掉我們不想看的資訊。

```
metrics.val_accuracy > 0.96
```


![](https://miro.medium.com/max/1400/1*CIOlw5gU-PrmkHygImlWxg.jpeg)

#### 組織化

可以看到，當我們有很多 model 需要訓練或者儲存時，只有一個 table，不但沒有增加理解度，反而把事情搞砸了，這邊想跟大家分享，當我們有好幾個 model 時，我們應該如何將每個 model 的訓練參數放到正確的 table 中。
- 建立新的 experiment，您可以使用 CLI 或者是 GUI 建立。

```
mlflow experiments create --experiment-name sklearn_train
```
- 接著預估會在哪個 experiment 跑實驗下 command 切換紀錄 table。

```
export MLFLOW_EXPERIMENT_NAME=sklearn_train
```
- 跑 Training 實驗。

```py
# The data set used in this example is from http://archive.ics.uci.edu/ml/datasets/Wine+Quality
# P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.
# Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet
import mlflow
import mlflow.sklearn
def eval_metrics(actual, pred):
    rmse = np.sqrt(mean_squared_error(actual, pred))
    mae = mean_absolute_error(actual, pred)
    r2 = r2_score(actual, pred)
    return rmse, mae, r2
    
alphas = [0.5, 0.7, 0.9]
l1_ratios = [0.3, 0.5, 0.9]
np.random.seed(40)
# Read the wine-quality csv file from the URL
csv_url ='http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'
try:
    data = pd.read_csv(csv_url, sep=';')
except Exception as e:
    print("Unable to download training & test CSV, check your internet connection. Error: %s", e)
# Split the data into training and test sets. (0.75, 0.25) split.
train, test = train_test_split(data)
# The predicted column is "quality" which is a scalar from [3, 9]
train_x = train.drop(["quality"], axis=1)
test_x = test.drop(["quality"], axis=1)
train_y = train[["quality"]]
test_y = test[["quality"]]
for alpha in alphas:
    for l1_ratio in l1_ratios:
        with mlflow.start_run():
            lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)
            lr.fit(train_x, train_y)
            predicted_qualities = lr.predict(test_x)
            (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)
            mlflow.log_param("alpha", alpha)
            mlflow.log_param("l1_ratio", l1_ratio)
            mlflow.log_metric("rmse", rmse)
            mlflow.log_metric("r2", r2)
            mlflow.log_metric("mae", mae)
            mlflow.sklearn.log_model(lr, "model")
```
```
python sklearn_elasticNet.py
```
- 可以看到切換紀錄 table 就是如此的輕鬆愜意。



![](https://miro.medium.com/max/1400/1*nzOteIm-WrpfBycsIRu8vg.jpeg)

- Jupyter 上如何實做 MLflow 追蹤以及組織化的整理呢 ，在這當中有兩種做法。
- 第一選擇想要跑的 experiment table 接著開啟 jupyter，在執行上述的程式，但這種方法過於麻煩。
- 第二直接在 jupyter 上選擇想要跑的 experiment table，但小壞處是，所有的 log 必須自己寫儲存方式。
- 最後，若是只想專注在 1 個 model ，其它儲存的東西都交給 MLflow 處理的話，那使用剛剛上述的 MNIST program 就可以再 jupyter 上執行。


下面的程式碼是介紹，第二種方法的 MLflow。
```py
from mlflow.tracking import MlflowClient
import mlflow
import mlflow.keras

from __future__ import print_function
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras import backend as K

######
# mlflow 紀錄的方法，先指定好超參數的變數，另外設定想要儲存在哪一個 experiments

client = MlflowClient()
experiments = client.list_experiments()
run = client.create_run(experiments[1].experiment_id)

first_layer_conv = 32
first_layer_kernel_size = (3,3)

second_layer_conv = 64
second_layer_kernel_size = (3,3)

pool_size = (2,2)
dropout_rate = 0.2
dense_neural = 128

client.log_param(run.info.run_id, "first_layer_conv", first_layer_conv)
client.log_param(run.info.run_id, "first_layer_kernel_size", first_layer_kernel_size)

client.log_param(run.info.run_id, "second_layer_conv", second_layer_conv)
client.log_param(run.info.run_id, "second_layer_kernel_size", second_layer_kernel_size)

client.log_param(run.info.run_id, "pool_size", pool_size)
client.log_param(run.info.run_id, "dropout", dropout_rate)
client.log_param(run.info.run_id, "dense_neural", dense_neural)

batch_size = 128
num_classes = 10
epochs = 1

# 將這些變數放到 model 設置，超參數的地方
#####

# input image dimensions
img_rows, img_cols = 28, 28

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

if K.image_data_format() == 'channels_first':
    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    input_shape = (1, img_rows, img_cols)
else:
    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
    input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = Sequential()
model.add(Conv2D(first_layer_conv, kernel_size=first_layer_kernel_size,
                 activation='relu',
                 input_shape=input_shape))
model.add(Conv2D(second_layer_conv, second_layer_kernel_size, activation='relu'))
model.add(MaxPooling2D(pool_size=pool_size))
model.add(Dropout(dropout_rate))
model.add(Flatten())
model.add(Dense(dense_neural, activation='relu'))
model.add(Dropout(dropout_rate))
model.add(Dense(num_classes, activation='softmax'))

model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])

history = model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))

train_accuracy = history.history['accuracy']
test_accuracy = history.history['val_accuracy']

train_loss= history.history['loss']
test_loss = history.history['val_loss']

client.log_metric(run.info.run_id, "train_accuracy", train_accuracy[0])
client.log_metric(run.info.run_id, "test_accuracy", test_accuracy[0])
client.log_metric(run.info.run_id, "train_loss", train_loss[0])
client.log_metric(run.info.run_id, "test_loss", test_loss[0])

score = model.evaluate(x_test, y_test, verbose=0)
```
- 透過上述的第二種方法，就可以使資料科學家快速的在 jupyter 調參，也可以達到我們希望自動記錄 MLflow log 的目標。



![](https://miro.medium.com/max/1400/1*j7_r1RMuOicd0PUf7Fn8ag.jpeg)

- 最後小提醒，若是想要可以在 jupyter 上快速調參，記得複製我 \# \# \# 上下的 code ，在一個 cell ，在執行 shift \+ enter，不然它會把你的調參認為是前一份工作的事情，導致誤報。

#### MLflow 再探
- 到這邊 Mlflow 初探大致上結束，其實 MLflow 還有非常多值得跟大家分享的特色，如實驗資料儲存 DB，data version control，不同版本 model 直接存取，超參數自動選擇儲存，選擇 Model 跑不同的環境等等。
- 這篇的目標在於讓大家可以快速上手 MLflow ，之後再把上述說特色一個個補齊吧。



![](https://miro.medium.com/max/1400/1*shVJ-RyxNxq3BYxzTuELDQ.jpeg)

#### 參考網址
- MLflow 官方網站 \( [website](https://mlflow.org/){:target="_blank"} \)
- AIA 教學 \( [website](https://medium.com/ai-academy-taiwan/mlflow-a-machine-learning-lifecycle-platform-%E5%85%A5%E9%96%80%E6%95%99%E5%AD%B8-5ec222abf5f8){:target="_blank"} \)
- MLflow github \( [website](https://github.com/mlflow/mlflow){:target="_blank"} \)
- MLflow 介紹文 \( [website](https://www.jianshu.com/p/d70b25bf3cf4){:target="_blank"} \)

#### 感謝

本篇文的產生，還是需要感謝我們 team 願意讓我嘗試與部屬，另外也非常感謝 james 的 k8s ，快速的 training model ，讓我開始思考，如何更自動化，有效率的幫助自己快速釐清，我 train 了幾百個 model 之間，不同參數所代表的意義。

當 GPU 的運算力，是你的競爭力時，如何做出藍寶堅尼與布加迪之間的差別，就是我們工程師的能力了。



_[Post](https://medium.com/jacky-life/mlflow-%E7%B4%80%E9%8C%84%E6%82%A8-train-%E6%AD%A5%E9%A9%9F%E7%9A%84%E5%B9%B3%E5%8F%B0-c239929d2176){:target="_blank"} converted from Medium by [ZMediumToMarkdown](https://github.com/ZhgChgLi/ZMediumToMarkdown){:target="_blank"}._

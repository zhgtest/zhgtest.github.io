---
title: "AWS EC2 中部署 LLM"
author: "黃馨平"
date: 2024-08-25T03:33:37.699+0000
last_modified_at: 2024-08-25T03:33:37.699+0000
categories: ["Jackycsie"]
tags: ["ec2","llama-3"]
description: "前言"
image:
  path: /assets/82bebda01507/1*4cq3L1fcAw45QNw3UmG2Gw.jpeg
render_with_liquid: false
---

### EC2 中部署 LLM


![](/assets/82bebda01507/1*4cq3L1fcAw45QNw3UmG2Gw.jpeg)

### 前言

由於最近開刀在家休息，沒辦法出去走走，想說閒閒沒事做，來把 LLM 部署在 AWS EC2 上玩看看。

那下面幾個是我在找 LLM Open Source Project 時的幾個考量。
1. 安全性：當 LLM 下載到 EC2 以後，可以隔離對外網路也可以正常使用，不然我就使用 ChatGPT 就好拉 ＸＤＤ，就是不想敏感資料上傳網路。
2. 方便性：目前很多都是透過 API 的方式自己寫程式進行溝通，但我又蠻懶的，想要直接找可以像 ChatGPT 有互動式的介面可以直接使用。
3. 可以有登入的介面，並且搭配 Security Group 讓想要使用的朋友同事，也可以使用隔絕對外網路的 LLM。


經過簡單的查找，我發現只要兩個 Github Project 就可以達到我的目標。
1. Ollama \(6\.7K Fork\)： [https://github\.com/ollama/ollama](https://github.com/ollama/ollama){:target="_blank"}


Ollama 是一個開源的專案，目標是讓使用者能夠在自己的電腦上運行大型語言模型，而不需要依賴雲端服務或強大的硬體設備。

以下是 Ollama 專案的一些主要特色：
- **本地化運行** ：Ollama 可以在 macOS 和 Linux 系統上本地運行，讓使用者能夠更方便地控制和使用大型語言模型，同時保護資料隱私。
- **高效能** ：Ollama 採用了多種優化技術，包括量化、模型壓縮等，使得大型語言模型能夠在消費級硬體上高效運行。
- **易於使用** ：Ollama 提供了簡單易用的命令列介面和圖形化使用者介面，讓使用者能夠輕鬆地安裝、管理和使用各種大型語言模型。
- **開放原始碼** ：Ollama 是一個完全開源的專案，使用者可以自由地檢視、修改和貢獻程式碼。


2\. Open WebUI \(Formerly Ollama WebUI\) \(4\.2K Fork\): [https://github\.com/open\-webui/open\-webui](https://github.com/open-webui/open-webui){:target="_blank"}

這個 GitHub 專案 [Open WebUI](https://github.com/open-webui/open-webui){:target="_blank"} 提供了一個使用者友善的網頁介面，用於操作大型語言模型。它是一個自架設的網頁應用程式，支援多種 LLM 執行器。

Open WebUI 的主要特色包括：
- **輕鬆設定** ：可輕鬆安裝和設定，無需複雜的配置。
- Ollama/OpenAI API **整合** ：支援 Ollama 和 OpenAI API，可使用各種大型語言模型。
- **可自訂的管線** ：使用者可以自訂模型處理管線，以滿足特定需求。
- **響應式設計** ：介面可自動調整大小，以適應不同螢幕尺寸。


那說明一下，其實我也只是搬運工，本身沒有做什麼事情，純粹只是跟大家分享一下，在 EC2 部署隔絕外網的 LLM 以及有身份驗證的功能而已 ＸＤ
### 步驟分享：
1. 我先在 EC2 ubuntu 24\.04 上安裝 Ollama，使用的機型是 p3\.8xlarge，當然可以不需要用那麼好的設備，我是想要玩超大的 LLM 才用比較好的設備，玩完以後就用最便宜的了。

```typescript
$ curl -fsSL https://ollama.com/install.sh | sh
```

2\. 因為 Open WebUI 這個 Project 是使用 Docker 運行，那因為若是 Docker 運行的話要先安裝一個 `nvidia-container-toolkit` ，到時候 Docker 調用 GPU 的時候才不會報錯，所以我先安裝了 `nvidia-container-toolkit` ，步驟如下。
```typescript
# Configure the repository:
1. curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey |sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
&& curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list \
&& sudo apt-get update

# Install the NVIDIA Container Toolkit packages:
$ sudo apt-get install -y nvidia-container-toolkit

# Configure the container runtime by using the nvidia-ctk command:
$ sudo nvidia-ctk runtime configure --runtime=docker

# Restart the Docker daemon:
$ sudo systemctl restart docker
```

3\. 安裝 Open WebUI
```typescript
docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
```

4\. 確認有安裝成功
```typescript
$ docker ps
CONTAINER ID   IMAGE                                  COMMAND           CREATED             STATUS                       PORTS                                       NAMES
671f6fc22b24   ghcr.io/open-webui/open-webui:ollama   "bash start.sh"   About an hour ago   Up About an hour (healthy)   0.0.0.0:3000->8080/tcp, :::3000->8080/tcp   open-webui
```

5\. 透過 Public IP 搭配 Port 登入到 EC2，介面如下。一開始需要註冊一組帳號密碼，之後再透過這組帳號密碼登入就可以了。


![](/assets/82bebda01507/1*_-f8jxzCW37xxBOhlqEmrA.png)


6\. 登入進來後的介面如下，但這個時候我們還沒有安裝 LLM ，所以還不能使用。


![](/assets/82bebda01507/1*TxKOxTXtAgEWwicErH5XsA.png)


7\. 下面教學怎麼安裝 LLM model。


![](/assets/82bebda01507/1*O781EdwxpCDFmyBfjz78vQ.png)



![](/assets/82bebda01507/1*x_1PEF7LZBSHiaDN7omlyw.png)



![](/assets/82bebda01507/1*WRvV5nUV-StDYWmB2QPOzw.png)


8\. 安裝成功


![](/assets/82bebda01507/1*MRf3PzlxQRkZJ5CQ2NQNdQ.png)


9\. 我圖片安裝的是 Llama 的 他有 70B 的參數，約 40GB的大小，若是想要玩玩看其他的 LLM 可以查看這個 [Github](https://github.com/ollama/ollama){:target="_blank"} 網站，他有眾多其他 LLM 可以選擇。


![](/assets/82bebda01507/1*2VOOVzVga33hU93CF0zEXA.png)


10\. 我請他用 Java 寫一個簡單的 9 X 9 乘法表。


![](/assets/82bebda01507/1*ASicV9ZpcWObYQH9cYyJPQ.png)


11\.請他解讀照片目前都還解讀失敗中，看樣子私有雲的 LLM 大模型，可能還不能解讀照片跟檔案，之後再認真研究一下，因為這塊我的需求比較小，所以沒有認真玩，我主要都在寫程式 ＸＤ


![](/assets/82bebda01507/1*n3zWg_VdF4f8Ryp1kAQUgA.png)


11\. 好玩的地方在於，當 LLM 在回覆時，全部的 GPU 都會跑，但是我在寫入資料時，只有一個 GPU 在跑。


![](/assets/82bebda01507/1*m6EK1hRtg1pYnww39PC-Ww.png)

### 結束

超級簡單的分享 ＸＤＤ，想說來做個筆記如何佈建的，大家記得 Security group 在 outbound 也要設定自己的 IP 喔，不然這就不屬於個人使用的 LLM 模型了，敏感資料就可以能 Share 出去了，畢竟是 Open Source Project，大概是這樣～ 其他的大家可以自己部署玩玩看～

如果有其他好玩的 LLM 相關的 Project 也可以分享一下喔，感謝各位大大。


![](/assets/82bebda01507/1*B8DjqE-zK85swSUIupssYQ.jpeg)




_[Post](https://medium.com/jacky-life/aws-ec2-%E4%B8%AD%E9%83%A8%E7%BD%B2-llm-82bebda01507){:target="_blank"} converted from Medium by [ZMediumToMarkdown](https://github.com/ZhgChgLi/ZMediumToMarkdown){:target="_blank"}._

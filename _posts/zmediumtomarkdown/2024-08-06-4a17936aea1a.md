---
title: "AWS 雲端 591 租屋爬蟲架構"
author: "黃馨平"
date: 2024-08-06T07:30:21.372+0000
last_modified_at: 2024-10-14T16:37:18.302+0000
categories: ["Jackycsie"]
tags: ["aws","memorydb","sns"]
description: "基於 AWS 架構，透過 Python 撈取 591 最新資訊，傳送到信箱"
image:
  path: /assets/4a17936aea1a/1*nna5JxC0yYVVKlBPwYMvFA.jpeg
pin: true
render_with_liquid: false
---

### AWS 雲端 591 租屋爬蟲架構


![](/assets/4a17936aea1a/1*nna5JxC0yYVVKlBPwYMvFA.jpeg)

### 緣由

最近因為家裡有租屋的需求，剛好比較忙比較沒有空自己一直刷 591 來看，想說剛好自己 own 的服務自己測，所以我就拿了自己的服務玩一下，看看有哪裡沒有很熟的，剛好來了解一下學習，為了避免怕我以後忘記我在寫什麼程式想說久違的使用文章記錄一下。
### 架構圖


![](/assets/4a17936aea1a/1*2sm6UMFzZ68GIiRAniCdTQ.png)

### 架構解說

我這邊主要使用到的服務是 EC2 On\-Demand, EC2 Spot, SNS, MemoryDB；程式這邊我是使用 Python3\.10\.12 來做開發。
1. 每天早上 9 點到晚上 9 點，透過 EC2 591 Crawler 程式將 591 上面的房屋資訊撈下來。
2. 將撈下來的房屋資訊與 MemoryDB 做比對，若是重複的 ID 就不儲存。
3. 確認 ID 沒有重複過後透過 SNS 傳送給 Mail 讓我們收到目前最新的租屋資訊。
4. Spot Change EIP 每天的晚上 12 點會出發 crontab 將 EC2 591 Crawler 的 EIP 切換，避免追蹤被黑名單。
5. 每天透過 Data Lifecycle Manager 幫 EC2 591 Crawler 做備份，目的在於每一次都會儲存 logs 以及每天都可能會修改資料所以確保我的資料都還會存在，我的 snapshot 是保留 5 天，超過 5 天我就刪掉。

### 591 撈取房屋資料介紹

關於 591 撈取房屋資料的程式我是參考這篇文章來進行修改的 [\[Python爬蟲實例\] 591 租屋網 — 搜尋房屋與房屋詳情](https://blog.jiatool.com/posts/house591_spider/){:target="_blank"} ，這篇文章說明的很詳細，不管是買房，租屋，都可以透過這個程式做修改達到自己的需求，推推。

2024/10/15 更新: 由於 591 那邊的租屋 API 有更新我這邊有寫了新版的爬蟲程式，請大家跟著下面的這個教學網址搭配使用，謝謝:
```py
import time
import random
import requests
import re
from bs4 import BeautifulSoup
from urllib.parse import urlencode

class House591Spider:
    def __init__(self):
        """初始化 session 和 headers"""
        self.session = requests.Session()
        self.headers = {
            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',
        }

    def get_house_info(self, house_url):
        """根據給定的 URL 取得網頁內容並解析出房屋 ID"""
        try:
            response = self.session.get(house_url, headers=self.headers)
            response.raise_for_status()  # 如果狀態碼不是 200，則引發異常
        except requests.exceptions.RequestException as e:
            print(f"Error fetching data: {e}")
            return set()  # 返回空的 set

        soup = BeautifulSoup(response.text, 'html.parser')
        house_ids = self.get_house_ids_from_page(soup)
        return house_ids

    def get_house_ids_from_page(self, soup):
        """從網頁內容中提取所有房屋 ID"""
        house_ids = set()  # 使用 set 來存儲唯一的房屋 ID
        for link in soup.find_all('a', href=True):
            href = link['href']
            match = re.search(r'https://rent\.591\.com\.tw/(\d+)', href)
            if match:
                house_id = match.group(1)  # 提取數字部分
                house_ids.add(house_id)    # 使用 set 的 add 方法來添加元素
        return house_ids

    def search(self, filter_params=None, sort_params=None):
        """搜尋房屋，根據篩選條件和排序條件"""
        base_url = 'https://rent.591.com.tw/list?'
        params = filter_params or {'region': '1', 'kind': '0'}  # 預設篩選條件
        if sort_params:
            params.update(sort_params)  # 添加排序條件

        # 使用 urllib.parse 組合查詢參數
        search_url = base_url + urlencode(params)
        print(f"Requesting: {search_url}")

        # 獲取房屋資訊並添加隨機延遲
        house_ids = self.get_house_info(search_url)
        time.sleep(random.uniform(1, 3))  # 添加隨機延遲，模擬人為請求

        return len(house_ids), house_ids

if __name__ == "__main__":
    house591_spider = House591Spider()

    # 定義篩選條件
    filter_params = {
        'metro': '125',        # 捷運線：淡水信義線
        'station': '66300,4188'  # 捷運站：信義安和 & 大安
    }

    # 定義排序條件
    sort_params = {
        'sort': 'money_desc'  # 租金由高到低排序
    }

    # 執行搜尋
    total_count, houses = house591_spider.search(filter_params, sort_params)
    
    # 打印結果
    print(f"總共找到 {total_count} 間房屋")
    print("房屋 ID 列表：", houses)
```


[![](https://blog.jiatool.com/images/posts/house591_spider_meta.jpg)](https://blog.jiatool.com/posts/house591_spider/){:target="_blank"}

### 去重複，寫入 DB
1. 有介紹到我有透過 TLS 加密所以在 python 中需要特別設定 SSL 為 True 另外我這邊有使用 connection pool 接著再使用 redis connect
2. 這樣做的好處可以減少每次程式需要連線時先經過 TLS 的協定，來提高效能。
3. 寫入資料前先比對，資料是否重複，若是資料已經重複就不儲存。
4. 若是資料沒有存在 DB 中 開始做資料前處理，來做傳送給 SNS 的資料做前處理。
5. 資料前處理完成，傳給 SNS feature 來做傳送。

```py
import redis
import json
import time
import random
import requests
import boto3
import datetime
import pytz
from bs4 import BeautifulSoup

def connect(REDIS_HOST, REDIS_PORT, REDIS_SSL_CONNECTION, redis_db_instance):
    try:
        # Check if SSL connection is required
        is_ssl_connection = REDIS_SSL_CONNECTION

        # Create a Redis connection pool with SSL/TLS support
        redis_connection_pool = redis.ConnectionPool(
            host=REDIS_HOST,
            port=REDIS_PORT,
            db=redis_db_instance,
            connection_class=redis.SSLConnection if is_ssl_connection else redis.Connection
        )

        # Create a Redis client using the connection pool
        redis_client = redis.Redis(connection_pool=redis_connection_pool)

        return redis_client
    except Exception as err:
        print("Error while connecting Redis client >> ", str(err))
        return False

def write_db(redis_client, house_info):

    new_house_list_str = ""  # 初始化為空字串
    i = 1

    if redis_client:
        print("Redis 連線成功！")

        # 將資料寫入 MemoryDB
        for key, value in house_info.items():
            # 檢查 key 是否已存在 (使用 EXISTS 命令)
            if not redis_client.exists(key):
                # 將資料寫入 MemoryDB
                redis_client.set(key, json.dumps(value))
                house_url = "https://rent.591.com.tw/home/" + str(key)
                new_house_list_str += f"{i} {house_url}: {value}\n"  # 將資料添加到字串
                i = i+1
                print(f"已儲存 key: {key}, value: {value}")
            else:
                # print(redis_client.get(key))
                print(f"key: {key} 已存在，不儲存")
    else:
        print("Redis 連線失敗")
    return new_house_list_str

if __name__ == "__main__":

    REDIS_HOST="clustercfg.crawler591.m0szxl.memorydb.ap-northeast-1.amazonaws.com"
    REDIS_PORT="6379"
    REDIS_SSL_CONNECTION=True
    redis_db_instance=0

    # 測試連線
    redis_client = connect(REDIS_HOST, REDIS_PORT, REDIS_SSL_CONNECTION, redis_db_instance)
    new_house_list = write_db(redis_client, house_detail)
```
### 透過 SNS 將資料 Push 到 Mail

當 519 Crawler 的資料前處理完成以後我們會將資料的內容透過 String 的方式傳送到 Mail box，而那個區間沒有新資料的，我這邊還是會傳送 mail 給我知道目前沒有新資料，不然會擔心服務是不是掛掉了，這樣可比較安心。

另外我在寄送郵件時也有放個今天的時間以確保收到 mail 的時候不會混亂。
```py
import redis
import json
import time
import random
import requests
import boto3
import datetime
import pytz
from bs4 import BeautifulSoup

def get_currect_time():

    tz = pytz.timezone('Asia/Taipei')  # 或 pytz.timezone('Etc/GMT+8')
    utc_now = datetime.datetime.now(datetime.timezone.utc)
    local_now = utc_now.astimezone(tz)
    formatted_time = local_now.strftime("%Y-%m-%d")

    return formatted_time

def send_SNS_mail(new_house_list):

    formatted_time = get_currect_time()
    # 建立 SNS Client (無需提供憑證)
    sns = boto3.client('sns', region_name='ap-northeast-1')
    # 設定郵件內容
    subject = formatted_time + ' --- 大安租屋爬蟲'
    topic_arn = 'arn:aws:sns:ap-northeast-1:237089372480:Rent_591'

    if new_house_list != "":

        message = f'這是來自 591 爬蟲的通知郵件。\n\n{new_house_list}'  # 將 new_house_list_str 添加到訊息中
        # print(message_with_default)

        # 發送郵件
        response = sns.publish(
            TopicArn=topic_arn,
            Message=message,
            Subject=subject,
            MessageStructure='string'  # 確保純文字格式
        )
        # print(f'Message ID: {response["MessageId"]}')

    else:
        message = f'目前沒有新的資料'
        response = sns.publish(
            TopicArn=topic_arn,
            Message=message,
            Subject=subject,
            MessageStructure='string'  # 確保純文字格式
        )

if __name__ == "__main__":
    send_SNS_mail(new_house_list)
```
### 隨機睡眠時間

寫這個的目的主要在於降低被抓的機率，如果每天在都在相同時間撈資料，久而久之就容易被 block 掉，這樣的好處就是可以再降低被發現是爬蟲的服務。
```py
def random_sleep():
    """
    隨機休眠 5 分鐘到 10 分鐘之間。
    """

    # 計算休眠秒數範圍
    min_seconds = 5 * 60  # 5 分鐘
    max_seconds = 10 * 60  # 10 分鐘

    # 隨機生成休眠秒數
    sleep_seconds = random.randint(min_seconds, max_seconds)

    print(f"開始休眠 {sleep_seconds} 秒...")
    time.sleep(sleep_seconds)  # 進行休眠
    print("休眠結束！")
```
### **更改 EIP**

由於擔心一直爬網站會被 591 Bang ，所以我這邊用了一個定時換 Public IP 的程式，讓實例每天在晚上 12 點的時候，會自動把舊的 EIP 刪除，並且換上新的 EIP 。
```py
import boto3
import os

ec2_client = boto3.client('ec2')

def get_eip_allocation_id_by_instance_id(instance_id):
    """根據 Instance ID 取得 EIP Allocation ID"""
    try:
        response = ec2_client.describe_addresses(
            Filters=[
                {   
                    'Name': 'instance-id',
                    'Values': [instance_id]
                }
            ]
        )

        if response['Addresses']:
            return response['Addresses'][0]['AllocationId'], response['Addresses'][0]['AssociationId']
        else:
            return None

    except Exception as e:
        print(f"取得 EIP Allocation ID 時發生錯誤: {e}")
        return None

def dissociate_eip(allocation_id):
    """將 EIP 與 EC2 instance 解除關聯"""
    try:
        response = ec2_client.disassociate_address(
            AssociationId=allocation_id
        )
        print(f"EIP {allocation_id} 已解除關聯")
    except Exception as e:
        print(f"解除關聯 EIP {allocation_id} 時發生錯誤: {e}")

def release_eip(allocation_id):
    """釋放 EIP"""
    try:
        response = ec2_client.release_address(
            AllocationId=allocation_id
        )
        print(f"EIP {allocation_id} 已釋放")
    except Exception as e:
        print(f"釋放 EIP {allocation_id} 時發生錯誤: {e}")


def change_ec2_public_ip(instance_id, region_name):
    """
    更換指定 EC2 執行個體的公有 IP 地址，安全處理 EIP 和 Public IP 的各種情況。

    Args:
        instance_id (str): EC2 執行個體的 ID。
        region_name (str): EC2 執行個體所在的 AWS 區域名稱。
    """

    ec2 = boto3.resource('ec2', region_name=region_name)
    instance = ec2.Instance(instance_id)

    # 分配新的 EIP
    new_allocation = ec2.meta.client.allocate_address(Domain='vpc')
    response = ec2.meta.client.associate_address(
        AllocationId=new_allocation['AllocationId'],
        InstanceId=instance_id
    )

    print(f"已成功為 EC2 執行個體 {instance_id} 更換公有 IP 地址為 {response['AssociationId']}")

if __name__ == "__main__":

    instance_id = [""]
    region_name = "ap-northeast-1"
    for i in range(0, len(instance_id), 1):
        eip_data = get_eip_allocation_id_by_instance_id(instance_id[i])
        if eip_data:
            eip_AllocationId, eip_AssociationId = eip_data
            print(f"Instance {instance_id[i]} 的 EIP Allocation ID 為: {eip_AssociationId}")
            dissociate_eip(eip_AssociationId)
            release_eip(eip_AllocationId)
        else:
            print(f"Instance {instance_id[i]} 沒有關聯的 EIP")
        change_ec2_public_ip(instance_id[i], region_name)
```
### 總結

簡單的架構圖與程式碼差不多就寫好了，整體來說蠻簡單的，主要是來練習一下透過 Python3 程式碼與這些串接，以及來測試一下平常客戶端可能會碰到什麼痛點來模擬一下，以及用客戶開發的想法來思考事情，祝大家能找到自己喜歡的房子。
### 下一個計畫

目前看到信義房屋這邊還沒有人寫，下次用其他 DB 來玩一下，不然 MemoryDB 這個太貴了，燒我太多錢了。


![](/assets/4a17936aea1a/1*HLHLkk_Q9N1c3TeDQ9PjXA.jpeg)




_[Post](https://medium.com/jacky-life/aws-%E9%9B%B2%E7%AB%AF-591-%E7%A7%9F%E5%B1%8B%E7%88%AC%E8%9F%B2%E6%9E%B6%E6%A7%8B-4a17936aea1a){:target="_blank"} converted from Medium by [ZMediumToMarkdown](https://github.com/ZhgChgLi/ZMediumToMarkdown){:target="_blank"}._
